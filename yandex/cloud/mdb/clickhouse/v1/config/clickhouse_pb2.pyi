"""
@generated by mypy-protobuf.  Do not edit manually!
isort:skip_file
"""

import builtins
import collections.abc
import google.protobuf.descriptor
import google.protobuf.internal.containers
import google.protobuf.internal.enum_type_wrapper
import google.protobuf.message
import google.protobuf.wrappers_pb2
import sys
import typing

if sys.version_info >= (3, 10):
    import typing as typing_extensions
else:
    import typing_extensions

DESCRIPTOR: google.protobuf.descriptor.FileDescriptor

@typing.final
class ClickhouseConfig(google.protobuf.message.Message):
    """ClickHouse configuration options. Detailed description for each set of options
    is available in [ClickHouse documentation](https://clickhouse.com/docs/en/operations/server_settings/settings/).

    Any options not listed here are not supported.
    """

    DESCRIPTOR: google.protobuf.descriptor.Descriptor

    class _LogLevel:
        ValueType = typing.NewType("ValueType", builtins.int)
        V: typing_extensions.TypeAlias = ValueType

    class _LogLevelEnumTypeWrapper(google.protobuf.internal.enum_type_wrapper._EnumTypeWrapper[ClickhouseConfig._LogLevel.ValueType], builtins.type):
        DESCRIPTOR: google.protobuf.descriptor.EnumDescriptor
        LOG_LEVEL_UNSPECIFIED: ClickhouseConfig._LogLevel.ValueType  # 0
        TRACE: ClickhouseConfig._LogLevel.ValueType  # 1
        DEBUG: ClickhouseConfig._LogLevel.ValueType  # 2
        INFORMATION: ClickhouseConfig._LogLevel.ValueType  # 3
        WARNING: ClickhouseConfig._LogLevel.ValueType  # 4
        ERROR: ClickhouseConfig._LogLevel.ValueType  # 5

    class LogLevel(_LogLevel, metaclass=_LogLevelEnumTypeWrapper): ...
    LOG_LEVEL_UNSPECIFIED: ClickhouseConfig.LogLevel.ValueType  # 0
    TRACE: ClickhouseConfig.LogLevel.ValueType  # 1
    DEBUG: ClickhouseConfig.LogLevel.ValueType  # 2
    INFORMATION: ClickhouseConfig.LogLevel.ValueType  # 3
    WARNING: ClickhouseConfig.LogLevel.ValueType  # 4
    ERROR: ClickhouseConfig.LogLevel.ValueType  # 5

    @typing.final
    class MergeTree(google.protobuf.message.Message):
        """Options specific to the MergeTree table engine."""

        DESCRIPTOR: google.protobuf.descriptor.Descriptor

        REPLICATED_DEDUPLICATION_WINDOW_FIELD_NUMBER: builtins.int
        REPLICATED_DEDUPLICATION_WINDOW_SECONDS_FIELD_NUMBER: builtins.int
        PARTS_TO_DELAY_INSERT_FIELD_NUMBER: builtins.int
        PARTS_TO_THROW_INSERT_FIELD_NUMBER: builtins.int
        INACTIVE_PARTS_TO_DELAY_INSERT_FIELD_NUMBER: builtins.int
        INACTIVE_PARTS_TO_THROW_INSERT_FIELD_NUMBER: builtins.int
        MAX_REPLICATED_MERGES_IN_QUEUE_FIELD_NUMBER: builtins.int
        NUMBER_OF_FREE_ENTRIES_IN_POOL_TO_LOWER_MAX_SIZE_OF_MERGE_FIELD_NUMBER: builtins.int
        MAX_BYTES_TO_MERGE_AT_MIN_SPACE_IN_POOL_FIELD_NUMBER: builtins.int
        MAX_BYTES_TO_MERGE_AT_MAX_SPACE_IN_POOL_FIELD_NUMBER: builtins.int
        MIN_BYTES_FOR_WIDE_PART_FIELD_NUMBER: builtins.int
        MIN_ROWS_FOR_WIDE_PART_FIELD_NUMBER: builtins.int
        TTL_ONLY_DROP_PARTS_FIELD_NUMBER: builtins.int
        ALLOW_REMOTE_FS_ZERO_COPY_REPLICATION_FIELD_NUMBER: builtins.int
        MERGE_WITH_TTL_TIMEOUT_FIELD_NUMBER: builtins.int
        MERGE_WITH_RECOMPRESSION_TTL_TIMEOUT_FIELD_NUMBER: builtins.int
        MAX_PARTS_IN_TOTAL_FIELD_NUMBER: builtins.int
        MAX_NUMBER_OF_MERGES_WITH_TTL_IN_POOL_FIELD_NUMBER: builtins.int
        CLEANUP_DELAY_PERIOD_FIELD_NUMBER: builtins.int
        NUMBER_OF_FREE_ENTRIES_IN_POOL_TO_EXECUTE_MUTATION_FIELD_NUMBER: builtins.int
        MAX_AVG_PART_SIZE_FOR_TOO_MANY_PARTS_FIELD_NUMBER: builtins.int
        MIN_AGE_TO_FORCE_MERGE_SECONDS_FIELD_NUMBER: builtins.int
        MIN_AGE_TO_FORCE_MERGE_ON_PARTITION_ONLY_FIELD_NUMBER: builtins.int
        MERGE_SELECTING_SLEEP_MS_FIELD_NUMBER: builtins.int
        @property
        def replicated_deduplication_window(self) -> google.protobuf.wrappers_pb2.Int64Value:
            """Number of blocks of hashes to keep in ZooKeeper."""

        @property
        def replicated_deduplication_window_seconds(self) -> google.protobuf.wrappers_pb2.Int64Value:
            """Period of time to keep blocks of hashes for."""

        @property
        def parts_to_delay_insert(self) -> google.protobuf.wrappers_pb2.Int64Value:
            """If table contains at least that many active parts in single partition, artificially slow down insert into table."""

        @property
        def parts_to_throw_insert(self) -> google.protobuf.wrappers_pb2.Int64Value:
            """If more than this number active parts in single partition, throw 'Too many parts ...' exception."""

        @property
        def inactive_parts_to_delay_insert(self) -> google.protobuf.wrappers_pb2.Int64Value: ...
        @property
        def inactive_parts_to_throw_insert(self) -> google.protobuf.wrappers_pb2.Int64Value: ...
        @property
        def max_replicated_merges_in_queue(self) -> google.protobuf.wrappers_pb2.Int64Value:
            """How many tasks of merging and mutating parts are allowed simultaneously in ReplicatedMergeTree queue."""

        @property
        def number_of_free_entries_in_pool_to_lower_max_size_of_merge(self) -> google.protobuf.wrappers_pb2.Int64Value:
            """If there is less than specified number of free entries in background pool (or replicated queue), start to lower
            maximum size of merge to process.
            """

        @property
        def max_bytes_to_merge_at_min_space_in_pool(self) -> google.protobuf.wrappers_pb2.Int64Value:
            """Maximum in total size of parts to merge, when there are minimum free threads in background pool (or entries
            in replication queue).
            """

        @property
        def max_bytes_to_merge_at_max_space_in_pool(self) -> google.protobuf.wrappers_pb2.Int64Value: ...
        @property
        def min_bytes_for_wide_part(self) -> google.protobuf.wrappers_pb2.Int64Value:
            """Minimum number of bytes in a data part that can be stored in **Wide** format.

            More info see in [ClickHouse documentation](https://clickhouse.com/docs/en/engines/table-engines/mergetree-family/mergetree/#min_bytes_for_wide_part).
            """

        @property
        def min_rows_for_wide_part(self) -> google.protobuf.wrappers_pb2.Int64Value:
            """Minimum number of rows in a data part that can be stored in **Wide** format.

            More info see in [ClickHouse documentation](https://clickhouse.com/docs/en/engines/table-engines/mergetree-family/mergetree/#min_bytes_for_wide_part).
            """

        @property
        def ttl_only_drop_parts(self) -> google.protobuf.wrappers_pb2.BoolValue:
            """Enables or disables complete dropping of data parts where all rows are expired in MergeTree tables.

            More info see in [ClickHouse documentation](https://clickhouse.com/docs/en/operations/settings/settings/#ttl_only_drop_parts).
            """

        @property
        def allow_remote_fs_zero_copy_replication(self) -> google.protobuf.wrappers_pb2.BoolValue: ...
        @property
        def merge_with_ttl_timeout(self) -> google.protobuf.wrappers_pb2.Int64Value: ...
        @property
        def merge_with_recompression_ttl_timeout(self) -> google.protobuf.wrappers_pb2.Int64Value: ...
        @property
        def max_parts_in_total(self) -> google.protobuf.wrappers_pb2.Int64Value: ...
        @property
        def max_number_of_merges_with_ttl_in_pool(self) -> google.protobuf.wrappers_pb2.Int64Value: ...
        @property
        def cleanup_delay_period(self) -> google.protobuf.wrappers_pb2.Int64Value: ...
        @property
        def number_of_free_entries_in_pool_to_execute_mutation(self) -> google.protobuf.wrappers_pb2.Int64Value: ...
        @property
        def max_avg_part_size_for_too_many_parts(self) -> google.protobuf.wrappers_pb2.Int64Value:
            """The 'too many parts' check according to 'parts_to_delay_insert' and 'parts_to_throw_insert' will be active only if the average part size (in the relevant partition) is not larger than the specified threshold. If it is larger than the specified threshold, the INSERTs will be neither delayed or rejected. This allows to have hundreds of terabytes in a single table on a single server if the parts are successfully merged to larger parts. This does not affect the thresholds on inactive parts or total parts.
            Default: 1 GiB
            Min version: 22.10
            See in-depth description in [ClickHouse GitHub](https://github.com/ClickHouse/ClickHouse/blob/f9558345e886876b9132d9c018e357f7fa9b22a3/src/Storages/MergeTree/MergeTreeSettings.h#L80)
            """

        @property
        def min_age_to_force_merge_seconds(self) -> google.protobuf.wrappers_pb2.Int64Value:
            """Merge parts if every part in the range is older than the value of min_age_to_force_merge_seconds.
            Default: 0 - disabled
            Min_version: 22.10
            See in-depth description in [ClickHouse documentation](https://clickhouse.com/docs/en/operations/settings/merge-tree-settings#min_age_to_force_merge_seconds)
            """

        @property
        def min_age_to_force_merge_on_partition_only(self) -> google.protobuf.wrappers_pb2.BoolValue:
            """Whether min_age_to_force_merge_seconds should be applied only on the entire partition and not on subset.
            Default: false
            Min_version: 22.11
            See in-depth description in [ClickHouse documentation](https://clickhouse.com/docs/en/operations/settings/merge-tree-settings#min_age_to_force_merge_seconds)
            """

        @property
        def merge_selecting_sleep_ms(self) -> google.protobuf.wrappers_pb2.Int64Value:
            """Sleep time for merge selecting when no part is selected. A lower setting triggers selecting tasks in background_schedule_pool frequently, which results in a large number of requests to ClickHouse Keeper in large-scale clusters.
            Default: 5000
            Min_version: 21.10
            See in-depth description in [ClickHouse documentation](https://clickhouse.com/docs/en/operations/settings/settings#merge_selecting_sleep_ms)
            """

        def __init__(
            self,
            *,
            replicated_deduplication_window: google.protobuf.wrappers_pb2.Int64Value | None = ...,
            replicated_deduplication_window_seconds: google.protobuf.wrappers_pb2.Int64Value | None = ...,
            parts_to_delay_insert: google.protobuf.wrappers_pb2.Int64Value | None = ...,
            parts_to_throw_insert: google.protobuf.wrappers_pb2.Int64Value | None = ...,
            inactive_parts_to_delay_insert: google.protobuf.wrappers_pb2.Int64Value | None = ...,
            inactive_parts_to_throw_insert: google.protobuf.wrappers_pb2.Int64Value | None = ...,
            max_replicated_merges_in_queue: google.protobuf.wrappers_pb2.Int64Value | None = ...,
            number_of_free_entries_in_pool_to_lower_max_size_of_merge: google.protobuf.wrappers_pb2.Int64Value | None = ...,
            max_bytes_to_merge_at_min_space_in_pool: google.protobuf.wrappers_pb2.Int64Value | None = ...,
            max_bytes_to_merge_at_max_space_in_pool: google.protobuf.wrappers_pb2.Int64Value | None = ...,
            min_bytes_for_wide_part: google.protobuf.wrappers_pb2.Int64Value | None = ...,
            min_rows_for_wide_part: google.protobuf.wrappers_pb2.Int64Value | None = ...,
            ttl_only_drop_parts: google.protobuf.wrappers_pb2.BoolValue | None = ...,
            allow_remote_fs_zero_copy_replication: google.protobuf.wrappers_pb2.BoolValue | None = ...,
            merge_with_ttl_timeout: google.protobuf.wrappers_pb2.Int64Value | None = ...,
            merge_with_recompression_ttl_timeout: google.protobuf.wrappers_pb2.Int64Value | None = ...,
            max_parts_in_total: google.protobuf.wrappers_pb2.Int64Value | None = ...,
            max_number_of_merges_with_ttl_in_pool: google.protobuf.wrappers_pb2.Int64Value | None = ...,
            cleanup_delay_period: google.protobuf.wrappers_pb2.Int64Value | None = ...,
            number_of_free_entries_in_pool_to_execute_mutation: google.protobuf.wrappers_pb2.Int64Value | None = ...,
            max_avg_part_size_for_too_many_parts: google.protobuf.wrappers_pb2.Int64Value | None = ...,
            min_age_to_force_merge_seconds: google.protobuf.wrappers_pb2.Int64Value | None = ...,
            min_age_to_force_merge_on_partition_only: google.protobuf.wrappers_pb2.BoolValue | None = ...,
            merge_selecting_sleep_ms: google.protobuf.wrappers_pb2.Int64Value | None = ...,
        ) -> None: ...
        def HasField(self, field_name: typing.Literal["allow_remote_fs_zero_copy_replication", b"allow_remote_fs_zero_copy_replication", "cleanup_delay_period", b"cleanup_delay_period", "inactive_parts_to_delay_insert", b"inactive_parts_to_delay_insert", "inactive_parts_to_throw_insert", b"inactive_parts_to_throw_insert", "max_avg_part_size_for_too_many_parts", b"max_avg_part_size_for_too_many_parts", "max_bytes_to_merge_at_max_space_in_pool", b"max_bytes_to_merge_at_max_space_in_pool", "max_bytes_to_merge_at_min_space_in_pool", b"max_bytes_to_merge_at_min_space_in_pool", "max_number_of_merges_with_ttl_in_pool", b"max_number_of_merges_with_ttl_in_pool", "max_parts_in_total", b"max_parts_in_total", "max_replicated_merges_in_queue", b"max_replicated_merges_in_queue", "merge_selecting_sleep_ms", b"merge_selecting_sleep_ms", "merge_with_recompression_ttl_timeout", b"merge_with_recompression_ttl_timeout", "merge_with_ttl_timeout", b"merge_with_ttl_timeout", "min_age_to_force_merge_on_partition_only", b"min_age_to_force_merge_on_partition_only", "min_age_to_force_merge_seconds", b"min_age_to_force_merge_seconds", "min_bytes_for_wide_part", b"min_bytes_for_wide_part", "min_rows_for_wide_part", b"min_rows_for_wide_part", "number_of_free_entries_in_pool_to_execute_mutation", b"number_of_free_entries_in_pool_to_execute_mutation", "number_of_free_entries_in_pool_to_lower_max_size_of_merge", b"number_of_free_entries_in_pool_to_lower_max_size_of_merge", "parts_to_delay_insert", b"parts_to_delay_insert", "parts_to_throw_insert", b"parts_to_throw_insert", "replicated_deduplication_window", b"replicated_deduplication_window", "replicated_deduplication_window_seconds", b"replicated_deduplication_window_seconds", "ttl_only_drop_parts", b"ttl_only_drop_parts"]) -> builtins.bool: ...
        def ClearField(self, field_name: typing.Literal["allow_remote_fs_zero_copy_replication", b"allow_remote_fs_zero_copy_replication", "cleanup_delay_period", b"cleanup_delay_period", "inactive_parts_to_delay_insert", b"inactive_parts_to_delay_insert", "inactive_parts_to_throw_insert", b"inactive_parts_to_throw_insert", "max_avg_part_size_for_too_many_parts", b"max_avg_part_size_for_too_many_parts", "max_bytes_to_merge_at_max_space_in_pool", b"max_bytes_to_merge_at_max_space_in_pool", "max_bytes_to_merge_at_min_space_in_pool", b"max_bytes_to_merge_at_min_space_in_pool", "max_number_of_merges_with_ttl_in_pool", b"max_number_of_merges_with_ttl_in_pool", "max_parts_in_total", b"max_parts_in_total", "max_replicated_merges_in_queue", b"max_replicated_merges_in_queue", "merge_selecting_sleep_ms", b"merge_selecting_sleep_ms", "merge_with_recompression_ttl_timeout", b"merge_with_recompression_ttl_timeout", "merge_with_ttl_timeout", b"merge_with_ttl_timeout", "min_age_to_force_merge_on_partition_only", b"min_age_to_force_merge_on_partition_only", "min_age_to_force_merge_seconds", b"min_age_to_force_merge_seconds", "min_bytes_for_wide_part", b"min_bytes_for_wide_part", "min_rows_for_wide_part", b"min_rows_for_wide_part", "number_of_free_entries_in_pool_to_execute_mutation", b"number_of_free_entries_in_pool_to_execute_mutation", "number_of_free_entries_in_pool_to_lower_max_size_of_merge", b"number_of_free_entries_in_pool_to_lower_max_size_of_merge", "parts_to_delay_insert", b"parts_to_delay_insert", "parts_to_throw_insert", b"parts_to_throw_insert", "replicated_deduplication_window", b"replicated_deduplication_window", "replicated_deduplication_window_seconds", b"replicated_deduplication_window_seconds", "ttl_only_drop_parts", b"ttl_only_drop_parts"]) -> None: ...

    @typing.final
    class Kafka(google.protobuf.message.Message):
        DESCRIPTOR: google.protobuf.descriptor.Descriptor

        class _SecurityProtocol:
            ValueType = typing.NewType("ValueType", builtins.int)
            V: typing_extensions.TypeAlias = ValueType

        class _SecurityProtocolEnumTypeWrapper(google.protobuf.internal.enum_type_wrapper._EnumTypeWrapper[ClickhouseConfig.Kafka._SecurityProtocol.ValueType], builtins.type):
            DESCRIPTOR: google.protobuf.descriptor.EnumDescriptor
            SECURITY_PROTOCOL_UNSPECIFIED: ClickhouseConfig.Kafka._SecurityProtocol.ValueType  # 0
            SECURITY_PROTOCOL_PLAINTEXT: ClickhouseConfig.Kafka._SecurityProtocol.ValueType  # 1
            SECURITY_PROTOCOL_SSL: ClickhouseConfig.Kafka._SecurityProtocol.ValueType  # 2
            SECURITY_PROTOCOL_SASL_PLAINTEXT: ClickhouseConfig.Kafka._SecurityProtocol.ValueType  # 3
            SECURITY_PROTOCOL_SASL_SSL: ClickhouseConfig.Kafka._SecurityProtocol.ValueType  # 4

        class SecurityProtocol(_SecurityProtocol, metaclass=_SecurityProtocolEnumTypeWrapper): ...
        SECURITY_PROTOCOL_UNSPECIFIED: ClickhouseConfig.Kafka.SecurityProtocol.ValueType  # 0
        SECURITY_PROTOCOL_PLAINTEXT: ClickhouseConfig.Kafka.SecurityProtocol.ValueType  # 1
        SECURITY_PROTOCOL_SSL: ClickhouseConfig.Kafka.SecurityProtocol.ValueType  # 2
        SECURITY_PROTOCOL_SASL_PLAINTEXT: ClickhouseConfig.Kafka.SecurityProtocol.ValueType  # 3
        SECURITY_PROTOCOL_SASL_SSL: ClickhouseConfig.Kafka.SecurityProtocol.ValueType  # 4

        class _SaslMechanism:
            ValueType = typing.NewType("ValueType", builtins.int)
            V: typing_extensions.TypeAlias = ValueType

        class _SaslMechanismEnumTypeWrapper(google.protobuf.internal.enum_type_wrapper._EnumTypeWrapper[ClickhouseConfig.Kafka._SaslMechanism.ValueType], builtins.type):
            DESCRIPTOR: google.protobuf.descriptor.EnumDescriptor
            SASL_MECHANISM_UNSPECIFIED: ClickhouseConfig.Kafka._SaslMechanism.ValueType  # 0
            SASL_MECHANISM_GSSAPI: ClickhouseConfig.Kafka._SaslMechanism.ValueType  # 1
            SASL_MECHANISM_PLAIN: ClickhouseConfig.Kafka._SaslMechanism.ValueType  # 2
            SASL_MECHANISM_SCRAM_SHA_256: ClickhouseConfig.Kafka._SaslMechanism.ValueType  # 3
            SASL_MECHANISM_SCRAM_SHA_512: ClickhouseConfig.Kafka._SaslMechanism.ValueType  # 4

        class SaslMechanism(_SaslMechanism, metaclass=_SaslMechanismEnumTypeWrapper): ...
        SASL_MECHANISM_UNSPECIFIED: ClickhouseConfig.Kafka.SaslMechanism.ValueType  # 0
        SASL_MECHANISM_GSSAPI: ClickhouseConfig.Kafka.SaslMechanism.ValueType  # 1
        SASL_MECHANISM_PLAIN: ClickhouseConfig.Kafka.SaslMechanism.ValueType  # 2
        SASL_MECHANISM_SCRAM_SHA_256: ClickhouseConfig.Kafka.SaslMechanism.ValueType  # 3
        SASL_MECHANISM_SCRAM_SHA_512: ClickhouseConfig.Kafka.SaslMechanism.ValueType  # 4

        SECURITY_PROTOCOL_FIELD_NUMBER: builtins.int
        SASL_MECHANISM_FIELD_NUMBER: builtins.int
        SASL_USERNAME_FIELD_NUMBER: builtins.int
        SASL_PASSWORD_FIELD_NUMBER: builtins.int
        ENABLE_SSL_CERTIFICATE_VERIFICATION_FIELD_NUMBER: builtins.int
        MAX_POLL_INTERVAL_MS_FIELD_NUMBER: builtins.int
        SESSION_TIMEOUT_MS_FIELD_NUMBER: builtins.int
        security_protocol: global___ClickhouseConfig.Kafka.SecurityProtocol.ValueType
        sasl_mechanism: global___ClickhouseConfig.Kafka.SaslMechanism.ValueType
        sasl_username: builtins.str
        sasl_password: builtins.str
        @property
        def enable_ssl_certificate_verification(self) -> google.protobuf.wrappers_pb2.BoolValue: ...
        @property
        def max_poll_interval_ms(self) -> google.protobuf.wrappers_pb2.Int64Value: ...
        @property
        def session_timeout_ms(self) -> google.protobuf.wrappers_pb2.Int64Value: ...
        def __init__(
            self,
            *,
            security_protocol: global___ClickhouseConfig.Kafka.SecurityProtocol.ValueType = ...,
            sasl_mechanism: global___ClickhouseConfig.Kafka.SaslMechanism.ValueType = ...,
            sasl_username: builtins.str = ...,
            sasl_password: builtins.str = ...,
            enable_ssl_certificate_verification: google.protobuf.wrappers_pb2.BoolValue | None = ...,
            max_poll_interval_ms: google.protobuf.wrappers_pb2.Int64Value | None = ...,
            session_timeout_ms: google.protobuf.wrappers_pb2.Int64Value | None = ...,
        ) -> None: ...
        def HasField(self, field_name: typing.Literal["enable_ssl_certificate_verification", b"enable_ssl_certificate_verification", "max_poll_interval_ms", b"max_poll_interval_ms", "session_timeout_ms", b"session_timeout_ms"]) -> builtins.bool: ...
        def ClearField(self, field_name: typing.Literal["enable_ssl_certificate_verification", b"enable_ssl_certificate_verification", "max_poll_interval_ms", b"max_poll_interval_ms", "sasl_mechanism", b"sasl_mechanism", "sasl_password", b"sasl_password", "sasl_username", b"sasl_username", "security_protocol", b"security_protocol", "session_timeout_ms", b"session_timeout_ms"]) -> None: ...

    @typing.final
    class KafkaTopic(google.protobuf.message.Message):
        DESCRIPTOR: google.protobuf.descriptor.Descriptor

        NAME_FIELD_NUMBER: builtins.int
        SETTINGS_FIELD_NUMBER: builtins.int
        name: builtins.str
        @property
        def settings(self) -> global___ClickhouseConfig.Kafka: ...
        def __init__(
            self,
            *,
            name: builtins.str = ...,
            settings: global___ClickhouseConfig.Kafka | None = ...,
        ) -> None: ...
        def HasField(self, field_name: typing.Literal["settings", b"settings"]) -> builtins.bool: ...
        def ClearField(self, field_name: typing.Literal["name", b"name", "settings", b"settings"]) -> None: ...

    @typing.final
    class Rabbitmq(google.protobuf.message.Message):
        DESCRIPTOR: google.protobuf.descriptor.Descriptor

        USERNAME_FIELD_NUMBER: builtins.int
        PASSWORD_FIELD_NUMBER: builtins.int
        VHOST_FIELD_NUMBER: builtins.int
        username: builtins.str
        """[RabbitMQ](https://clickhouse.com/docs/en/engines/table-engines/integrations/rabbitmq/) username"""
        password: builtins.str
        """[RabbitMQ](https://clickhouse.com/docs/en/engines/table-engines/integrations/rabbitmq/) password"""
        vhost: builtins.str
        """[RabbitMQ](https://clickhouse.com/docs/en/engines/table-engines/integrations/rabbitmq/) virtual host"""
        def __init__(
            self,
            *,
            username: builtins.str = ...,
            password: builtins.str = ...,
            vhost: builtins.str = ...,
        ) -> None: ...
        def ClearField(self, field_name: typing.Literal["password", b"password", "username", b"username", "vhost", b"vhost"]) -> None: ...

    @typing.final
    class Compression(google.protobuf.message.Message):
        DESCRIPTOR: google.protobuf.descriptor.Descriptor

        class _Method:
            ValueType = typing.NewType("ValueType", builtins.int)
            V: typing_extensions.TypeAlias = ValueType

        class _MethodEnumTypeWrapper(google.protobuf.internal.enum_type_wrapper._EnumTypeWrapper[ClickhouseConfig.Compression._Method.ValueType], builtins.type):
            DESCRIPTOR: google.protobuf.descriptor.EnumDescriptor
            METHOD_UNSPECIFIED: ClickhouseConfig.Compression._Method.ValueType  # 0
            LZ4: ClickhouseConfig.Compression._Method.ValueType  # 1
            """[LZ4 compression algorithm](https://lz4.github.io/lz4/)."""
            ZSTD: ClickhouseConfig.Compression._Method.ValueType  # 2
            """[Zstandard compression algorithm](https://facebook.github.io/zstd/)."""

        class Method(_Method, metaclass=_MethodEnumTypeWrapper): ...
        METHOD_UNSPECIFIED: ClickhouseConfig.Compression.Method.ValueType  # 0
        LZ4: ClickhouseConfig.Compression.Method.ValueType  # 1
        """[LZ4 compression algorithm](https://lz4.github.io/lz4/)."""
        ZSTD: ClickhouseConfig.Compression.Method.ValueType  # 2
        """[Zstandard compression algorithm](https://facebook.github.io/zstd/)."""

        METHOD_FIELD_NUMBER: builtins.int
        MIN_PART_SIZE_FIELD_NUMBER: builtins.int
        MIN_PART_SIZE_RATIO_FIELD_NUMBER: builtins.int
        LEVEL_FIELD_NUMBER: builtins.int
        method: global___ClickhouseConfig.Compression.Method.ValueType
        """Compression method to use for the specified combination of [min_part_size] and [min_part_size_ratio]."""
        min_part_size: builtins.int
        """Minimum size of a part of a table."""
        min_part_size_ratio: builtins.float
        """Minimum ratio of a part relative to the size of all the data in the table."""
        @property
        def level(self) -> google.protobuf.wrappers_pb2.Int64Value: ...
        def __init__(
            self,
            *,
            method: global___ClickhouseConfig.Compression.Method.ValueType = ...,
            min_part_size: builtins.int = ...,
            min_part_size_ratio: builtins.float = ...,
            level: google.protobuf.wrappers_pb2.Int64Value | None = ...,
        ) -> None: ...
        def HasField(self, field_name: typing.Literal["level", b"level"]) -> builtins.bool: ...
        def ClearField(self, field_name: typing.Literal["level", b"level", "method", b"method", "min_part_size", b"min_part_size", "min_part_size_ratio", b"min_part_size_ratio"]) -> None: ...

    @typing.final
    class ExternalDictionary(google.protobuf.message.Message):
        DESCRIPTOR: google.protobuf.descriptor.Descriptor

        @typing.final
        class HttpSource(google.protobuf.message.Message):
            DESCRIPTOR: google.protobuf.descriptor.Descriptor

            URL_FIELD_NUMBER: builtins.int
            FORMAT_FIELD_NUMBER: builtins.int
            url: builtins.str
            """URL of the source dictionary available over HTTP."""
            format: builtins.str
            """The data format. Valid values are all formats supported by ClickHouse SQL dialect."""
            def __init__(
                self,
                *,
                url: builtins.str = ...,
                format: builtins.str = ...,
            ) -> None: ...
            def ClearField(self, field_name: typing.Literal["format", b"format", "url", b"url"]) -> None: ...

        @typing.final
        class MysqlSource(google.protobuf.message.Message):
            DESCRIPTOR: google.protobuf.descriptor.Descriptor

            @typing.final
            class Replica(google.protobuf.message.Message):
                DESCRIPTOR: google.protobuf.descriptor.Descriptor

                HOST_FIELD_NUMBER: builtins.int
                PRIORITY_FIELD_NUMBER: builtins.int
                PORT_FIELD_NUMBER: builtins.int
                USER_FIELD_NUMBER: builtins.int
                PASSWORD_FIELD_NUMBER: builtins.int
                host: builtins.str
                """MySQL host of the replica."""
                priority: builtins.int
                """The priority of the replica that ClickHouse takes into account when connecting.
                Replica with the highest priority should have this field set to the lowest number.
                """
                port: builtins.int
                """Port to use when connecting to the replica.
                If a port is not specified for a replica, ClickHouse uses the port specified for the source.
                """
                user: builtins.str
                """Name of the MySQL database user."""
                password: builtins.str
                """Password of the MySQL database user."""
                def __init__(
                    self,
                    *,
                    host: builtins.str = ...,
                    priority: builtins.int = ...,
                    port: builtins.int = ...,
                    user: builtins.str = ...,
                    password: builtins.str = ...,
                ) -> None: ...
                def ClearField(self, field_name: typing.Literal["host", b"host", "password", b"password", "port", b"port", "priority", b"priority", "user", b"user"]) -> None: ...

            DB_FIELD_NUMBER: builtins.int
            TABLE_FIELD_NUMBER: builtins.int
            PORT_FIELD_NUMBER: builtins.int
            USER_FIELD_NUMBER: builtins.int
            PASSWORD_FIELD_NUMBER: builtins.int
            REPLICAS_FIELD_NUMBER: builtins.int
            WHERE_FIELD_NUMBER: builtins.int
            INVALIDATE_QUERY_FIELD_NUMBER: builtins.int
            db: builtins.str
            """Name of the MySQL database to connect to."""
            table: builtins.str
            """Name of the database table to use as a ClickHouse dictionary."""
            port: builtins.int
            """Default port to use when connecting to a replica of the dictionary source."""
            user: builtins.str
            """Name of the default user for replicas of the dictionary source."""
            password: builtins.str
            """Password of the default user for replicas of the dictionary source."""
            where: builtins.str
            """Selection criteria for the data in the specified MySQL table."""
            invalidate_query: builtins.str
            """Query for checking the dictionary status, to pull only updated data.
            For more details, see [ClickHouse documentation on dictionaries](https://clickhouse.com/docs/en/query_language/dicts/external_dicts_dict_lifetime/).
            """
            @property
            def replicas(self) -> google.protobuf.internal.containers.RepeatedCompositeFieldContainer[global___ClickhouseConfig.ExternalDictionary.MysqlSource.Replica]:
                """List of MySQL replicas of the database used as dictionary source."""

            def __init__(
                self,
                *,
                db: builtins.str = ...,
                table: builtins.str = ...,
                port: builtins.int = ...,
                user: builtins.str = ...,
                password: builtins.str = ...,
                replicas: collections.abc.Iterable[global___ClickhouseConfig.ExternalDictionary.MysqlSource.Replica] | None = ...,
                where: builtins.str = ...,
                invalidate_query: builtins.str = ...,
            ) -> None: ...
            def ClearField(self, field_name: typing.Literal["db", b"db", "invalidate_query", b"invalidate_query", "password", b"password", "port", b"port", "replicas", b"replicas", "table", b"table", "user", b"user", "where", b"where"]) -> None: ...

        @typing.final
        class ClickhouseSource(google.protobuf.message.Message):
            DESCRIPTOR: google.protobuf.descriptor.Descriptor

            DB_FIELD_NUMBER: builtins.int
            TABLE_FIELD_NUMBER: builtins.int
            HOST_FIELD_NUMBER: builtins.int
            PORT_FIELD_NUMBER: builtins.int
            USER_FIELD_NUMBER: builtins.int
            PASSWORD_FIELD_NUMBER: builtins.int
            WHERE_FIELD_NUMBER: builtins.int
            db: builtins.str
            """Name of the ClickHouse database."""
            table: builtins.str
            """Name of the table in the specified database to be used as the dictionary source."""
            host: builtins.str
            """ClickHouse host of the specified database."""
            port: builtins.int
            """Port to use when connecting to the host."""
            user: builtins.str
            """Name of the ClickHouse database user."""
            password: builtins.str
            """Password of the ClickHouse database user."""
            where: builtins.str
            """Selection criteria for the data in the specified ClickHouse table."""
            def __init__(
                self,
                *,
                db: builtins.str = ...,
                table: builtins.str = ...,
                host: builtins.str = ...,
                port: builtins.int = ...,
                user: builtins.str = ...,
                password: builtins.str = ...,
                where: builtins.str = ...,
            ) -> None: ...
            def ClearField(self, field_name: typing.Literal["db", b"db", "host", b"host", "password", b"password", "port", b"port", "table", b"table", "user", b"user", "where", b"where"]) -> None: ...

        @typing.final
        class MongodbSource(google.protobuf.message.Message):
            DESCRIPTOR: google.protobuf.descriptor.Descriptor

            DB_FIELD_NUMBER: builtins.int
            COLLECTION_FIELD_NUMBER: builtins.int
            HOST_FIELD_NUMBER: builtins.int
            PORT_FIELD_NUMBER: builtins.int
            USER_FIELD_NUMBER: builtins.int
            PASSWORD_FIELD_NUMBER: builtins.int
            OPTIONS_FIELD_NUMBER: builtins.int
            db: builtins.str
            """Name of the MongoDB database."""
            collection: builtins.str
            """Name of the collection in the specified database to be used as the dictionary source."""
            host: builtins.str
            """MongoDB host of the specified database."""
            port: builtins.int
            """Port to use when connecting to the host."""
            user: builtins.str
            """Name of the MongoDB database user."""
            password: builtins.str
            """Password of the MongoDB database user."""
            options: builtins.str
            def __init__(
                self,
                *,
                db: builtins.str = ...,
                collection: builtins.str = ...,
                host: builtins.str = ...,
                port: builtins.int = ...,
                user: builtins.str = ...,
                password: builtins.str = ...,
                options: builtins.str = ...,
            ) -> None: ...
            def ClearField(self, field_name: typing.Literal["collection", b"collection", "db", b"db", "host", b"host", "options", b"options", "password", b"password", "port", b"port", "user", b"user"]) -> None: ...

        @typing.final
        class PostgresqlSource(google.protobuf.message.Message):
            DESCRIPTOR: google.protobuf.descriptor.Descriptor

            class _SslMode:
                ValueType = typing.NewType("ValueType", builtins.int)
                V: typing_extensions.TypeAlias = ValueType

            class _SslModeEnumTypeWrapper(google.protobuf.internal.enum_type_wrapper._EnumTypeWrapper[ClickhouseConfig.ExternalDictionary.PostgresqlSource._SslMode.ValueType], builtins.type):
                DESCRIPTOR: google.protobuf.descriptor.EnumDescriptor
                SSL_MODE_UNSPECIFIED: ClickhouseConfig.ExternalDictionary.PostgresqlSource._SslMode.ValueType  # 0
                DISABLE: ClickhouseConfig.ExternalDictionary.PostgresqlSource._SslMode.ValueType  # 1
                """Only try a non-SSL connection."""
                ALLOW: ClickhouseConfig.ExternalDictionary.PostgresqlSource._SslMode.ValueType  # 2
                """First try a non-SSL connection; if that fails, try an SSL connection."""
                PREFER: ClickhouseConfig.ExternalDictionary.PostgresqlSource._SslMode.ValueType  # 3
                """First try an SSL connection; if that fails, try a non-SSL connection."""
                VERIFY_CA: ClickhouseConfig.ExternalDictionary.PostgresqlSource._SslMode.ValueType  # 4
                """Only try an SSL connection, and verify that the server certificate is issued by a trusted certificate authority (CA)."""
                VERIFY_FULL: ClickhouseConfig.ExternalDictionary.PostgresqlSource._SslMode.ValueType  # 5
                """Only try an SSL connection, verify that the server certificate is issued by a trusted CA and that the requested server host name matches that in the certificate."""

            class SslMode(_SslMode, metaclass=_SslModeEnumTypeWrapper): ...
            SSL_MODE_UNSPECIFIED: ClickhouseConfig.ExternalDictionary.PostgresqlSource.SslMode.ValueType  # 0
            DISABLE: ClickhouseConfig.ExternalDictionary.PostgresqlSource.SslMode.ValueType  # 1
            """Only try a non-SSL connection."""
            ALLOW: ClickhouseConfig.ExternalDictionary.PostgresqlSource.SslMode.ValueType  # 2
            """First try a non-SSL connection; if that fails, try an SSL connection."""
            PREFER: ClickhouseConfig.ExternalDictionary.PostgresqlSource.SslMode.ValueType  # 3
            """First try an SSL connection; if that fails, try a non-SSL connection."""
            VERIFY_CA: ClickhouseConfig.ExternalDictionary.PostgresqlSource.SslMode.ValueType  # 4
            """Only try an SSL connection, and verify that the server certificate is issued by a trusted certificate authority (CA)."""
            VERIFY_FULL: ClickhouseConfig.ExternalDictionary.PostgresqlSource.SslMode.ValueType  # 5
            """Only try an SSL connection, verify that the server certificate is issued by a trusted CA and that the requested server host name matches that in the certificate."""

            DB_FIELD_NUMBER: builtins.int
            TABLE_FIELD_NUMBER: builtins.int
            HOSTS_FIELD_NUMBER: builtins.int
            PORT_FIELD_NUMBER: builtins.int
            USER_FIELD_NUMBER: builtins.int
            PASSWORD_FIELD_NUMBER: builtins.int
            INVALIDATE_QUERY_FIELD_NUMBER: builtins.int
            SSL_MODE_FIELD_NUMBER: builtins.int
            db: builtins.str
            """Name of the PostrgreSQL database."""
            table: builtins.str
            """Name of the table in the specified database to be used as the dictionary source."""
            port: builtins.int
            """Port to use when connecting to the host."""
            user: builtins.str
            """Name of the PostrgreSQL database user."""
            password: builtins.str
            """Password of the PostrgreSQL database user."""
            invalidate_query: builtins.str
            """Query for checking the dictionary status, to pull only updated data.
            For more details, see [ClickHouse documentation on dictionaries](https://clickhouse.com/docs/en/query_language/dicts/external_dicts_dict_lifetime/).
            """
            ssl_mode: global___ClickhouseConfig.ExternalDictionary.PostgresqlSource.SslMode.ValueType
            """Mode of SSL TCP/IP connection to the PostgreSQL host.
            For more details, see [PostgreSQL documentation](https://www.postgresql.org/docs/current/libpq-ssl.html).
            """
            @property
            def hosts(self) -> google.protobuf.internal.containers.RepeatedScalarFieldContainer[builtins.str]:
                """Name of the PostrgreSQL host"""

            def __init__(
                self,
                *,
                db: builtins.str = ...,
                table: builtins.str = ...,
                hosts: collections.abc.Iterable[builtins.str] | None = ...,
                port: builtins.int = ...,
                user: builtins.str = ...,
                password: builtins.str = ...,
                invalidate_query: builtins.str = ...,
                ssl_mode: global___ClickhouseConfig.ExternalDictionary.PostgresqlSource.SslMode.ValueType = ...,
            ) -> None: ...
            def ClearField(self, field_name: typing.Literal["db", b"db", "hosts", b"hosts", "invalidate_query", b"invalidate_query", "password", b"password", "port", b"port", "ssl_mode", b"ssl_mode", "table", b"table", "user", b"user"]) -> None: ...

        @typing.final
        class Structure(google.protobuf.message.Message):
            DESCRIPTOR: google.protobuf.descriptor.Descriptor

            @typing.final
            class Attribute(google.protobuf.message.Message):
                DESCRIPTOR: google.protobuf.descriptor.Descriptor

                NAME_FIELD_NUMBER: builtins.int
                TYPE_FIELD_NUMBER: builtins.int
                NULL_VALUE_FIELD_NUMBER: builtins.int
                EXPRESSION_FIELD_NUMBER: builtins.int
                HIERARCHICAL_FIELD_NUMBER: builtins.int
                INJECTIVE_FIELD_NUMBER: builtins.int
                name: builtins.str
                """Name of the column."""
                type: builtins.str
                """Type of the column."""
                null_value: builtins.str
                """Default value for an element without data (for example, an empty string)."""
                expression: builtins.str
                """Expression, describing the attribute, if applicable."""
                hierarchical: builtins.bool
                """Indication of hierarchy support.
                Default value: `false`.
                """
                injective: builtins.bool
                """Indication of injective mapping "id -> attribute".
                Default value: `false`.
                """
                def __init__(
                    self,
                    *,
                    name: builtins.str = ...,
                    type: builtins.str = ...,
                    null_value: builtins.str = ...,
                    expression: builtins.str = ...,
                    hierarchical: builtins.bool = ...,
                    injective: builtins.bool = ...,
                ) -> None: ...
                def ClearField(self, field_name: typing.Literal["expression", b"expression", "hierarchical", b"hierarchical", "injective", b"injective", "name", b"name", "null_value", b"null_value", "type", b"type"]) -> None: ...

            @typing.final
            class Id(google.protobuf.message.Message):
                """Numeric key."""

                DESCRIPTOR: google.protobuf.descriptor.Descriptor

                NAME_FIELD_NUMBER: builtins.int
                name: builtins.str
                """Name of the numeric key."""
                def __init__(
                    self,
                    *,
                    name: builtins.str = ...,
                ) -> None: ...
                def ClearField(self, field_name: typing.Literal["name", b"name"]) -> None: ...

            @typing.final
            class Key(google.protobuf.message.Message):
                """Complex key."""

                DESCRIPTOR: google.protobuf.descriptor.Descriptor

                ATTRIBUTES_FIELD_NUMBER: builtins.int
                @property
                def attributes(self) -> google.protobuf.internal.containers.RepeatedCompositeFieldContainer[global___ClickhouseConfig.ExternalDictionary.Structure.Attribute]:
                    """Attributes of a complex key."""

                def __init__(
                    self,
                    *,
                    attributes: collections.abc.Iterable[global___ClickhouseConfig.ExternalDictionary.Structure.Attribute] | None = ...,
                ) -> None: ...
                def ClearField(self, field_name: typing.Literal["attributes", b"attributes"]) -> None: ...

            ID_FIELD_NUMBER: builtins.int
            KEY_FIELD_NUMBER: builtins.int
            RANGE_MIN_FIELD_NUMBER: builtins.int
            RANGE_MAX_FIELD_NUMBER: builtins.int
            ATTRIBUTES_FIELD_NUMBER: builtins.int
            @property
            def id(self) -> global___ClickhouseConfig.ExternalDictionary.Structure.Id:
                """Single numeric key column for the dictionary."""

            @property
            def key(self) -> global___ClickhouseConfig.ExternalDictionary.Structure.Key:
                """Composite key for the dictionary, containing of one or more key columns.
                For details, see [ClickHouse documentation](https://clickhouse.com/docs/en/query_language/dicts/external_dicts_dict_structure/#composite-key).
                """

            @property
            def range_min(self) -> global___ClickhouseConfig.ExternalDictionary.Structure.Attribute:
                """Field holding the beginning of the range for dictionaries with `RANGE_HASHED` layout.
                For details, see [ClickHouse documentation](https://clickhouse.com/docs/en/query_language/dicts/external_dicts_dict_layout/#range-hashed).
                """

            @property
            def range_max(self) -> global___ClickhouseConfig.ExternalDictionary.Structure.Attribute:
                """Field holding the end of the range for dictionaries with `RANGE_HASHED` layout.
                For details, see [ClickHouse documentation](https://clickhouse.com/docs/en/query_language/dicts/external_dicts_dict_layout/#range-hashed).
                """

            @property
            def attributes(self) -> google.protobuf.internal.containers.RepeatedCompositeFieldContainer[global___ClickhouseConfig.ExternalDictionary.Structure.Attribute]:
                """Description of the fields available for database queries.
                For details, see [ClickHouse documentation](https://clickhouse.com/docs/en/query_language/dicts/external_dicts_dict_structure/#attributes).
                """

            def __init__(
                self,
                *,
                id: global___ClickhouseConfig.ExternalDictionary.Structure.Id | None = ...,
                key: global___ClickhouseConfig.ExternalDictionary.Structure.Key | None = ...,
                range_min: global___ClickhouseConfig.ExternalDictionary.Structure.Attribute | None = ...,
                range_max: global___ClickhouseConfig.ExternalDictionary.Structure.Attribute | None = ...,
                attributes: collections.abc.Iterable[global___ClickhouseConfig.ExternalDictionary.Structure.Attribute] | None = ...,
            ) -> None: ...
            def HasField(self, field_name: typing.Literal["id", b"id", "key", b"key", "range_max", b"range_max", "range_min", b"range_min"]) -> builtins.bool: ...
            def ClearField(self, field_name: typing.Literal["attributes", b"attributes", "id", b"id", "key", b"key", "range_max", b"range_max", "range_min", b"range_min"]) -> None: ...

        @typing.final
        class Layout(google.protobuf.message.Message):
            """Layout determining how to store the dictionary in memory."""

            DESCRIPTOR: google.protobuf.descriptor.Descriptor

            class _Type:
                ValueType = typing.NewType("ValueType", builtins.int)
                V: typing_extensions.TypeAlias = ValueType

            class _TypeEnumTypeWrapper(google.protobuf.internal.enum_type_wrapper._EnumTypeWrapper[ClickhouseConfig.ExternalDictionary.Layout._Type.ValueType], builtins.type):
                DESCRIPTOR: google.protobuf.descriptor.EnumDescriptor
                TYPE_UNSPECIFIED: ClickhouseConfig.ExternalDictionary.Layout._Type.ValueType  # 0
                FLAT: ClickhouseConfig.ExternalDictionary.Layout._Type.ValueType  # 1
                """The entire dictionary is stored in memory in the form of flat arrays.
                Available for all dictionary sources.
                """
                HASHED: ClickhouseConfig.ExternalDictionary.Layout._Type.ValueType  # 2
                """The entire dictionary is stored in memory in the form of a hash table.
                Available for all dictionary sources.
                """
                COMPLEX_KEY_HASHED: ClickhouseConfig.ExternalDictionary.Layout._Type.ValueType  # 3
                """Similar to HASHED, to be used with composite keys.
                Available for all dictionary sources.
                """
                RANGE_HASHED: ClickhouseConfig.ExternalDictionary.Layout._Type.ValueType  # 4
                """The entire dictionary is stored in memory in the form of a hash table,
                with an ordered array of ranges and their corresponding values.
                Available for all dictionary sources.
                """
                CACHE: ClickhouseConfig.ExternalDictionary.Layout._Type.ValueType  # 5
                """The dictionary is stored in a cache with a set number of cells.
                Available for MySQL, ClickHouse and HTTP dictionary sources.
                """
                COMPLEX_KEY_CACHE: ClickhouseConfig.ExternalDictionary.Layout._Type.ValueType  # 6
                """Similar to CACHE, to be used with composite keys.
                Available for MySQL, ClickHouse and HTTP dictionary sources.
                """

            class Type(_Type, metaclass=_TypeEnumTypeWrapper): ...
            TYPE_UNSPECIFIED: ClickhouseConfig.ExternalDictionary.Layout.Type.ValueType  # 0
            FLAT: ClickhouseConfig.ExternalDictionary.Layout.Type.ValueType  # 1
            """The entire dictionary is stored in memory in the form of flat arrays.
            Available for all dictionary sources.
            """
            HASHED: ClickhouseConfig.ExternalDictionary.Layout.Type.ValueType  # 2
            """The entire dictionary is stored in memory in the form of a hash table.
            Available for all dictionary sources.
            """
            COMPLEX_KEY_HASHED: ClickhouseConfig.ExternalDictionary.Layout.Type.ValueType  # 3
            """Similar to HASHED, to be used with composite keys.
            Available for all dictionary sources.
            """
            RANGE_HASHED: ClickhouseConfig.ExternalDictionary.Layout.Type.ValueType  # 4
            """The entire dictionary is stored in memory in the form of a hash table,
            with an ordered array of ranges and their corresponding values.
            Available for all dictionary sources.
            """
            CACHE: ClickhouseConfig.ExternalDictionary.Layout.Type.ValueType  # 5
            """The dictionary is stored in a cache with a set number of cells.
            Available for MySQL, ClickHouse and HTTP dictionary sources.
            """
            COMPLEX_KEY_CACHE: ClickhouseConfig.ExternalDictionary.Layout.Type.ValueType  # 6
            """Similar to CACHE, to be used with composite keys.
            Available for MySQL, ClickHouse and HTTP dictionary sources.
            """

            TYPE_FIELD_NUMBER: builtins.int
            SIZE_IN_CELLS_FIELD_NUMBER: builtins.int
            type: global___ClickhouseConfig.ExternalDictionary.Layout.Type.ValueType
            """Layout type for an external dictionary."""
            size_in_cells: builtins.int
            """Number of cells in the cache. Rounded up to a power of two.
            Applicable only for CACHE and COMPLEX_KEY_CACHE layout types.
            """
            def __init__(
                self,
                *,
                type: global___ClickhouseConfig.ExternalDictionary.Layout.Type.ValueType = ...,
                size_in_cells: builtins.int = ...,
            ) -> None: ...
            def ClearField(self, field_name: typing.Literal["size_in_cells", b"size_in_cells", "type", b"type"]) -> None: ...

        @typing.final
        class Range(google.protobuf.message.Message):
            DESCRIPTOR: google.protobuf.descriptor.Descriptor

            MIN_FIELD_NUMBER: builtins.int
            MAX_FIELD_NUMBER: builtins.int
            min: builtins.int
            """Minimum dictionary lifetime."""
            max: builtins.int
            """Maximum dictionary lifetime."""
            def __init__(
                self,
                *,
                min: builtins.int = ...,
                max: builtins.int = ...,
            ) -> None: ...
            def ClearField(self, field_name: typing.Literal["max", b"max", "min", b"min"]) -> None: ...

        NAME_FIELD_NUMBER: builtins.int
        STRUCTURE_FIELD_NUMBER: builtins.int
        LAYOUT_FIELD_NUMBER: builtins.int
        FIXED_LIFETIME_FIELD_NUMBER: builtins.int
        LIFETIME_RANGE_FIELD_NUMBER: builtins.int
        HTTP_SOURCE_FIELD_NUMBER: builtins.int
        MYSQL_SOURCE_FIELD_NUMBER: builtins.int
        CLICKHOUSE_SOURCE_FIELD_NUMBER: builtins.int
        MONGODB_SOURCE_FIELD_NUMBER: builtins.int
        POSTGRESQL_SOURCE_FIELD_NUMBER: builtins.int
        name: builtins.str
        """Name of the external dictionary."""
        fixed_lifetime: builtins.int
        """Fixed interval between dictionary updates."""
        @property
        def structure(self) -> global___ClickhouseConfig.ExternalDictionary.Structure:
            """Set of attributes for the external dictionary.
            For in-depth description, see [ClickHouse documentation](https://clickhouse.com/docs/en/query_language/dicts/external_dicts_dict_structure/).
            """

        @property
        def layout(self) -> global___ClickhouseConfig.ExternalDictionary.Layout:
            """Layout for storing the dictionary in memory.
            For in-depth description, see [ClickHouse documentation](https://clickhouse.com/docs/en/query_language/dicts/external_dicts_dict_layout/).
            """

        @property
        def lifetime_range(self) -> global___ClickhouseConfig.ExternalDictionary.Range:
            """Range of intervals between dictionary updates for ClickHouse to choose from."""

        @property
        def http_source(self) -> global___ClickhouseConfig.ExternalDictionary.HttpSource:
            """HTTP source for the dictionary."""

        @property
        def mysql_source(self) -> global___ClickhouseConfig.ExternalDictionary.MysqlSource:
            """MySQL source for the dictionary."""

        @property
        def clickhouse_source(self) -> global___ClickhouseConfig.ExternalDictionary.ClickhouseSource:
            """ClickHouse source for the dictionary."""

        @property
        def mongodb_source(self) -> global___ClickhouseConfig.ExternalDictionary.MongodbSource:
            """MongoDB source for the dictionary."""

        @property
        def postgresql_source(self) -> global___ClickhouseConfig.ExternalDictionary.PostgresqlSource:
            """PostgreSQL source for the dictionary."""

        def __init__(
            self,
            *,
            name: builtins.str = ...,
            structure: global___ClickhouseConfig.ExternalDictionary.Structure | None = ...,
            layout: global___ClickhouseConfig.ExternalDictionary.Layout | None = ...,
            fixed_lifetime: builtins.int = ...,
            lifetime_range: global___ClickhouseConfig.ExternalDictionary.Range | None = ...,
            http_source: global___ClickhouseConfig.ExternalDictionary.HttpSource | None = ...,
            mysql_source: global___ClickhouseConfig.ExternalDictionary.MysqlSource | None = ...,
            clickhouse_source: global___ClickhouseConfig.ExternalDictionary.ClickhouseSource | None = ...,
            mongodb_source: global___ClickhouseConfig.ExternalDictionary.MongodbSource | None = ...,
            postgresql_source: global___ClickhouseConfig.ExternalDictionary.PostgresqlSource | None = ...,
        ) -> None: ...
        def HasField(self, field_name: typing.Literal["clickhouse_source", b"clickhouse_source", "fixed_lifetime", b"fixed_lifetime", "http_source", b"http_source", "layout", b"layout", "lifetime", b"lifetime", "lifetime_range", b"lifetime_range", "mongodb_source", b"mongodb_source", "mysql_source", b"mysql_source", "postgresql_source", b"postgresql_source", "source", b"source", "structure", b"structure"]) -> builtins.bool: ...
        def ClearField(self, field_name: typing.Literal["clickhouse_source", b"clickhouse_source", "fixed_lifetime", b"fixed_lifetime", "http_source", b"http_source", "layout", b"layout", "lifetime", b"lifetime", "lifetime_range", b"lifetime_range", "mongodb_source", b"mongodb_source", "mysql_source", b"mysql_source", "name", b"name", "postgresql_source", b"postgresql_source", "source", b"source", "structure", b"structure"]) -> None: ...
        @typing.overload
        def WhichOneof(self, oneof_group: typing.Literal["lifetime", b"lifetime"]) -> typing.Literal["fixed_lifetime", "lifetime_range"] | None: ...
        @typing.overload
        def WhichOneof(self, oneof_group: typing.Literal["source", b"source"]) -> typing.Literal["http_source", "mysql_source", "clickhouse_source", "mongodb_source", "postgresql_source"] | None: ...

    @typing.final
    class GraphiteRollup(google.protobuf.message.Message):
        """Rollup settings for the GraphiteMergeTree table engine."""

        DESCRIPTOR: google.protobuf.descriptor.Descriptor

        @typing.final
        class Pattern(google.protobuf.message.Message):
            DESCRIPTOR: google.protobuf.descriptor.Descriptor

            @typing.final
            class Retention(google.protobuf.message.Message):
                DESCRIPTOR: google.protobuf.descriptor.Descriptor

                AGE_FIELD_NUMBER: builtins.int
                PRECISION_FIELD_NUMBER: builtins.int
                age: builtins.int
                """Minimum age of the data in seconds."""
                precision: builtins.int
                """Precision of determining the age of the data, in seconds."""
                def __init__(
                    self,
                    *,
                    age: builtins.int = ...,
                    precision: builtins.int = ...,
                ) -> None: ...
                def ClearField(self, field_name: typing.Literal["age", b"age", "precision", b"precision"]) -> None: ...

            REGEXP_FIELD_NUMBER: builtins.int
            FUNCTION_FIELD_NUMBER: builtins.int
            RETENTION_FIELD_NUMBER: builtins.int
            regexp: builtins.str
            """Pattern for metric names."""
            function: builtins.str
            """Name of the aggregating function to apply to data of the age specified in [retention]."""
            @property
            def retention(self) -> google.protobuf.internal.containers.RepeatedCompositeFieldContainer[global___ClickhouseConfig.GraphiteRollup.Pattern.Retention]:
                """Age of data to use for thinning."""

            def __init__(
                self,
                *,
                regexp: builtins.str = ...,
                function: builtins.str = ...,
                retention: collections.abc.Iterable[global___ClickhouseConfig.GraphiteRollup.Pattern.Retention] | None = ...,
            ) -> None: ...
            def ClearField(self, field_name: typing.Literal["function", b"function", "regexp", b"regexp", "retention", b"retention"]) -> None: ...

        NAME_FIELD_NUMBER: builtins.int
        PATTERNS_FIELD_NUMBER: builtins.int
        name: builtins.str
        """Name for the specified combination of settings for Graphite rollup."""
        @property
        def patterns(self) -> google.protobuf.internal.containers.RepeatedCompositeFieldContainer[global___ClickhouseConfig.GraphiteRollup.Pattern]:
            """Pattern to use for the rollup."""

        def __init__(
            self,
            *,
            name: builtins.str = ...,
            patterns: collections.abc.Iterable[global___ClickhouseConfig.GraphiteRollup.Pattern] | None = ...,
        ) -> None: ...
        def ClearField(self, field_name: typing.Literal["name", b"name", "patterns", b"patterns"]) -> None: ...

    LOG_LEVEL_FIELD_NUMBER: builtins.int
    MERGE_TREE_FIELD_NUMBER: builtins.int
    COMPRESSION_FIELD_NUMBER: builtins.int
    DICTIONARIES_FIELD_NUMBER: builtins.int
    GRAPHITE_ROLLUP_FIELD_NUMBER: builtins.int
    KAFKA_FIELD_NUMBER: builtins.int
    KAFKA_TOPICS_FIELD_NUMBER: builtins.int
    RABBITMQ_FIELD_NUMBER: builtins.int
    MAX_CONNECTIONS_FIELD_NUMBER: builtins.int
    MAX_CONCURRENT_QUERIES_FIELD_NUMBER: builtins.int
    KEEP_ALIVE_TIMEOUT_FIELD_NUMBER: builtins.int
    UNCOMPRESSED_CACHE_SIZE_FIELD_NUMBER: builtins.int
    MARK_CACHE_SIZE_FIELD_NUMBER: builtins.int
    MAX_TABLE_SIZE_TO_DROP_FIELD_NUMBER: builtins.int
    MAX_PARTITION_SIZE_TO_DROP_FIELD_NUMBER: builtins.int
    BUILTIN_DICTIONARIES_RELOAD_INTERVAL_FIELD_NUMBER: builtins.int
    TIMEZONE_FIELD_NUMBER: builtins.int
    GEOBASE_ENABLED_FIELD_NUMBER: builtins.int
    GEOBASE_URI_FIELD_NUMBER: builtins.int
    QUERY_LOG_RETENTION_SIZE_FIELD_NUMBER: builtins.int
    QUERY_LOG_RETENTION_TIME_FIELD_NUMBER: builtins.int
    QUERY_THREAD_LOG_ENABLED_FIELD_NUMBER: builtins.int
    QUERY_THREAD_LOG_RETENTION_SIZE_FIELD_NUMBER: builtins.int
    QUERY_THREAD_LOG_RETENTION_TIME_FIELD_NUMBER: builtins.int
    PART_LOG_RETENTION_SIZE_FIELD_NUMBER: builtins.int
    PART_LOG_RETENTION_TIME_FIELD_NUMBER: builtins.int
    METRIC_LOG_ENABLED_FIELD_NUMBER: builtins.int
    METRIC_LOG_RETENTION_SIZE_FIELD_NUMBER: builtins.int
    METRIC_LOG_RETENTION_TIME_FIELD_NUMBER: builtins.int
    TRACE_LOG_ENABLED_FIELD_NUMBER: builtins.int
    TRACE_LOG_RETENTION_SIZE_FIELD_NUMBER: builtins.int
    TRACE_LOG_RETENTION_TIME_FIELD_NUMBER: builtins.int
    TEXT_LOG_ENABLED_FIELD_NUMBER: builtins.int
    TEXT_LOG_RETENTION_SIZE_FIELD_NUMBER: builtins.int
    TEXT_LOG_RETENTION_TIME_FIELD_NUMBER: builtins.int
    TEXT_LOG_LEVEL_FIELD_NUMBER: builtins.int
    OPENTELEMETRY_SPAN_LOG_ENABLED_FIELD_NUMBER: builtins.int
    OPENTELEMETRY_SPAN_LOG_RETENTION_SIZE_FIELD_NUMBER: builtins.int
    OPENTELEMETRY_SPAN_LOG_RETENTION_TIME_FIELD_NUMBER: builtins.int
    QUERY_VIEWS_LOG_ENABLED_FIELD_NUMBER: builtins.int
    QUERY_VIEWS_LOG_RETENTION_SIZE_FIELD_NUMBER: builtins.int
    QUERY_VIEWS_LOG_RETENTION_TIME_FIELD_NUMBER: builtins.int
    ASYNCHRONOUS_METRIC_LOG_ENABLED_FIELD_NUMBER: builtins.int
    ASYNCHRONOUS_METRIC_LOG_RETENTION_SIZE_FIELD_NUMBER: builtins.int
    ASYNCHRONOUS_METRIC_LOG_RETENTION_TIME_FIELD_NUMBER: builtins.int
    SESSION_LOG_ENABLED_FIELD_NUMBER: builtins.int
    SESSION_LOG_RETENTION_SIZE_FIELD_NUMBER: builtins.int
    SESSION_LOG_RETENTION_TIME_FIELD_NUMBER: builtins.int
    ZOOKEEPER_LOG_ENABLED_FIELD_NUMBER: builtins.int
    ZOOKEEPER_LOG_RETENTION_SIZE_FIELD_NUMBER: builtins.int
    ZOOKEEPER_LOG_RETENTION_TIME_FIELD_NUMBER: builtins.int
    ASYNCHRONOUS_INSERT_LOG_ENABLED_FIELD_NUMBER: builtins.int
    ASYNCHRONOUS_INSERT_LOG_RETENTION_SIZE_FIELD_NUMBER: builtins.int
    ASYNCHRONOUS_INSERT_LOG_RETENTION_TIME_FIELD_NUMBER: builtins.int
    BACKGROUND_POOL_SIZE_FIELD_NUMBER: builtins.int
    BACKGROUND_MERGES_MUTATIONS_CONCURRENCY_RATIO_FIELD_NUMBER: builtins.int
    BACKGROUND_SCHEDULE_POOL_SIZE_FIELD_NUMBER: builtins.int
    BACKGROUND_FETCHES_POOL_SIZE_FIELD_NUMBER: builtins.int
    BACKGROUND_MOVE_POOL_SIZE_FIELD_NUMBER: builtins.int
    BACKGROUND_DISTRIBUTED_SCHEDULE_POOL_SIZE_FIELD_NUMBER: builtins.int
    BACKGROUND_BUFFER_FLUSH_SCHEDULE_POOL_SIZE_FIELD_NUMBER: builtins.int
    BACKGROUND_MESSAGE_BROKER_SCHEDULE_POOL_SIZE_FIELD_NUMBER: builtins.int
    BACKGROUND_COMMON_POOL_SIZE_FIELD_NUMBER: builtins.int
    DEFAULT_DATABASE_FIELD_NUMBER: builtins.int
    TOTAL_MEMORY_PROFILER_STEP_FIELD_NUMBER: builtins.int
    TOTAL_MEMORY_TRACKER_SAMPLE_PROBABILITY_FIELD_NUMBER: builtins.int
    log_level: global___ClickhouseConfig.LogLevel.ValueType
    """Logging level for the ClickHouse cluster. Possible values: TRACE, DEBUG, INFORMATION, WARNING, ERROR."""
    timezone: builtins.str
    """The server's time zone to be used in DateTime fields conversions. Specified as an IANA identifier."""
    geobase_uri: builtins.str
    """Address of the archive with the user geobase in Object Storage."""
    text_log_level: global___ClickhouseConfig.LogLevel.ValueType
    """Logging level for text_log system table. Possible values: TRACE, DEBUG, INFORMATION, WARNING, ERROR."""
    @property
    def merge_tree(self) -> global___ClickhouseConfig.MergeTree:
        """Settings for the MergeTree engine.
        See description in [ClickHouse documentation](https://clickhouse.com/docs/en/operations/server_settings/settings/#merge_tree).
        """

    @property
    def compression(self) -> google.protobuf.internal.containers.RepeatedCompositeFieldContainer[global___ClickhouseConfig.Compression]:
        """Compression settings for the ClickHouse cluster.
        See in-depth description in [ClickHouse documentation](https://clickhouse.com/docs/en/operations/server_settings/settings/#compression).
        """

    @property
    def dictionaries(self) -> google.protobuf.internal.containers.RepeatedCompositeFieldContainer[global___ClickhouseConfig.ExternalDictionary]:
        """Configuration of external dictionaries to be used by the ClickHouse cluster.
        See in-depth description in [ClickHouse documentation](https://clickhouse.com/docs/en/query_language/dicts/external_dicts/).
        """

    @property
    def graphite_rollup(self) -> google.protobuf.internal.containers.RepeatedCompositeFieldContainer[global___ClickhouseConfig.GraphiteRollup]:
        """Settings for thinning Graphite data.
        See in-depth description in [ClickHouse documentation](https://clickhouse.com/docs/en/operations/server_settings/settings/#server_settings-graphite_rollup).
        """

    @property
    def kafka(self) -> global___ClickhouseConfig.Kafka: ...
    @property
    def kafka_topics(self) -> google.protobuf.internal.containers.RepeatedCompositeFieldContainer[global___ClickhouseConfig.KafkaTopic]: ...
    @property
    def rabbitmq(self) -> global___ClickhouseConfig.Rabbitmq: ...
    @property
    def max_connections(self) -> google.protobuf.wrappers_pb2.Int64Value:
        """Maximum number of inbound connections."""

    @property
    def max_concurrent_queries(self) -> google.protobuf.wrappers_pb2.Int64Value:
        """Maximum number of simultaneously processed requests."""

    @property
    def keep_alive_timeout(self) -> google.protobuf.wrappers_pb2.Int64Value:
        """Number of milliseconds that ClickHouse waits for incoming requests before closing the connection."""

    @property
    def uncompressed_cache_size(self) -> google.protobuf.wrappers_pb2.Int64Value:
        """Cache size (in bytes) for uncompressed data used by MergeTree tables."""

    @property
    def mark_cache_size(self) -> google.protobuf.wrappers_pb2.Int64Value:
        """Approximate size (in bytes) of the cache of "marks" used by MergeTree tables."""

    @property
    def max_table_size_to_drop(self) -> google.protobuf.wrappers_pb2.Int64Value:
        """Maximum size of the table that can be deleted using a DROP query."""

    @property
    def max_partition_size_to_drop(self) -> google.protobuf.wrappers_pb2.Int64Value:
        """Maximum size of the partition that can be deleted using a DROP query."""

    @property
    def builtin_dictionaries_reload_interval(self) -> google.protobuf.wrappers_pb2.Int64Value:
        """The setting is deprecated and has no effect."""

    @property
    def geobase_enabled(self) -> google.protobuf.wrappers_pb2.BoolValue:
        """Enable or disable geobase."""

    @property
    def query_log_retention_size(self) -> google.protobuf.wrappers_pb2.Int64Value:
        """The maximum size that query_log can grow to before old data will be removed. If set to 0, automatic removal of
        query_log data based on size is disabled.
        """

    @property
    def query_log_retention_time(self) -> google.protobuf.wrappers_pb2.Int64Value:
        """The maximum time that query_log records will be retained before removal. If set to 0, automatic removal of
        query_log data based on time is disabled.
        """

    @property
    def query_thread_log_enabled(self) -> google.protobuf.wrappers_pb2.BoolValue:
        """Whether query_thread_log system table is enabled."""

    @property
    def query_thread_log_retention_size(self) -> google.protobuf.wrappers_pb2.Int64Value:
        """The maximum size that query_thread_log can grow to before old data will be removed. If set to 0, automatic removal of
        query_thread_log data based on size is disabled.
        """

    @property
    def query_thread_log_retention_time(self) -> google.protobuf.wrappers_pb2.Int64Value:
        """The maximum time that query_thread_log records will be retained before removal. If set to 0, automatic removal of
        query_thread_log data based on time is disabled.
        """

    @property
    def part_log_retention_size(self) -> google.protobuf.wrappers_pb2.Int64Value:
        """The maximum size that part_log can grow to before old data will be removed. If set to 0, automatic removal of
        part_log data based on size is disabled.
        """

    @property
    def part_log_retention_time(self) -> google.protobuf.wrappers_pb2.Int64Value:
        """The maximum time that part_log records will be retained before removal. If set to 0, automatic removal of
        part_log data based on time is disabled.
        """

    @property
    def metric_log_enabled(self) -> google.protobuf.wrappers_pb2.BoolValue:
        """Whether metric_log system table is enabled."""

    @property
    def metric_log_retention_size(self) -> google.protobuf.wrappers_pb2.Int64Value:
        """The maximum size that metric_log can grow to before old data will be removed. If set to 0, automatic removal of
        metric_log data based on size is disabled.
        """

    @property
    def metric_log_retention_time(self) -> google.protobuf.wrappers_pb2.Int64Value:
        """The maximum time that metric_log records will be retained before removal. If set to 0, automatic removal of
        metric_log data based on time is disabled.
        """

    @property
    def trace_log_enabled(self) -> google.protobuf.wrappers_pb2.BoolValue:
        """Whether trace_log system table is enabled."""

    @property
    def trace_log_retention_size(self) -> google.protobuf.wrappers_pb2.Int64Value:
        """The maximum size that trace_log can grow to before old data will be removed. If set to 0, automatic removal of
        trace_log data based on size is disabled.
        """

    @property
    def trace_log_retention_time(self) -> google.protobuf.wrappers_pb2.Int64Value:
        """The maximum time that trace_log records will be retained before removal. If set to 0, automatic removal of
        trace_log data based on time is disabled.
        """

    @property
    def text_log_enabled(self) -> google.protobuf.wrappers_pb2.BoolValue:
        """Whether text_log system table is enabled."""

    @property
    def text_log_retention_size(self) -> google.protobuf.wrappers_pb2.Int64Value:
        """The maximum size that text_log can grow to before old data will be removed. If set to 0, automatic removal of
        text_log data based on size is disabled.
        """

    @property
    def text_log_retention_time(self) -> google.protobuf.wrappers_pb2.Int64Value:
        """The maximum time that text_log records will be retained before removal. If set to 0, automatic removal of
        text_log data based on time is disabled.
        """

    @property
    def opentelemetry_span_log_enabled(self) -> google.protobuf.wrappers_pb2.BoolValue:
        """Enable or disable opentelemetry_span_log system table. Default value: false."""

    @property
    def opentelemetry_span_log_retention_size(self) -> google.protobuf.wrappers_pb2.Int64Value:
        """The maximum size that opentelemetry_span_log can grow to before old data will be removed. If set to 0 (default),
        automatic removal of opentelemetry_span_log data based on size is disabled.
        """

    @property
    def opentelemetry_span_log_retention_time(self) -> google.protobuf.wrappers_pb2.Int64Value:
        """The maximum time that opentelemetry_span_log records will be retained before removal. If set to 0,
        automatic removal of opentelemetry_span_log data based on time is disabled.
        """

    @property
    def query_views_log_enabled(self) -> google.protobuf.wrappers_pb2.BoolValue:
        """Enable or disable query_views_log system table. Default value: false."""

    @property
    def query_views_log_retention_size(self) -> google.protobuf.wrappers_pb2.Int64Value:
        """The maximum size that query_views_log can grow to before old data will be removed. If set to 0 (default),
        automatic removal of query_views_log data based on size is disabled.
        """

    @property
    def query_views_log_retention_time(self) -> google.protobuf.wrappers_pb2.Int64Value:
        """The maximum time that query_views_log records will be retained before removal. If set to 0,
        automatic removal of query_views_log data based on time is disabled.
        """

    @property
    def asynchronous_metric_log_enabled(self) -> google.protobuf.wrappers_pb2.BoolValue:
        """Enable or disable asynchronous_metric_log system table. Default value: false."""

    @property
    def asynchronous_metric_log_retention_size(self) -> google.protobuf.wrappers_pb2.Int64Value:
        """The maximum size that asynchronous_metric_log can grow to before old data will be removed. If set to 0 (default),
        automatic removal of asynchronous_metric_log data based on size is disabled.
        """

    @property
    def asynchronous_metric_log_retention_time(self) -> google.protobuf.wrappers_pb2.Int64Value:
        """The maximum time that asynchronous_metric_log records will be retained before removal. If set to 0,
        automatic removal of asynchronous_metric_log data based on time is disabled.
        """

    @property
    def session_log_enabled(self) -> google.protobuf.wrappers_pb2.BoolValue:
        """Enable or disable session_log system table. Default value: false."""

    @property
    def session_log_retention_size(self) -> google.protobuf.wrappers_pb2.Int64Value:
        """The maximum size that session_log can grow to before old data will be removed. If set to 0 (default),
        automatic removal of session_log data based on size is disabled.
        """

    @property
    def session_log_retention_time(self) -> google.protobuf.wrappers_pb2.Int64Value:
        """The maximum time that session_log records will be retained before removal. If set to 0,
        automatic removal of session_log data based on time is disabled.
        """

    @property
    def zookeeper_log_enabled(self) -> google.protobuf.wrappers_pb2.BoolValue:
        """Enable or disable zookeeper_log system table. Default value: false."""

    @property
    def zookeeper_log_retention_size(self) -> google.protobuf.wrappers_pb2.Int64Value:
        """The maximum size that zookeeper_log can grow to before old data will be removed. If set to 0 (default),
        automatic removal of zookeeper_log data based on size is disabled.
        """

    @property
    def zookeeper_log_retention_time(self) -> google.protobuf.wrappers_pb2.Int64Value:
        """The maximum time that zookeeper_log records will be retained before removal. If set to 0,
        automatic removal of zookeeper_log data based on time is disabled.
        """

    @property
    def asynchronous_insert_log_enabled(self) -> google.protobuf.wrappers_pb2.BoolValue:
        """Enable or disable asynchronous_insert_log system table. Default value: false.
        Minimal required ClickHouse version: 22.10.
        """

    @property
    def asynchronous_insert_log_retention_size(self) -> google.protobuf.wrappers_pb2.Int64Value:
        """The maximum size that asynchronous_insert_log can grow to before old data will be removed. If set to 0 (default),
        automatic removal of asynchronous_insert_log data based on size is disabled.
        """

    @property
    def asynchronous_insert_log_retention_time(self) -> google.protobuf.wrappers_pb2.Int64Value:
        """The maximum time that asynchronous_insert_log records will be retained before removal. If set to 0,
        automatic removal of asynchronous_insert_log data based on time is disabled.
        """

    @property
    def background_pool_size(self) -> google.protobuf.wrappers_pb2.Int64Value: ...
    @property
    def background_merges_mutations_concurrency_ratio(self) -> google.protobuf.wrappers_pb2.Int64Value:
        """Sets a ratio between the number of threads and the number of background merges and mutations that can be executed concurrently. For example, if the ratio equals to 2 and background_pool_size is set to 16 then ClickHouse can execute 32 background merges concurrently. This is possible, because background operations could be suspended and postponed. This is needed to give small merges more execution priority. You can only increase this ratio at runtime. To lower it you have to restart the server. The same as for background_pool_size setting background_merges_mutations_concurrency_ratio could be applied from the default profile for backward compatibility.
        Default: 2
        See in-depth description in [ClickHouse documentation](https://clickhouse.com/docs/en/operations/server-configuration-parameters/settings#background_merges_mutations_concurrency_ratio)
        """

    @property
    def background_schedule_pool_size(self) -> google.protobuf.wrappers_pb2.Int64Value: ...
    @property
    def background_fetches_pool_size(self) -> google.protobuf.wrappers_pb2.Int64Value:
        """Sets the number of threads performing background fetches for tables with **ReplicatedMergeTree** engines. Default value: 8.

        More info see in [ClickHouse documentation](https://clickhouse.com/docs/en/operations/server-configuration-parameters/settings/#background_fetches_pool_size).
        """

    @property
    def background_move_pool_size(self) -> google.protobuf.wrappers_pb2.Int64Value: ...
    @property
    def background_distributed_schedule_pool_size(self) -> google.protobuf.wrappers_pb2.Int64Value: ...
    @property
    def background_buffer_flush_schedule_pool_size(self) -> google.protobuf.wrappers_pb2.Int64Value: ...
    @property
    def background_message_broker_schedule_pool_size(self) -> google.protobuf.wrappers_pb2.Int64Value: ...
    @property
    def background_common_pool_size(self) -> google.protobuf.wrappers_pb2.Int64Value:
        """The maximum number of threads that will be used for performing a variety of operations (mostly garbage collection) for *MergeTree-engine tables in a background.
        Default: 8
        See in-depth description in [ClickHouse documentation](https://clickhouse.com/docs/en/operations/server-configuration-parameters/settings#background_common_pool_size)
        """

    @property
    def default_database(self) -> google.protobuf.wrappers_pb2.StringValue:
        """The default database.

        To get a list of cluster databases, see [Yandex Managed ClickHouse documentation](/docs/managed-clickhouse/operations/databases#list-db).
        """

    @property
    def total_memory_profiler_step(self) -> google.protobuf.wrappers_pb2.Int64Value:
        """Sets the memory size (in bytes) for a stack trace at every peak allocation step. Default value: **4194304**.

        More info see in [ClickHouse documentation](https://clickhouse.com/docs/en/operations/server-configuration-parameters/settings/#total-memory-profiler-step).
        """

    @property
    def total_memory_tracker_sample_probability(self) -> google.protobuf.wrappers_pb2.DoubleValue: ...
    def __init__(
        self,
        *,
        log_level: global___ClickhouseConfig.LogLevel.ValueType = ...,
        merge_tree: global___ClickhouseConfig.MergeTree | None = ...,
        compression: collections.abc.Iterable[global___ClickhouseConfig.Compression] | None = ...,
        dictionaries: collections.abc.Iterable[global___ClickhouseConfig.ExternalDictionary] | None = ...,
        graphite_rollup: collections.abc.Iterable[global___ClickhouseConfig.GraphiteRollup] | None = ...,
        kafka: global___ClickhouseConfig.Kafka | None = ...,
        kafka_topics: collections.abc.Iterable[global___ClickhouseConfig.KafkaTopic] | None = ...,
        rabbitmq: global___ClickhouseConfig.Rabbitmq | None = ...,
        max_connections: google.protobuf.wrappers_pb2.Int64Value | None = ...,
        max_concurrent_queries: google.protobuf.wrappers_pb2.Int64Value | None = ...,
        keep_alive_timeout: google.protobuf.wrappers_pb2.Int64Value | None = ...,
        uncompressed_cache_size: google.protobuf.wrappers_pb2.Int64Value | None = ...,
        mark_cache_size: google.protobuf.wrappers_pb2.Int64Value | None = ...,
        max_table_size_to_drop: google.protobuf.wrappers_pb2.Int64Value | None = ...,
        max_partition_size_to_drop: google.protobuf.wrappers_pb2.Int64Value | None = ...,
        builtin_dictionaries_reload_interval: google.protobuf.wrappers_pb2.Int64Value | None = ...,
        timezone: builtins.str = ...,
        geobase_enabled: google.protobuf.wrappers_pb2.BoolValue | None = ...,
        geobase_uri: builtins.str = ...,
        query_log_retention_size: google.protobuf.wrappers_pb2.Int64Value | None = ...,
        query_log_retention_time: google.protobuf.wrappers_pb2.Int64Value | None = ...,
        query_thread_log_enabled: google.protobuf.wrappers_pb2.BoolValue | None = ...,
        query_thread_log_retention_size: google.protobuf.wrappers_pb2.Int64Value | None = ...,
        query_thread_log_retention_time: google.protobuf.wrappers_pb2.Int64Value | None = ...,
        part_log_retention_size: google.protobuf.wrappers_pb2.Int64Value | None = ...,
        part_log_retention_time: google.protobuf.wrappers_pb2.Int64Value | None = ...,
        metric_log_enabled: google.protobuf.wrappers_pb2.BoolValue | None = ...,
        metric_log_retention_size: google.protobuf.wrappers_pb2.Int64Value | None = ...,
        metric_log_retention_time: google.protobuf.wrappers_pb2.Int64Value | None = ...,
        trace_log_enabled: google.protobuf.wrappers_pb2.BoolValue | None = ...,
        trace_log_retention_size: google.protobuf.wrappers_pb2.Int64Value | None = ...,
        trace_log_retention_time: google.protobuf.wrappers_pb2.Int64Value | None = ...,
        text_log_enabled: google.protobuf.wrappers_pb2.BoolValue | None = ...,
        text_log_retention_size: google.protobuf.wrappers_pb2.Int64Value | None = ...,
        text_log_retention_time: google.protobuf.wrappers_pb2.Int64Value | None = ...,
        text_log_level: global___ClickhouseConfig.LogLevel.ValueType = ...,
        opentelemetry_span_log_enabled: google.protobuf.wrappers_pb2.BoolValue | None = ...,
        opentelemetry_span_log_retention_size: google.protobuf.wrappers_pb2.Int64Value | None = ...,
        opentelemetry_span_log_retention_time: google.protobuf.wrappers_pb2.Int64Value | None = ...,
        query_views_log_enabled: google.protobuf.wrappers_pb2.BoolValue | None = ...,
        query_views_log_retention_size: google.protobuf.wrappers_pb2.Int64Value | None = ...,
        query_views_log_retention_time: google.protobuf.wrappers_pb2.Int64Value | None = ...,
        asynchronous_metric_log_enabled: google.protobuf.wrappers_pb2.BoolValue | None = ...,
        asynchronous_metric_log_retention_size: google.protobuf.wrappers_pb2.Int64Value | None = ...,
        asynchronous_metric_log_retention_time: google.protobuf.wrappers_pb2.Int64Value | None = ...,
        session_log_enabled: google.protobuf.wrappers_pb2.BoolValue | None = ...,
        session_log_retention_size: google.protobuf.wrappers_pb2.Int64Value | None = ...,
        session_log_retention_time: google.protobuf.wrappers_pb2.Int64Value | None = ...,
        zookeeper_log_enabled: google.protobuf.wrappers_pb2.BoolValue | None = ...,
        zookeeper_log_retention_size: google.protobuf.wrappers_pb2.Int64Value | None = ...,
        zookeeper_log_retention_time: google.protobuf.wrappers_pb2.Int64Value | None = ...,
        asynchronous_insert_log_enabled: google.protobuf.wrappers_pb2.BoolValue | None = ...,
        asynchronous_insert_log_retention_size: google.protobuf.wrappers_pb2.Int64Value | None = ...,
        asynchronous_insert_log_retention_time: google.protobuf.wrappers_pb2.Int64Value | None = ...,
        background_pool_size: google.protobuf.wrappers_pb2.Int64Value | None = ...,
        background_merges_mutations_concurrency_ratio: google.protobuf.wrappers_pb2.Int64Value | None = ...,
        background_schedule_pool_size: google.protobuf.wrappers_pb2.Int64Value | None = ...,
        background_fetches_pool_size: google.protobuf.wrappers_pb2.Int64Value | None = ...,
        background_move_pool_size: google.protobuf.wrappers_pb2.Int64Value | None = ...,
        background_distributed_schedule_pool_size: google.protobuf.wrappers_pb2.Int64Value | None = ...,
        background_buffer_flush_schedule_pool_size: google.protobuf.wrappers_pb2.Int64Value | None = ...,
        background_message_broker_schedule_pool_size: google.protobuf.wrappers_pb2.Int64Value | None = ...,
        background_common_pool_size: google.protobuf.wrappers_pb2.Int64Value | None = ...,
        default_database: google.protobuf.wrappers_pb2.StringValue | None = ...,
        total_memory_profiler_step: google.protobuf.wrappers_pb2.Int64Value | None = ...,
        total_memory_tracker_sample_probability: google.protobuf.wrappers_pb2.DoubleValue | None = ...,
    ) -> None: ...
    def HasField(self, field_name: typing.Literal["asynchronous_insert_log_enabled", b"asynchronous_insert_log_enabled", "asynchronous_insert_log_retention_size", b"asynchronous_insert_log_retention_size", "asynchronous_insert_log_retention_time", b"asynchronous_insert_log_retention_time", "asynchronous_metric_log_enabled", b"asynchronous_metric_log_enabled", "asynchronous_metric_log_retention_size", b"asynchronous_metric_log_retention_size", "asynchronous_metric_log_retention_time", b"asynchronous_metric_log_retention_time", "background_buffer_flush_schedule_pool_size", b"background_buffer_flush_schedule_pool_size", "background_common_pool_size", b"background_common_pool_size", "background_distributed_schedule_pool_size", b"background_distributed_schedule_pool_size", "background_fetches_pool_size", b"background_fetches_pool_size", "background_merges_mutations_concurrency_ratio", b"background_merges_mutations_concurrency_ratio", "background_message_broker_schedule_pool_size", b"background_message_broker_schedule_pool_size", "background_move_pool_size", b"background_move_pool_size", "background_pool_size", b"background_pool_size", "background_schedule_pool_size", b"background_schedule_pool_size", "builtin_dictionaries_reload_interval", b"builtin_dictionaries_reload_interval", "default_database", b"default_database", "geobase_enabled", b"geobase_enabled", "kafka", b"kafka", "keep_alive_timeout", b"keep_alive_timeout", "mark_cache_size", b"mark_cache_size", "max_concurrent_queries", b"max_concurrent_queries", "max_connections", b"max_connections", "max_partition_size_to_drop", b"max_partition_size_to_drop", "max_table_size_to_drop", b"max_table_size_to_drop", "merge_tree", b"merge_tree", "metric_log_enabled", b"metric_log_enabled", "metric_log_retention_size", b"metric_log_retention_size", "metric_log_retention_time", b"metric_log_retention_time", "opentelemetry_span_log_enabled", b"opentelemetry_span_log_enabled", "opentelemetry_span_log_retention_size", b"opentelemetry_span_log_retention_size", "opentelemetry_span_log_retention_time", b"opentelemetry_span_log_retention_time", "part_log_retention_size", b"part_log_retention_size", "part_log_retention_time", b"part_log_retention_time", "query_log_retention_size", b"query_log_retention_size", "query_log_retention_time", b"query_log_retention_time", "query_thread_log_enabled", b"query_thread_log_enabled", "query_thread_log_retention_size", b"query_thread_log_retention_size", "query_thread_log_retention_time", b"query_thread_log_retention_time", "query_views_log_enabled", b"query_views_log_enabled", "query_views_log_retention_size", b"query_views_log_retention_size", "query_views_log_retention_time", b"query_views_log_retention_time", "rabbitmq", b"rabbitmq", "session_log_enabled", b"session_log_enabled", "session_log_retention_size", b"session_log_retention_size", "session_log_retention_time", b"session_log_retention_time", "text_log_enabled", b"text_log_enabled", "text_log_retention_size", b"text_log_retention_size", "text_log_retention_time", b"text_log_retention_time", "total_memory_profiler_step", b"total_memory_profiler_step", "total_memory_tracker_sample_probability", b"total_memory_tracker_sample_probability", "trace_log_enabled", b"trace_log_enabled", "trace_log_retention_size", b"trace_log_retention_size", "trace_log_retention_time", b"trace_log_retention_time", "uncompressed_cache_size", b"uncompressed_cache_size", "zookeeper_log_enabled", b"zookeeper_log_enabled", "zookeeper_log_retention_size", b"zookeeper_log_retention_size", "zookeeper_log_retention_time", b"zookeeper_log_retention_time"]) -> builtins.bool: ...
    def ClearField(self, field_name: typing.Literal["asynchronous_insert_log_enabled", b"asynchronous_insert_log_enabled", "asynchronous_insert_log_retention_size", b"asynchronous_insert_log_retention_size", "asynchronous_insert_log_retention_time", b"asynchronous_insert_log_retention_time", "asynchronous_metric_log_enabled", b"asynchronous_metric_log_enabled", "asynchronous_metric_log_retention_size", b"asynchronous_metric_log_retention_size", "asynchronous_metric_log_retention_time", b"asynchronous_metric_log_retention_time", "background_buffer_flush_schedule_pool_size", b"background_buffer_flush_schedule_pool_size", "background_common_pool_size", b"background_common_pool_size", "background_distributed_schedule_pool_size", b"background_distributed_schedule_pool_size", "background_fetches_pool_size", b"background_fetches_pool_size", "background_merges_mutations_concurrency_ratio", b"background_merges_mutations_concurrency_ratio", "background_message_broker_schedule_pool_size", b"background_message_broker_schedule_pool_size", "background_move_pool_size", b"background_move_pool_size", "background_pool_size", b"background_pool_size", "background_schedule_pool_size", b"background_schedule_pool_size", "builtin_dictionaries_reload_interval", b"builtin_dictionaries_reload_interval", "compression", b"compression", "default_database", b"default_database", "dictionaries", b"dictionaries", "geobase_enabled", b"geobase_enabled", "geobase_uri", b"geobase_uri", "graphite_rollup", b"graphite_rollup", "kafka", b"kafka", "kafka_topics", b"kafka_topics", "keep_alive_timeout", b"keep_alive_timeout", "log_level", b"log_level", "mark_cache_size", b"mark_cache_size", "max_concurrent_queries", b"max_concurrent_queries", "max_connections", b"max_connections", "max_partition_size_to_drop", b"max_partition_size_to_drop", "max_table_size_to_drop", b"max_table_size_to_drop", "merge_tree", b"merge_tree", "metric_log_enabled", b"metric_log_enabled", "metric_log_retention_size", b"metric_log_retention_size", "metric_log_retention_time", b"metric_log_retention_time", "opentelemetry_span_log_enabled", b"opentelemetry_span_log_enabled", "opentelemetry_span_log_retention_size", b"opentelemetry_span_log_retention_size", "opentelemetry_span_log_retention_time", b"opentelemetry_span_log_retention_time", "part_log_retention_size", b"part_log_retention_size", "part_log_retention_time", b"part_log_retention_time", "query_log_retention_size", b"query_log_retention_size", "query_log_retention_time", b"query_log_retention_time", "query_thread_log_enabled", b"query_thread_log_enabled", "query_thread_log_retention_size", b"query_thread_log_retention_size", "query_thread_log_retention_time", b"query_thread_log_retention_time", "query_views_log_enabled", b"query_views_log_enabled", "query_views_log_retention_size", b"query_views_log_retention_size", "query_views_log_retention_time", b"query_views_log_retention_time", "rabbitmq", b"rabbitmq", "session_log_enabled", b"session_log_enabled", "session_log_retention_size", b"session_log_retention_size", "session_log_retention_time", b"session_log_retention_time", "text_log_enabled", b"text_log_enabled", "text_log_level", b"text_log_level", "text_log_retention_size", b"text_log_retention_size", "text_log_retention_time", b"text_log_retention_time", "timezone", b"timezone", "total_memory_profiler_step", b"total_memory_profiler_step", "total_memory_tracker_sample_probability", b"total_memory_tracker_sample_probability", "trace_log_enabled", b"trace_log_enabled", "trace_log_retention_size", b"trace_log_retention_size", "trace_log_retention_time", b"trace_log_retention_time", "uncompressed_cache_size", b"uncompressed_cache_size", "zookeeper_log_enabled", b"zookeeper_log_enabled", "zookeeper_log_retention_size", b"zookeeper_log_retention_size", "zookeeper_log_retention_time", b"zookeeper_log_retention_time"]) -> None: ...

global___ClickhouseConfig = ClickhouseConfig

@typing.final
class ClickhouseConfigSet(google.protobuf.message.Message):
    DESCRIPTOR: google.protobuf.descriptor.Descriptor

    EFFECTIVE_CONFIG_FIELD_NUMBER: builtins.int
    USER_CONFIG_FIELD_NUMBER: builtins.int
    DEFAULT_CONFIG_FIELD_NUMBER: builtins.int
    @property
    def effective_config(self) -> global___ClickhouseConfig:
        """Effective settings for a ClickHouse cluster (a combination of settings defined
        in [user_config] and [default_config]).
        """

    @property
    def user_config(self) -> global___ClickhouseConfig:
        """User-defined settings for a ClickHouse cluster."""

    @property
    def default_config(self) -> global___ClickhouseConfig:
        """Default configuration for a ClickHouse cluster."""

    def __init__(
        self,
        *,
        effective_config: global___ClickhouseConfig | None = ...,
        user_config: global___ClickhouseConfig | None = ...,
        default_config: global___ClickhouseConfig | None = ...,
    ) -> None: ...
    def HasField(self, field_name: typing.Literal["default_config", b"default_config", "effective_config", b"effective_config", "user_config", b"user_config"]) -> builtins.bool: ...
    def ClearField(self, field_name: typing.Literal["default_config", b"default_config", "effective_config", b"effective_config", "user_config", b"user_config"]) -> None: ...

global___ClickhouseConfigSet = ClickhouseConfigSet
