"""
@generated by mypy-protobuf.  Do not edit manually!
isort:skip_file
"""

import builtins
import collections.abc
import google.protobuf.descriptor
import google.protobuf.internal.containers
import google.protobuf.internal.enum_type_wrapper
import google.protobuf.message
import google.protobuf.wrappers_pb2
import sys
import typing

if sys.version_info >= (3, 10):
    import typing as typing_extensions
else:
    import typing_extensions

DESCRIPTOR: google.protobuf.descriptor.FileDescriptor

@typing.final
class User(google.protobuf.message.Message):
    """A ClickHouse User resource. For more information, see
    the [Developer's guide](/docs/managed-clickhouse/concepts).
    """

    DESCRIPTOR: google.protobuf.descriptor.Descriptor

    NAME_FIELD_NUMBER: builtins.int
    CLUSTER_ID_FIELD_NUMBER: builtins.int
    PERMISSIONS_FIELD_NUMBER: builtins.int
    SETTINGS_FIELD_NUMBER: builtins.int
    QUOTAS_FIELD_NUMBER: builtins.int
    name: builtins.str
    """Name of the ClickHouse user."""
    cluster_id: builtins.str
    """ID of the ClickHouse cluster the user belongs to."""
    @property
    def permissions(self) -> google.protobuf.internal.containers.RepeatedCompositeFieldContainer[global___Permission]:
        """Set of permissions granted to the user."""

    @property
    def settings(self) -> global___UserSettings: ...
    @property
    def quotas(self) -> google.protobuf.internal.containers.RepeatedCompositeFieldContainer[global___UserQuota]:
        """Set of quotas assigned to the user."""

    def __init__(
        self,
        *,
        name: builtins.str = ...,
        cluster_id: builtins.str = ...,
        permissions: collections.abc.Iterable[global___Permission] | None = ...,
        settings: global___UserSettings | None = ...,
        quotas: collections.abc.Iterable[global___UserQuota] | None = ...,
    ) -> None: ...
    def HasField(self, field_name: typing.Literal["settings", b"settings"]) -> builtins.bool: ...
    def ClearField(self, field_name: typing.Literal["cluster_id", b"cluster_id", "name", b"name", "permissions", b"permissions", "quotas", b"quotas", "settings", b"settings"]) -> None: ...

global___User = User

@typing.final
class Permission(google.protobuf.message.Message):
    DESCRIPTOR: google.protobuf.descriptor.Descriptor

    DATABASE_NAME_FIELD_NUMBER: builtins.int
    database_name: builtins.str
    """Name of the database that the permission grants access to."""
    def __init__(
        self,
        *,
        database_name: builtins.str = ...,
    ) -> None: ...
    def ClearField(self, field_name: typing.Literal["database_name", b"database_name"]) -> None: ...

global___Permission = Permission

@typing.final
class UserSpec(google.protobuf.message.Message):
    DESCRIPTOR: google.protobuf.descriptor.Descriptor

    NAME_FIELD_NUMBER: builtins.int
    PASSWORD_FIELD_NUMBER: builtins.int
    PERMISSIONS_FIELD_NUMBER: builtins.int
    SETTINGS_FIELD_NUMBER: builtins.int
    QUOTAS_FIELD_NUMBER: builtins.int
    name: builtins.str
    """Name of the ClickHouse user."""
    password: builtins.str
    """Password of the ClickHouse user."""
    @property
    def permissions(self) -> google.protobuf.internal.containers.RepeatedCompositeFieldContainer[global___Permission]:
        """Set of permissions to grant to the user. If not set, it's granted permissions to access all databases."""

    @property
    def settings(self) -> global___UserSettings: ...
    @property
    def quotas(self) -> google.protobuf.internal.containers.RepeatedCompositeFieldContainer[global___UserQuota]:
        """Set of quotas assigned to the user."""

    def __init__(
        self,
        *,
        name: builtins.str = ...,
        password: builtins.str = ...,
        permissions: collections.abc.Iterable[global___Permission] | None = ...,
        settings: global___UserSettings | None = ...,
        quotas: collections.abc.Iterable[global___UserQuota] | None = ...,
    ) -> None: ...
    def HasField(self, field_name: typing.Literal["settings", b"settings"]) -> builtins.bool: ...
    def ClearField(self, field_name: typing.Literal["name", b"name", "password", b"password", "permissions", b"permissions", "quotas", b"quotas", "settings", b"settings"]) -> None: ...

global___UserSpec = UserSpec

@typing.final
class UserSettings(google.protobuf.message.Message):
    """ClickHouse user settings. Supported settings are a limited subset of all settings
    described in [ClickHouse documentation](https://clickhouse.com/docs/en/operations/settings/).
    """

    DESCRIPTOR: google.protobuf.descriptor.Descriptor

    class _OverflowMode:
        ValueType = typing.NewType("ValueType", builtins.int)
        V: typing_extensions.TypeAlias = ValueType

    class _OverflowModeEnumTypeWrapper(google.protobuf.internal.enum_type_wrapper._EnumTypeWrapper[UserSettings._OverflowMode.ValueType], builtins.type):
        DESCRIPTOR: google.protobuf.descriptor.EnumDescriptor
        OVERFLOW_MODE_UNSPECIFIED: UserSettings._OverflowMode.ValueType  # 0
        OVERFLOW_MODE_THROW: UserSettings._OverflowMode.ValueType  # 1
        OVERFLOW_MODE_BREAK: UserSettings._OverflowMode.ValueType  # 2

    class OverflowMode(_OverflowMode, metaclass=_OverflowModeEnumTypeWrapper): ...
    OVERFLOW_MODE_UNSPECIFIED: UserSettings.OverflowMode.ValueType  # 0
    OVERFLOW_MODE_THROW: UserSettings.OverflowMode.ValueType  # 1
    OVERFLOW_MODE_BREAK: UserSettings.OverflowMode.ValueType  # 2

    class _GroupByOverflowMode:
        ValueType = typing.NewType("ValueType", builtins.int)
        V: typing_extensions.TypeAlias = ValueType

    class _GroupByOverflowModeEnumTypeWrapper(google.protobuf.internal.enum_type_wrapper._EnumTypeWrapper[UserSettings._GroupByOverflowMode.ValueType], builtins.type):
        DESCRIPTOR: google.protobuf.descriptor.EnumDescriptor
        GROUP_BY_OVERFLOW_MODE_UNSPECIFIED: UserSettings._GroupByOverflowMode.ValueType  # 0
        GROUP_BY_OVERFLOW_MODE_THROW: UserSettings._GroupByOverflowMode.ValueType  # 1
        GROUP_BY_OVERFLOW_MODE_BREAK: UserSettings._GroupByOverflowMode.ValueType  # 2
        GROUP_BY_OVERFLOW_MODE_ANY: UserSettings._GroupByOverflowMode.ValueType  # 3

    class GroupByOverflowMode(_GroupByOverflowMode, metaclass=_GroupByOverflowModeEnumTypeWrapper): ...
    GROUP_BY_OVERFLOW_MODE_UNSPECIFIED: UserSettings.GroupByOverflowMode.ValueType  # 0
    GROUP_BY_OVERFLOW_MODE_THROW: UserSettings.GroupByOverflowMode.ValueType  # 1
    GROUP_BY_OVERFLOW_MODE_BREAK: UserSettings.GroupByOverflowMode.ValueType  # 2
    GROUP_BY_OVERFLOW_MODE_ANY: UserSettings.GroupByOverflowMode.ValueType  # 3

    class _DistributedProductMode:
        ValueType = typing.NewType("ValueType", builtins.int)
        V: typing_extensions.TypeAlias = ValueType

    class _DistributedProductModeEnumTypeWrapper(google.protobuf.internal.enum_type_wrapper._EnumTypeWrapper[UserSettings._DistributedProductMode.ValueType], builtins.type):
        DESCRIPTOR: google.protobuf.descriptor.EnumDescriptor
        DISTRIBUTED_PRODUCT_MODE_UNSPECIFIED: UserSettings._DistributedProductMode.ValueType  # 0
        DISTRIBUTED_PRODUCT_MODE_DENY: UserSettings._DistributedProductMode.ValueType  # 1
        """Default value. Prohibits using these types of subqueries (returns the "Double-distributed in/JOIN subqueries is denied" exception)."""
        DISTRIBUTED_PRODUCT_MODE_LOCAL: UserSettings._DistributedProductMode.ValueType  # 2
        """Replaces the database and table in the subquery with local ones for the destination server (shard), leaving the normal IN/JOIN."""
        DISTRIBUTED_PRODUCT_MODE_GLOBAL: UserSettings._DistributedProductMode.ValueType  # 3
        """Replaces the IN/JOIN query with GLOBAL IN/GLOBAL JOIN."""
        DISTRIBUTED_PRODUCT_MODE_ALLOW: UserSettings._DistributedProductMode.ValueType  # 4
        """Allows the use of these types of subqueries."""

    class DistributedProductMode(_DistributedProductMode, metaclass=_DistributedProductModeEnumTypeWrapper): ...
    DISTRIBUTED_PRODUCT_MODE_UNSPECIFIED: UserSettings.DistributedProductMode.ValueType  # 0
    DISTRIBUTED_PRODUCT_MODE_DENY: UserSettings.DistributedProductMode.ValueType  # 1
    """Default value. Prohibits using these types of subqueries (returns the "Double-distributed in/JOIN subqueries is denied" exception)."""
    DISTRIBUTED_PRODUCT_MODE_LOCAL: UserSettings.DistributedProductMode.ValueType  # 2
    """Replaces the database and table in the subquery with local ones for the destination server (shard), leaving the normal IN/JOIN."""
    DISTRIBUTED_PRODUCT_MODE_GLOBAL: UserSettings.DistributedProductMode.ValueType  # 3
    """Replaces the IN/JOIN query with GLOBAL IN/GLOBAL JOIN."""
    DISTRIBUTED_PRODUCT_MODE_ALLOW: UserSettings.DistributedProductMode.ValueType  # 4
    """Allows the use of these types of subqueries."""

    class _QuotaMode:
        ValueType = typing.NewType("ValueType", builtins.int)
        V: typing_extensions.TypeAlias = ValueType

    class _QuotaModeEnumTypeWrapper(google.protobuf.internal.enum_type_wrapper._EnumTypeWrapper[UserSettings._QuotaMode.ValueType], builtins.type):
        DESCRIPTOR: google.protobuf.descriptor.EnumDescriptor
        QUOTA_MODE_UNSPECIFIED: UserSettings._QuotaMode.ValueType  # 0
        QUOTA_MODE_DEFAULT: UserSettings._QuotaMode.ValueType  # 1
        QUOTA_MODE_KEYED: UserSettings._QuotaMode.ValueType  # 2
        QUOTA_MODE_KEYED_BY_IP: UserSettings._QuotaMode.ValueType  # 3

    class QuotaMode(_QuotaMode, metaclass=_QuotaModeEnumTypeWrapper): ...
    QUOTA_MODE_UNSPECIFIED: UserSettings.QuotaMode.ValueType  # 0
    QUOTA_MODE_DEFAULT: UserSettings.QuotaMode.ValueType  # 1
    QUOTA_MODE_KEYED: UserSettings.QuotaMode.ValueType  # 2
    QUOTA_MODE_KEYED_BY_IP: UserSettings.QuotaMode.ValueType  # 3

    class _CountDistinctImplementation:
        ValueType = typing.NewType("ValueType", builtins.int)
        V: typing_extensions.TypeAlias = ValueType

    class _CountDistinctImplementationEnumTypeWrapper(google.protobuf.internal.enum_type_wrapper._EnumTypeWrapper[UserSettings._CountDistinctImplementation.ValueType], builtins.type):
        DESCRIPTOR: google.protobuf.descriptor.EnumDescriptor
        COUNT_DISTINCT_IMPLEMENTATION_UNSPECIFIED: UserSettings._CountDistinctImplementation.ValueType  # 0
        COUNT_DISTINCT_IMPLEMENTATION_UNIQ: UserSettings._CountDistinctImplementation.ValueType  # 1
        COUNT_DISTINCT_IMPLEMENTATION_UNIQ_COMBINED: UserSettings._CountDistinctImplementation.ValueType  # 2
        COUNT_DISTINCT_IMPLEMENTATION_UNIQ_COMBINED_64: UserSettings._CountDistinctImplementation.ValueType  # 3
        COUNT_DISTINCT_IMPLEMENTATION_UNIQ_HLL_12: UserSettings._CountDistinctImplementation.ValueType  # 4
        COUNT_DISTINCT_IMPLEMENTATION_UNIQ_EXACT: UserSettings._CountDistinctImplementation.ValueType  # 5

    class CountDistinctImplementation(_CountDistinctImplementation, metaclass=_CountDistinctImplementationEnumTypeWrapper): ...
    COUNT_DISTINCT_IMPLEMENTATION_UNSPECIFIED: UserSettings.CountDistinctImplementation.ValueType  # 0
    COUNT_DISTINCT_IMPLEMENTATION_UNIQ: UserSettings.CountDistinctImplementation.ValueType  # 1
    COUNT_DISTINCT_IMPLEMENTATION_UNIQ_COMBINED: UserSettings.CountDistinctImplementation.ValueType  # 2
    COUNT_DISTINCT_IMPLEMENTATION_UNIQ_COMBINED_64: UserSettings.CountDistinctImplementation.ValueType  # 3
    COUNT_DISTINCT_IMPLEMENTATION_UNIQ_HLL_12: UserSettings.CountDistinctImplementation.ValueType  # 4
    COUNT_DISTINCT_IMPLEMENTATION_UNIQ_EXACT: UserSettings.CountDistinctImplementation.ValueType  # 5

    class _JoinAlgorithm:
        ValueType = typing.NewType("ValueType", builtins.int)
        V: typing_extensions.TypeAlias = ValueType

    class _JoinAlgorithmEnumTypeWrapper(google.protobuf.internal.enum_type_wrapper._EnumTypeWrapper[UserSettings._JoinAlgorithm.ValueType], builtins.type):
        DESCRIPTOR: google.protobuf.descriptor.EnumDescriptor
        JOIN_ALGORITHM_UNSPECIFIED: UserSettings._JoinAlgorithm.ValueType  # 0
        JOIN_ALGORITHM_HASH: UserSettings._JoinAlgorithm.ValueType  # 1
        JOIN_ALGORITHM_PARALLEL_HASH: UserSettings._JoinAlgorithm.ValueType  # 2
        JOIN_ALGORITHM_PARTIAL_MERGE: UserSettings._JoinAlgorithm.ValueType  # 3
        JOIN_ALGORITHM_DIRECT: UserSettings._JoinAlgorithm.ValueType  # 4
        JOIN_ALGORITHM_AUTO: UserSettings._JoinAlgorithm.ValueType  # 5
        JOIN_ALGORITHM_FULL_SORTING_MERGE: UserSettings._JoinAlgorithm.ValueType  # 6
        JOIN_ALGORITHM_PREFER_PARTIAL_MERGE: UserSettings._JoinAlgorithm.ValueType  # 7

    class JoinAlgorithm(_JoinAlgorithm, metaclass=_JoinAlgorithmEnumTypeWrapper): ...
    JOIN_ALGORITHM_UNSPECIFIED: UserSettings.JoinAlgorithm.ValueType  # 0
    JOIN_ALGORITHM_HASH: UserSettings.JoinAlgorithm.ValueType  # 1
    JOIN_ALGORITHM_PARALLEL_HASH: UserSettings.JoinAlgorithm.ValueType  # 2
    JOIN_ALGORITHM_PARTIAL_MERGE: UserSettings.JoinAlgorithm.ValueType  # 3
    JOIN_ALGORITHM_DIRECT: UserSettings.JoinAlgorithm.ValueType  # 4
    JOIN_ALGORITHM_AUTO: UserSettings.JoinAlgorithm.ValueType  # 5
    JOIN_ALGORITHM_FULL_SORTING_MERGE: UserSettings.JoinAlgorithm.ValueType  # 6
    JOIN_ALGORITHM_PREFER_PARTIAL_MERGE: UserSettings.JoinAlgorithm.ValueType  # 7

    class _FormatRegexpEscapingRule:
        ValueType = typing.NewType("ValueType", builtins.int)
        V: typing_extensions.TypeAlias = ValueType

    class _FormatRegexpEscapingRuleEnumTypeWrapper(google.protobuf.internal.enum_type_wrapper._EnumTypeWrapper[UserSettings._FormatRegexpEscapingRule.ValueType], builtins.type):
        DESCRIPTOR: google.protobuf.descriptor.EnumDescriptor
        FORMAT_REGEXP_ESCAPING_RULE_UNSPECIFIED: UserSettings._FormatRegexpEscapingRule.ValueType  # 0
        FORMAT_REGEXP_ESCAPING_RULE_ESCAPED: UserSettings._FormatRegexpEscapingRule.ValueType  # 1
        FORMAT_REGEXP_ESCAPING_RULE_QUOTED: UserSettings._FormatRegexpEscapingRule.ValueType  # 2
        FORMAT_REGEXP_ESCAPING_RULE_CSV: UserSettings._FormatRegexpEscapingRule.ValueType  # 3
        FORMAT_REGEXP_ESCAPING_RULE_JSON: UserSettings._FormatRegexpEscapingRule.ValueType  # 4
        FORMAT_REGEXP_ESCAPING_RULE_XML: UserSettings._FormatRegexpEscapingRule.ValueType  # 5
        FORMAT_REGEXP_ESCAPING_RULE_RAW: UserSettings._FormatRegexpEscapingRule.ValueType  # 6

    class FormatRegexpEscapingRule(_FormatRegexpEscapingRule, metaclass=_FormatRegexpEscapingRuleEnumTypeWrapper): ...
    FORMAT_REGEXP_ESCAPING_RULE_UNSPECIFIED: UserSettings.FormatRegexpEscapingRule.ValueType  # 0
    FORMAT_REGEXP_ESCAPING_RULE_ESCAPED: UserSettings.FormatRegexpEscapingRule.ValueType  # 1
    FORMAT_REGEXP_ESCAPING_RULE_QUOTED: UserSettings.FormatRegexpEscapingRule.ValueType  # 2
    FORMAT_REGEXP_ESCAPING_RULE_CSV: UserSettings.FormatRegexpEscapingRule.ValueType  # 3
    FORMAT_REGEXP_ESCAPING_RULE_JSON: UserSettings.FormatRegexpEscapingRule.ValueType  # 4
    FORMAT_REGEXP_ESCAPING_RULE_XML: UserSettings.FormatRegexpEscapingRule.ValueType  # 5
    FORMAT_REGEXP_ESCAPING_RULE_RAW: UserSettings.FormatRegexpEscapingRule.ValueType  # 6

    class _DateTimeInputFormat:
        ValueType = typing.NewType("ValueType", builtins.int)
        V: typing_extensions.TypeAlias = ValueType

    class _DateTimeInputFormatEnumTypeWrapper(google.protobuf.internal.enum_type_wrapper._EnumTypeWrapper[UserSettings._DateTimeInputFormat.ValueType], builtins.type):
        DESCRIPTOR: google.protobuf.descriptor.EnumDescriptor
        DATE_TIME_INPUT_FORMAT_UNSPECIFIED: UserSettings._DateTimeInputFormat.ValueType  # 0
        DATE_TIME_INPUT_FORMAT_BEST_EFFORT: UserSettings._DateTimeInputFormat.ValueType  # 1
        DATE_TIME_INPUT_FORMAT_BASIC: UserSettings._DateTimeInputFormat.ValueType  # 2
        DATE_TIME_INPUT_FORMAT_BEST_EFFORT_US: UserSettings._DateTimeInputFormat.ValueType  # 3

    class DateTimeInputFormat(_DateTimeInputFormat, metaclass=_DateTimeInputFormatEnumTypeWrapper): ...
    DATE_TIME_INPUT_FORMAT_UNSPECIFIED: UserSettings.DateTimeInputFormat.ValueType  # 0
    DATE_TIME_INPUT_FORMAT_BEST_EFFORT: UserSettings.DateTimeInputFormat.ValueType  # 1
    DATE_TIME_INPUT_FORMAT_BASIC: UserSettings.DateTimeInputFormat.ValueType  # 2
    DATE_TIME_INPUT_FORMAT_BEST_EFFORT_US: UserSettings.DateTimeInputFormat.ValueType  # 3

    class _DateTimeOutputFormat:
        ValueType = typing.NewType("ValueType", builtins.int)
        V: typing_extensions.TypeAlias = ValueType

    class _DateTimeOutputFormatEnumTypeWrapper(google.protobuf.internal.enum_type_wrapper._EnumTypeWrapper[UserSettings._DateTimeOutputFormat.ValueType], builtins.type):
        DESCRIPTOR: google.protobuf.descriptor.EnumDescriptor
        DATE_TIME_OUTPUT_FORMAT_UNSPECIFIED: UserSettings._DateTimeOutputFormat.ValueType  # 0
        DATE_TIME_OUTPUT_FORMAT_SIMPLE: UserSettings._DateTimeOutputFormat.ValueType  # 1
        DATE_TIME_OUTPUT_FORMAT_ISO: UserSettings._DateTimeOutputFormat.ValueType  # 2
        DATE_TIME_OUTPUT_FORMAT_UNIX_TIMESTAMP: UserSettings._DateTimeOutputFormat.ValueType  # 3

    class DateTimeOutputFormat(_DateTimeOutputFormat, metaclass=_DateTimeOutputFormatEnumTypeWrapper): ...
    DATE_TIME_OUTPUT_FORMAT_UNSPECIFIED: UserSettings.DateTimeOutputFormat.ValueType  # 0
    DATE_TIME_OUTPUT_FORMAT_SIMPLE: UserSettings.DateTimeOutputFormat.ValueType  # 1
    DATE_TIME_OUTPUT_FORMAT_ISO: UserSettings.DateTimeOutputFormat.ValueType  # 2
    DATE_TIME_OUTPUT_FORMAT_UNIX_TIMESTAMP: UserSettings.DateTimeOutputFormat.ValueType  # 3

    class _LocalFilesystemReadMethod:
        ValueType = typing.NewType("ValueType", builtins.int)
        V: typing_extensions.TypeAlias = ValueType

    class _LocalFilesystemReadMethodEnumTypeWrapper(google.protobuf.internal.enum_type_wrapper._EnumTypeWrapper[UserSettings._LocalFilesystemReadMethod.ValueType], builtins.type):
        DESCRIPTOR: google.protobuf.descriptor.EnumDescriptor
        LOCAL_FILESYSTEM_READ_METHOD_UNSPECIFIED: UserSettings._LocalFilesystemReadMethod.ValueType  # 0
        LOCAL_FILESYSTEM_READ_METHOD_READ: UserSettings._LocalFilesystemReadMethod.ValueType  # 1
        LOCAL_FILESYSTEM_READ_METHOD_PREAD_THREADPOOL: UserSettings._LocalFilesystemReadMethod.ValueType  # 2
        LOCAL_FILESYSTEM_READ_METHOD_PREAD: UserSettings._LocalFilesystemReadMethod.ValueType  # 3
        LOCAL_FILESYSTEM_READ_METHOD_NMAP: UserSettings._LocalFilesystemReadMethod.ValueType  # 4

    class LocalFilesystemReadMethod(_LocalFilesystemReadMethod, metaclass=_LocalFilesystemReadMethodEnumTypeWrapper): ...
    LOCAL_FILESYSTEM_READ_METHOD_UNSPECIFIED: UserSettings.LocalFilesystemReadMethod.ValueType  # 0
    LOCAL_FILESYSTEM_READ_METHOD_READ: UserSettings.LocalFilesystemReadMethod.ValueType  # 1
    LOCAL_FILESYSTEM_READ_METHOD_PREAD_THREADPOOL: UserSettings.LocalFilesystemReadMethod.ValueType  # 2
    LOCAL_FILESYSTEM_READ_METHOD_PREAD: UserSettings.LocalFilesystemReadMethod.ValueType  # 3
    LOCAL_FILESYSTEM_READ_METHOD_NMAP: UserSettings.LocalFilesystemReadMethod.ValueType  # 4

    class _RemoteFilesystemReadMethod:
        ValueType = typing.NewType("ValueType", builtins.int)
        V: typing_extensions.TypeAlias = ValueType

    class _RemoteFilesystemReadMethodEnumTypeWrapper(google.protobuf.internal.enum_type_wrapper._EnumTypeWrapper[UserSettings._RemoteFilesystemReadMethod.ValueType], builtins.type):
        DESCRIPTOR: google.protobuf.descriptor.EnumDescriptor
        REMOTE_FILESYSTEM_READ_METHOD_UNSPECIFIED: UserSettings._RemoteFilesystemReadMethod.ValueType  # 0
        REMOTE_FILESYSTEM_READ_METHOD_READ: UserSettings._RemoteFilesystemReadMethod.ValueType  # 1
        REMOTE_FILESYSTEM_READ_METHOD_THREADPOOL: UserSettings._RemoteFilesystemReadMethod.ValueType  # 2

    class RemoteFilesystemReadMethod(_RemoteFilesystemReadMethod, metaclass=_RemoteFilesystemReadMethodEnumTypeWrapper): ...
    REMOTE_FILESYSTEM_READ_METHOD_UNSPECIFIED: UserSettings.RemoteFilesystemReadMethod.ValueType  # 0
    REMOTE_FILESYSTEM_READ_METHOD_READ: UserSettings.RemoteFilesystemReadMethod.ValueType  # 1
    REMOTE_FILESYSTEM_READ_METHOD_THREADPOOL: UserSettings.RemoteFilesystemReadMethod.ValueType  # 2

    READONLY_FIELD_NUMBER: builtins.int
    ALLOW_DDL_FIELD_NUMBER: builtins.int
    ALLOW_INTROSPECTION_FUNCTIONS_FIELD_NUMBER: builtins.int
    CONNECT_TIMEOUT_FIELD_NUMBER: builtins.int
    CONNECT_TIMEOUT_WITH_FAILOVER_FIELD_NUMBER: builtins.int
    RECEIVE_TIMEOUT_FIELD_NUMBER: builtins.int
    SEND_TIMEOUT_FIELD_NUMBER: builtins.int
    TIMEOUT_BEFORE_CHECKING_EXECUTION_SPEED_FIELD_NUMBER: builtins.int
    INSERT_QUORUM_FIELD_NUMBER: builtins.int
    INSERT_QUORUM_TIMEOUT_FIELD_NUMBER: builtins.int
    INSERT_QUORUM_PARALLEL_FIELD_NUMBER: builtins.int
    INSERT_NULL_AS_DEFAULT_FIELD_NUMBER: builtins.int
    SELECT_SEQUENTIAL_CONSISTENCY_FIELD_NUMBER: builtins.int
    DEDUPLICATE_BLOCKS_IN_DEPENDENT_MATERIALIZED_VIEWS_FIELD_NUMBER: builtins.int
    REPLICATION_ALTER_PARTITIONS_SYNC_FIELD_NUMBER: builtins.int
    MAX_REPLICA_DELAY_FOR_DISTRIBUTED_QUERIES_FIELD_NUMBER: builtins.int
    FALLBACK_TO_STALE_REPLICAS_FOR_DISTRIBUTED_QUERIES_FIELD_NUMBER: builtins.int
    DISTRIBUTED_PRODUCT_MODE_FIELD_NUMBER: builtins.int
    DISTRIBUTED_AGGREGATION_MEMORY_EFFICIENT_FIELD_NUMBER: builtins.int
    DISTRIBUTED_DDL_TASK_TIMEOUT_FIELD_NUMBER: builtins.int
    SKIP_UNAVAILABLE_SHARDS_FIELD_NUMBER: builtins.int
    COMPILE_EXPRESSIONS_FIELD_NUMBER: builtins.int
    MIN_COUNT_TO_COMPILE_EXPRESSION_FIELD_NUMBER: builtins.int
    MAX_BLOCK_SIZE_FIELD_NUMBER: builtins.int
    MIN_INSERT_BLOCK_SIZE_ROWS_FIELD_NUMBER: builtins.int
    MIN_INSERT_BLOCK_SIZE_BYTES_FIELD_NUMBER: builtins.int
    MAX_INSERT_BLOCK_SIZE_FIELD_NUMBER: builtins.int
    MIN_BYTES_TO_USE_DIRECT_IO_FIELD_NUMBER: builtins.int
    USE_UNCOMPRESSED_CACHE_FIELD_NUMBER: builtins.int
    MERGE_TREE_MAX_ROWS_TO_USE_CACHE_FIELD_NUMBER: builtins.int
    MERGE_TREE_MAX_BYTES_TO_USE_CACHE_FIELD_NUMBER: builtins.int
    MERGE_TREE_MIN_ROWS_FOR_CONCURRENT_READ_FIELD_NUMBER: builtins.int
    MERGE_TREE_MIN_BYTES_FOR_CONCURRENT_READ_FIELD_NUMBER: builtins.int
    MAX_BYTES_BEFORE_EXTERNAL_GROUP_BY_FIELD_NUMBER: builtins.int
    MAX_BYTES_BEFORE_EXTERNAL_SORT_FIELD_NUMBER: builtins.int
    GROUP_BY_TWO_LEVEL_THRESHOLD_FIELD_NUMBER: builtins.int
    GROUP_BY_TWO_LEVEL_THRESHOLD_BYTES_FIELD_NUMBER: builtins.int
    PRIORITY_FIELD_NUMBER: builtins.int
    MAX_THREADS_FIELD_NUMBER: builtins.int
    MAX_MEMORY_USAGE_FIELD_NUMBER: builtins.int
    MAX_MEMORY_USAGE_FOR_USER_FIELD_NUMBER: builtins.int
    MAX_NETWORK_BANDWIDTH_FIELD_NUMBER: builtins.int
    MAX_NETWORK_BANDWIDTH_FOR_USER_FIELD_NUMBER: builtins.int
    MAX_PARTITIONS_PER_INSERT_BLOCK_FIELD_NUMBER: builtins.int
    MAX_CONCURRENT_QUERIES_FOR_USER_FIELD_NUMBER: builtins.int
    FORCE_INDEX_BY_DATE_FIELD_NUMBER: builtins.int
    FORCE_PRIMARY_KEY_FIELD_NUMBER: builtins.int
    MAX_ROWS_TO_READ_FIELD_NUMBER: builtins.int
    MAX_BYTES_TO_READ_FIELD_NUMBER: builtins.int
    READ_OVERFLOW_MODE_FIELD_NUMBER: builtins.int
    MAX_ROWS_TO_GROUP_BY_FIELD_NUMBER: builtins.int
    GROUP_BY_OVERFLOW_MODE_FIELD_NUMBER: builtins.int
    MAX_ROWS_TO_SORT_FIELD_NUMBER: builtins.int
    MAX_BYTES_TO_SORT_FIELD_NUMBER: builtins.int
    SORT_OVERFLOW_MODE_FIELD_NUMBER: builtins.int
    MAX_RESULT_ROWS_FIELD_NUMBER: builtins.int
    MAX_RESULT_BYTES_FIELD_NUMBER: builtins.int
    RESULT_OVERFLOW_MODE_FIELD_NUMBER: builtins.int
    MAX_ROWS_IN_DISTINCT_FIELD_NUMBER: builtins.int
    MAX_BYTES_IN_DISTINCT_FIELD_NUMBER: builtins.int
    DISTINCT_OVERFLOW_MODE_FIELD_NUMBER: builtins.int
    MAX_ROWS_TO_TRANSFER_FIELD_NUMBER: builtins.int
    MAX_BYTES_TO_TRANSFER_FIELD_NUMBER: builtins.int
    TRANSFER_OVERFLOW_MODE_FIELD_NUMBER: builtins.int
    MAX_EXECUTION_TIME_FIELD_NUMBER: builtins.int
    TIMEOUT_OVERFLOW_MODE_FIELD_NUMBER: builtins.int
    MAX_ROWS_IN_SET_FIELD_NUMBER: builtins.int
    MAX_BYTES_IN_SET_FIELD_NUMBER: builtins.int
    SET_OVERFLOW_MODE_FIELD_NUMBER: builtins.int
    MAX_ROWS_IN_JOIN_FIELD_NUMBER: builtins.int
    MAX_BYTES_IN_JOIN_FIELD_NUMBER: builtins.int
    JOIN_OVERFLOW_MODE_FIELD_NUMBER: builtins.int
    JOIN_ALGORITHM_FIELD_NUMBER: builtins.int
    ANY_JOIN_DISTINCT_RIGHT_TABLE_KEYS_FIELD_NUMBER: builtins.int
    MAX_COLUMNS_TO_READ_FIELD_NUMBER: builtins.int
    MAX_TEMPORARY_COLUMNS_FIELD_NUMBER: builtins.int
    MAX_TEMPORARY_NON_CONST_COLUMNS_FIELD_NUMBER: builtins.int
    MAX_QUERY_SIZE_FIELD_NUMBER: builtins.int
    MAX_AST_DEPTH_FIELD_NUMBER: builtins.int
    MAX_AST_ELEMENTS_FIELD_NUMBER: builtins.int
    MAX_EXPANDED_AST_ELEMENTS_FIELD_NUMBER: builtins.int
    MIN_EXECUTION_SPEED_FIELD_NUMBER: builtins.int
    MIN_EXECUTION_SPEED_BYTES_FIELD_NUMBER: builtins.int
    COUNT_DISTINCT_IMPLEMENTATION_FIELD_NUMBER: builtins.int
    INPUT_FORMAT_VALUES_INTERPRET_EXPRESSIONS_FIELD_NUMBER: builtins.int
    INPUT_FORMAT_DEFAULTS_FOR_OMITTED_FIELDS_FIELD_NUMBER: builtins.int
    INPUT_FORMAT_NULL_AS_DEFAULT_FIELD_NUMBER: builtins.int
    DATE_TIME_INPUT_FORMAT_FIELD_NUMBER: builtins.int
    INPUT_FORMAT_WITH_NAMES_USE_HEADER_FIELD_NUMBER: builtins.int
    OUTPUT_FORMAT_JSON_QUOTE_64BIT_INTEGERS_FIELD_NUMBER: builtins.int
    OUTPUT_FORMAT_JSON_QUOTE_DENORMALS_FIELD_NUMBER: builtins.int
    DATE_TIME_OUTPUT_FORMAT_FIELD_NUMBER: builtins.int
    LOW_CARDINALITY_ALLOW_IN_NATIVE_FORMAT_FIELD_NUMBER: builtins.int
    ALLOW_SUSPICIOUS_LOW_CARDINALITY_TYPES_FIELD_NUMBER: builtins.int
    EMPTY_RESULT_FOR_AGGREGATION_BY_EMPTY_SET_FIELD_NUMBER: builtins.int
    HTTP_CONNECTION_TIMEOUT_FIELD_NUMBER: builtins.int
    HTTP_RECEIVE_TIMEOUT_FIELD_NUMBER: builtins.int
    HTTP_SEND_TIMEOUT_FIELD_NUMBER: builtins.int
    ENABLE_HTTP_COMPRESSION_FIELD_NUMBER: builtins.int
    SEND_PROGRESS_IN_HTTP_HEADERS_FIELD_NUMBER: builtins.int
    HTTP_HEADERS_PROGRESS_INTERVAL_FIELD_NUMBER: builtins.int
    ADD_HTTP_CORS_HEADER_FIELD_NUMBER: builtins.int
    CANCEL_HTTP_READONLY_QUERIES_ON_CLIENT_CLOSE_FIELD_NUMBER: builtins.int
    MAX_HTTP_GET_REDIRECTS_FIELD_NUMBER: builtins.int
    JOINED_SUBQUERY_REQUIRES_ALIAS_FIELD_NUMBER: builtins.int
    JOIN_USE_NULLS_FIELD_NUMBER: builtins.int
    TRANSFORM_NULL_IN_FIELD_NUMBER: builtins.int
    QUOTA_MODE_FIELD_NUMBER: builtins.int
    FLATTEN_NESTED_FIELD_NUMBER: builtins.int
    FORMAT_REGEXP_FIELD_NUMBER: builtins.int
    FORMAT_REGEXP_ESCAPING_RULE_FIELD_NUMBER: builtins.int
    FORMAT_REGEXP_SKIP_UNMATCHED_FIELD_NUMBER: builtins.int
    ASYNC_INSERT_FIELD_NUMBER: builtins.int
    ASYNC_INSERT_THREADS_FIELD_NUMBER: builtins.int
    WAIT_FOR_ASYNC_INSERT_FIELD_NUMBER: builtins.int
    WAIT_FOR_ASYNC_INSERT_TIMEOUT_FIELD_NUMBER: builtins.int
    ASYNC_INSERT_MAX_DATA_SIZE_FIELD_NUMBER: builtins.int
    ASYNC_INSERT_BUSY_TIMEOUT_FIELD_NUMBER: builtins.int
    ASYNC_INSERT_STALE_TIMEOUT_FIELD_NUMBER: builtins.int
    MEMORY_PROFILER_STEP_FIELD_NUMBER: builtins.int
    MEMORY_PROFILER_SAMPLE_PROBABILITY_FIELD_NUMBER: builtins.int
    MAX_FINAL_THREADS_FIELD_NUMBER: builtins.int
    INPUT_FORMAT_PARALLEL_PARSING_FIELD_NUMBER: builtins.int
    INPUT_FORMAT_IMPORT_NESTED_JSON_FIELD_NUMBER: builtins.int
    LOCAL_FILESYSTEM_READ_METHOD_FIELD_NUMBER: builtins.int
    MAX_READ_BUFFER_SIZE_FIELD_NUMBER: builtins.int
    INSERT_KEEPER_MAX_RETRIES_FIELD_NUMBER: builtins.int
    MAX_TEMPORARY_DATA_ON_DISK_SIZE_FOR_USER_FIELD_NUMBER: builtins.int
    MAX_TEMPORARY_DATA_ON_DISK_SIZE_FOR_QUERY_FIELD_NUMBER: builtins.int
    MAX_PARSER_DEPTH_FIELD_NUMBER: builtins.int
    REMOTE_FILESYSTEM_READ_METHOD_FIELD_NUMBER: builtins.int
    MEMORY_OVERCOMMIT_RATIO_DENOMINATOR_FIELD_NUMBER: builtins.int
    MEMORY_OVERCOMMIT_RATIO_DENOMINATOR_FOR_USER_FIELD_NUMBER: builtins.int
    MEMORY_USAGE_OVERCOMMIT_MAX_WAIT_MICROSECONDS_FIELD_NUMBER: builtins.int
    COMPILE_FIELD_NUMBER: builtins.int
    MIN_COUNT_TO_COMPILE_FIELD_NUMBER: builtins.int
    distributed_product_mode: global___UserSettings.DistributedProductMode.ValueType
    """Determine the behavior of distributed subqueries.

    See in-depth description in [ClickHouse documentation](https://clickhouse.com/docs/en/operations/settings/settings/#distributed-product-mode).
    """
    read_overflow_mode: global___UserSettings.OverflowMode.ValueType
    """Determines the behavior on exceeding [limits](https://clickhouse.com/docs/en/operations/settings/query-complexity/#restrictions-on-query-complexity) while reading the data.

    * **throw**-abort query execution, return an error.
    * **break**-stop query execution, return partial result.
    """
    group_by_overflow_mode: global___UserSettings.GroupByOverflowMode.ValueType
    """Determines the behavior on exceeding [limits](https://clickhouse.com/docs/en/operations/settings/query-complexity/#restrictions-on-query-complexity) while doing aggregation.

    * **throw**-abort query execution, return an error.
    * **break**-stop query execution, return partial result.
    * **any**-perform approximate **GROUP BY** operation by continuing aggregation for the keys that got into the set, but don't add new keys to the set.
    """
    sort_overflow_mode: global___UserSettings.OverflowMode.ValueType
    """Determines the behavior on exceeding [limits](https://clickhouse.com/docs/en/operations/settings/query-complexity/#restrictions-on-query-complexity) while sorting.

    * **throw**-abort query execution, return an error.
    * **break**-stop query execution, return partial result.
    """
    result_overflow_mode: global___UserSettings.OverflowMode.ValueType
    """Determines the behavior on exceeding [limits](https://clickhouse.com/docs/en/operations/settings/query-complexity/#restrictions-on-query-complexity) while forming result.

    * **throw**-abort query execution, return an error.
    * **break**-stop query execution, return partial result.
    """
    distinct_overflow_mode: global___UserSettings.OverflowMode.ValueType
    """Determines the behavior on exceeding [limits](https://clickhouse.com/docs/en/operations/settings/query-complexity/#restrictions-on-query-complexity) while doing **DISCTINCT**.

    * **throw**-abort query execution, return an error.
    * **break**-stop query execution, return partial result.
    """
    transfer_overflow_mode: global___UserSettings.OverflowMode.ValueType
    """Determines the behavior on exceeding [limits](https://clickhouse.com/docs/en/operations/settings/query-complexity/#restrictions-on-query-complexity) while doing transfers.

    * **throw**-abort query execution, return an error.
    * **break**-stop query execution, return partial result.
    """
    timeout_overflow_mode: global___UserSettings.OverflowMode.ValueType
    """Determines the behavior on exceeding [limits](https://clickhouse.com/docs/en/operations/settings/query-complexity/#restrictions-on-query-complexity) of execution time.

    * **throw**-abort query execution, return an error.
    * **break**-stop query execution, return partial result.
    """
    set_overflow_mode: global___UserSettings.OverflowMode.ValueType
    """Determine the behavior on exceeding max_rows_in_set or max_bytes_in_set limit.
    Possible values: OVERFLOW_MODE_THROW, OVERFLOW_MODE_BREAK.
    """
    join_overflow_mode: global___UserSettings.OverflowMode.ValueType
    """Determine the behavior on exceeding max_rows_in_join or max_bytes_in_join limit.
    Possible values: OVERFLOW_MODE_THROW, OVERFLOW_MODE_BREAK.
    """
    count_distinct_implementation: global___UserSettings.CountDistinctImplementation.ValueType
    """Aggregate function to use for implementation of count(DISTINCT ...)."""
    date_time_input_format: global___UserSettings.DateTimeInputFormat.ValueType
    """See in-depth description in [ClickHouse documentation](https://clickhouse.com/docs/en/operations/settings/settings/#date_time_input_format)."""
    date_time_output_format: global___UserSettings.DateTimeOutputFormat.ValueType
    """See in-depth description in [ClickHouse documentation](https://clickhouse.com/docs/en/operations/settings/settings/#date_time_output_format)."""
    quota_mode: global___UserSettings.QuotaMode.ValueType
    """Quota accounting mode. Possible values: QUOTA_MODE_DEFAULT, QUOTA_MODE_KEYED and QUOTA_MODE_KEYED_BY_IP."""
    format_regexp: builtins.str
    """Regular expression (for Regexp format)"""
    format_regexp_escaping_rule: global___UserSettings.FormatRegexpEscapingRule.ValueType
    """See in-depth description in [ClickHouse documentation](https://clickhouse.com/docs/en/operations/settings/settings/#format_regexp_escaping_rule)."""
    local_filesystem_read_method: global___UserSettings.LocalFilesystemReadMethod.ValueType
    """Method of reading data from local filesystem, one of: read, pread, mmap, io_uring, pread_threadpool. The 'io_uring' method is experimental and does not work for Log, TinyLog, StripeLog, File, Set and Join, and other tables with append-able files in presence of concurrent reads and writes."""
    remote_filesystem_read_method: global___UserSettings.RemoteFilesystemReadMethod.ValueType
    """Method of reading data from remote filesystem, one of: read, threadpool.
    Default: read
    Min_version: 21.11
    See in-depth description in [ClickHouse GitHub](https://github.com/ClickHouse/ClickHouse/blob/f9558345e886876b9132d9c018e357f7fa9b22a3/src/Core/Settings.h#L660)
    """
    @property
    def readonly(self) -> google.protobuf.wrappers_pb2.Int64Value:
        """Restricts permissions for non-DDL queries. To restrict permissions for DDL queries, use [allow_ddl] instead.
        * **0** (default)-no restrictions.
        * **1**-only read data queries are allowed.
        * **2**-read data and change settings queries are allowed.

        See in-depth description in [ClickHouse documentation](https://clickhouse.com/docs/en/operations/settings/permissions-for-queries/#settings_readonly).
        """

    @property
    def allow_ddl(self) -> google.protobuf.wrappers_pb2.BoolValue:
        """Determines whether DDL queries are allowed (e.g., **CREATE**, **ALTER**, **RENAME**, etc).

        Default value: **true**.

        See in-depth description in [ClickHouse documentation](https://clickhouse.com/docs/en/operations/settings/permissions-for-queries/#settings_allow_ddl).
        """

    @property
    def allow_introspection_functions(self) -> google.protobuf.wrappers_pb2.BoolValue:
        """Enables [introspections functions](https://clickhouse.com/docs/en/sql-reference/functions/introspection) for query profiling.

        More info see in [ClickHouse documentation](https://clickhouse.com/docs/en/operations/settings/settings/#settings-allow_introspection_functions).
        """

    @property
    def connect_timeout(self) -> google.protobuf.wrappers_pb2.Int64Value:
        """Connection timeout in milliseconds.

        Value must be greater than **0** (default: **10000**, 10 seconds).
        """

    @property
    def connect_timeout_with_failover(self) -> google.protobuf.wrappers_pb2.Int64Value:
        """The timeout in milliseconds for connecting to a remote server for a Distributed table engine. Applies only if the cluster uses sharding and replication. If unsuccessful, several attempts are made to connect to various replicas.

        Default value: **50**.

        More info see in [ClickHouse documentation](https://clickhouse.com/docs/en/operations/settings/settings/#connect-timeout-with-failover-ms).
        """

    @property
    def receive_timeout(self) -> google.protobuf.wrappers_pb2.Int64Value:
        """Receive timeout in milliseconds.

        Value must be greater than **0** (default: **300000**, 300 seconds or 5 minutes).
        """

    @property
    def send_timeout(self) -> google.protobuf.wrappers_pb2.Int64Value:
        """Send timeout in milliseconds.

        Value must be greater than **0** (default: **300000**, 300 seconds or 5 minutes).
        """

    @property
    def timeout_before_checking_execution_speed(self) -> google.protobuf.wrappers_pb2.Int64Value:
        """Timeout (in seconds) between checks of execution speed. It is checked that execution speed is not less that specified in [min_execution_speed] parameter.

        Default value: **10**.
        """

    @property
    def insert_quorum(self) -> google.protobuf.wrappers_pb2.Int64Value:
        """Enables or disables write quorum for ClickHouse cluster.
        If the value is less than **2**, then write quorum is disabled, otherwise it is enabled.

        When used, write quorum guarantees that ClickHouse has written data to the quorum of **insert_quorum** replicas with no errors until the [insert_quorum_timeout] expires.
        All replicas in the quorum are in the consistent state, meaning that they contain linearized data from the previous **INSERT** queries.
        Employ write quorum, if you need the guarantees that the written data would not be lost in case of one or more replicas failure.

        You can use [select_sequential_consistency] setting to read the data written with write quorum.

        See in-depth description in [ClickHouse documentation](https://clickhouse.com/docs/en/operations/settings/settings/#settings-insert_quorum).
        """

    @property
    def insert_quorum_timeout(self) -> google.protobuf.wrappers_pb2.Int64Value:
        """Quorum write timeout in milliseconds.

        If the write quorum is enabled in the cluster, this timeout expires and some data is not written to the [insert_quorum] replicas, then ClickHouse will abort the execution of **INSERT** query and return an error.
        In this case, the client must send the query again to write the data block into the same or another replica.

        Minimum value: **1000**, 1 second (default: **60000**, 1 minute).
        """

    @property
    def insert_quorum_parallel(self) -> google.protobuf.wrappers_pb2.BoolValue:
        """See in-depth description in [ClickHouse documentation](https://clickhouse.com/docs/en/operations/settings/settings/#settings-insert_quorum_parallel)."""

    @property
    def insert_null_as_default(self) -> google.protobuf.wrappers_pb2.BoolValue:
        """Enables the insertion of default values instead of NULL into columns with not nullable data type.

        Default value: **true**.

        More info see in [ClickHouse documentation](https://clickhouse.com/docs/en/operations/settings/settings/#insert_null_as_default).
        """

    @property
    def select_sequential_consistency(self) -> google.protobuf.wrappers_pb2.BoolValue:
        """Determines the behavior of **SELECT** queries from the replicated table: if enabled, ClickHouse will terminate a query with error message in case the replica does not have a chunk written with the quorum and will not read the parts that have not yet been written with the quorum.

        Default value: **false** (sequential consistency is disabled).
        """

    @property
    def deduplicate_blocks_in_dependent_materialized_views(self) -> google.protobuf.wrappers_pb2.BoolValue:
        """See in-depth description in [ClickHouse documentation](https://clickhouse.com/docs/en/operations/settings/settings/#settings-deduplicate-blocks-in-dependent-materialized-views)."""

    @property
    def replication_alter_partitions_sync(self) -> google.protobuf.wrappers_pb2.Int64Value:
        """Wait mode for asynchronous actions in **ALTER** queries on replicated tables:

        * **0**-do not wait for replicas.
        * **1**-only wait for own execution (default).
        * **2**-wait for all replicas.

        See in-depth description in [ClickHouse documentation](https://clickhouse.com/docs/en/sql-reference/statements/alter/#synchronicity-of-alter-queries).
        """

    @property
    def max_replica_delay_for_distributed_queries(self) -> google.protobuf.wrappers_pb2.Int64Value:
        """Max replica delay in milliseconds. If a replica lags more than the set value, this replica is not used and becomes a stale one.

        Minimum value: **1000**, 1 second (default: **300000**, 300 seconds or 5 minutes).

        See in-depth description in [ClickHouse documentation](https://clickhouse.com/docs/en/operations/settings/settings/#settings-max_replica_delay_for_distributed_queries).
        """

    @property
    def fallback_to_stale_replicas_for_distributed_queries(self) -> google.protobuf.wrappers_pb2.BoolValue:
        """Enables or disables query forcing to a stale replica in case the actual data is unavailable.
        If enabled, ClickHouse will choose the most up-to-date replica and force the query to use the data in this replica.
        This setting can be used when doing **SELECT** query from a distributed table that points to replicated tables.

        Default value: **true** (query forcing is enabled).

        See in-depth description in [ClickHouse documentation](https://clickhouse.com/docs/en/operations/settings/settings/#settings-fallback_to_stale_replicas_for_distributed_queries).
        """

    @property
    def distributed_aggregation_memory_efficient(self) -> google.protobuf.wrappers_pb2.BoolValue:
        """Enables of disables memory saving mode when doing distributed aggregation.

        When ClickHouse works with a distributed query, external aggregation is done on remote servers.
        Enable this setting to achieve a smaller memory footprint on the server that sourced such a distributed query.

        Default value: **false** (memory saving mode is disabled).

        See in-depth description in [ClickHouse documentation](https://clickhouse.com/docs/en/sql-reference/statements/select/group-by/#select-group-by-in-external-memory).
        """

    @property
    def distributed_ddl_task_timeout(self) -> google.protobuf.wrappers_pb2.Int64Value:
        """Timeout for DDL queries, in milliseconds."""

    @property
    def skip_unavailable_shards(self) -> google.protobuf.wrappers_pb2.BoolValue:
        """Enables or disables silent skipping of unavailable shards.

        A shard is considered unavailable if all its replicas are also unavailable.

        Default value: **false** (silent skipping is disabled).

        See in-depth description in [ClickHouse documentation](https://clickhouse.com/docs/en/operations/settings/settings/#settings-skip_unavailable_shards).
        """

    @property
    def compile_expressions(self) -> google.protobuf.wrappers_pb2.BoolValue:
        """Enables or disables expression compilation.
        If you execute a lot of queries that contain identical expressions, then enable this setting.
        As a result, such queries may be executed faster due to use of compiled expressions.

        Use this setting in combination with [min_count_to_compile_expression] setting.

        Default value: **false** (expression compilation is disabled).
        """

    @property
    def min_count_to_compile_expression(self) -> google.protobuf.wrappers_pb2.Int64Value:
        """How many identical expressions ClickHouse has to encounter before they are compiled.

        Minimum value: **0** (default: **3**).

        For the **0** value compilation is synchronous: a query waits for expression compilation process to complete prior to continuing execution.
        It is recommended to set this value only for testing purposes.

        For all other values, compilation is asynchronous: the compilation process executes in a separate thread.
        When a compiled expression is ready, it will be used by ClickHouse for eligible queries, including the ones that are currently running.
        """

    @property
    def max_block_size(self) -> google.protobuf.wrappers_pb2.Int64Value:
        """The maximum block size for reading.

        Data in ClickHouse is organized and processed by blocks (block is a set of columns' parts).
        The internal processing cycles for a single block are efficient enough, but there are noticeable expenditures on each block.

        This setting is a recommendation for size of block (in a count of rows) that should be loaded from tables.

        Value must be greater than **0** (default: **65536**).

        See in-depth description in [ClickHouse documentation](https://clickhouse.com/docs/en/operations/settings/settings/#setting-max_block_size).
        """

    @property
    def min_insert_block_size_rows(self) -> google.protobuf.wrappers_pb2.Int64Value:
        """Limits the minimum number of rows in a block to be inserted in a table by **INSERT** query.
        Blocks that are smaller than the specified value, will be squashed together into the bigger blocks.

        Minimal value: **0**, block squashing is disabled (default: **1048576**).
        """

    @property
    def min_insert_block_size_bytes(self) -> google.protobuf.wrappers_pb2.Int64Value:
        """Limits the minimum number of bytes in a block to be inserted in a table by **INSERT** query.
        Blocks that are smaller than the specified value, will be squashed together into the bigger blocks.

        Minimal value: **0**, block squashing is disabled (default: **268435456**, 256 MB).
        """

    @property
    def max_insert_block_size(self) -> google.protobuf.wrappers_pb2.Int64Value:
        """Allows to form blocks of the specified size (in bytes) when inserting data in a table.
        This setting has effect only if server is creating such blocks by itself.

        Value must be greater than **0** (default: **1048576**).

        See in-depth description in [ClickHouse documentation](https://clickhouse.com/docs/en/operations/settings/settings/#settings-max_insert_block_size).
        """

    @property
    def min_bytes_to_use_direct_io(self) -> google.protobuf.wrappers_pb2.Int64Value:
        """Limits the minimum number of bytes to enable unbuffered direct reads from disk (Direct I/O).

        By default, ClickHouse does not read data directly from disk, but relies on the filesystem and its cache instead.
        Such reading strategy is effective when the data volume is small.
        If the amount of the data to read is huge, it is more effective to read directly from the disk, bypassing the filesystem cache.

        If the total amount of the data to read is greater than the value of this setting, then ClickHouse will fetch this data directly from the disk.

        Minimal value and default value: **0**, Direct I/O is disabled.
        """

    @property
    def use_uncompressed_cache(self) -> google.protobuf.wrappers_pb2.BoolValue:
        """Determines whether to use the cache of uncompressed blocks, or not.
        Using this cache can significantly reduce latency and increase the throughput when a huge amount of small queries is to be processed.
        Enable this setting for the users who instantiates small queries frequently.

        This setting has effect only for tables of the MergeTree family.

        Default value: **false** (uncompressed cache is disabled).

        See in-depth description in [ClickHouse documentation](https://clickhouse.com/docs/en/operations/settings/settings/#setting-use_uncompressed_cache).
        """

    @property
    def merge_tree_max_rows_to_use_cache(self) -> google.protobuf.wrappers_pb2.Int64Value:
        """Limits the maximum size in rows of the request that can use the cache of uncompressed data. The cache is not used for requests larger
        than the specified value.

        Use this setting in combination with [use_uncompressed_cache] setting.

        Value must be greater than **0** (default: **128x8192**).
        """

    @property
    def merge_tree_max_bytes_to_use_cache(self) -> google.protobuf.wrappers_pb2.Int64Value:
        """Limits the maximum size in bytes of the request that can use the cache of uncompressed data. The cache is not used for requests larger
        than the specified value.

        Use this setting in combination with [use_uncompressed_cache] setting.

        Value must be greater than **0** (default: **192x10x1024x1024**).
        """

    @property
    def merge_tree_min_rows_for_concurrent_read(self) -> google.protobuf.wrappers_pb2.Int64Value:
        """Limits the minimum number of rows to be read from a file to enable concurrent read.
        If the number of rows to be read exceeds this value, then ClickHouse will try to use a few threads to read from a file concurrently.

        This setting has effect only for tables of the MergeTree family.

        Value must be greater than **0** (default: **20x8192**).
        """

    @property
    def merge_tree_min_bytes_for_concurrent_read(self) -> google.protobuf.wrappers_pb2.Int64Value:
        """Limits the number of bytes to be read from a file to enable concurrent read.
        If the number of bytes to be read exceeds this value, then ClickHouse will try to use a few threads to read from a file concurrently.

        This setting has effect only for tables of the MergeTree family.

        Value must be greater than **0** (default: **24x10x1024x1024**).
        """

    @property
    def max_bytes_before_external_group_by(self) -> google.protobuf.wrappers_pb2.Int64Value:
        """Sets the threshold of RAM consumption (in bytes) after that the temporary data, collected during the **GROUP BY** operation, should be flushed to disk to limit the RAM comsumption.

        By default, aggregation is done by employing hash table that resides in RAM.
        A query can result in aggregation of huge data volumes that can lead to memory exhaustion and abortion of the query (see the [max_memory_usage] setting).
        For such queries, you can use this setting to force ClickHouse to do flushing and complete aggregation successfully.

        Minimal value and default value: **0**, **GROUP BY** in the external memory is disabled.

        When using aggregation in external memory, it is recommended to set the value of this setting twice as low as the [max_memory_usage] setting value (by default, the maximum memory usage is limited to ten gigabytes).

        See in-depth description in [ClickHouse documentation](https://clickhouse.com/docs/en/sql-reference/statements/select/group-by/#select-group-by-in-external-memory).

        See also: the [distributed_aggregation_memory_efficient] setting.
        """

    @property
    def max_bytes_before_external_sort(self) -> google.protobuf.wrappers_pb2.Int64Value:
        """This setting is equivalent of the [max_bytes_before_external_group_by] setting, except for it is for sort operation (**ORDER BY**), not aggregation."""

    @property
    def group_by_two_level_threshold(self) -> google.protobuf.wrappers_pb2.Int64Value:
        """Sets the threshold of the number of keys, after that the two-level aggregation should be used.

        Minimal value: **0**, threshold is not set (default: **10000**).
        """

    @property
    def group_by_two_level_threshold_bytes(self) -> google.protobuf.wrappers_pb2.Int64Value:
        """Sets the threshold of the number of bytes, after that the two-level aggregation should be used.

        Minimal value: **0**, threshold is not set (default: **100000000**).
        """

    @property
    def priority(self) -> google.protobuf.wrappers_pb2.Int64Value:
        """Sets the priority of a query.

        * **0**-priority is not used.
        * **1**-the highest priority.
        * and so on. The higher the number, the lower a query's priority.

        This setting should be set up for each query individually.

        If ClickHouse is working with the high-priority queries, and a low-priority query enters, then the low-priority query is paused until higher-priority queries are completed.

        Minimal value and default value: **0**, priority is not used.
        """

    @property
    def max_threads(self) -> google.protobuf.wrappers_pb2.Int64Value:
        """Limits the maximum number of threads to process the request (setting does not take threads that read data from remote servers into account).

        This setting applies to threads that perform the same stages of the query processing pipeline in parallel.

        Minimal value and default value: **0** (the thread number is calculated automatically based on the number of physical CPU cores, no HyperThreading cores are taken into account).

        See in-depth description in [ClickHouse documentation](https://clickhouse.com/docs/en/operations/settings/settings/#settings-max_threads).
        """

    @property
    def max_memory_usage(self) -> google.protobuf.wrappers_pb2.Int64Value:
        """Limits the maximum memory usage (in bytes) for processing of a single user's query on a single server.
        This setting does not take server's free RAM amount or total RAM amount into account.

        This limitation is enforced for any user's single query on a single server.

        Minimal value: **0**, no limitation is set.
        Value that is set in the ClickHouse default config file: **10737418240** (10 GB).

        If you use [max_bytes_before_external_group_by] or [max_bytes_before_external_sort] setting, then it is recommended to set their values twice as low as [max_memory_usage] setting value.

        See in-depth description in [ClickHouse documentation](https://clickhouse.com/docs/en/operations/settings/query-complexity/#settings_max_memory_usage).
        """

    @property
    def max_memory_usage_for_user(self) -> google.protobuf.wrappers_pb2.Int64Value:
        """Limits the maximum memory usage (in bytes) for processing of user's queries on a single server.
        This setting does not take server's free RAM amount or total RAM amount into account.

        This limitation is enforced for all queries that belong to one user and run simultaneously on a single server.

        Minimal value and default value: **0**, no limitation is set.
        """

    @property
    def max_network_bandwidth(self) -> google.protobuf.wrappers_pb2.Int64Value:
        """The maximum speed of data exchange over the network in bytes per second for a query.

        Minimal value and default value: **0**, no limitation is set.
        """

    @property
    def max_network_bandwidth_for_user(self) -> google.protobuf.wrappers_pb2.Int64Value:
        """The maximum speed of data exchange over the network in bytes per second for all concurrently running user queries.

        Minimal value and default value: **0**, no limitation is set.
        """

    @property
    def max_partitions_per_insert_block(self) -> google.protobuf.wrappers_pb2.Int64Value:
        """See in-depth description in [ClickHouse documentation](https://clickhouse.com/docs/ru/operations/settings/query-complexity/#max-partitions-per-insert-block)."""

    @property
    def max_concurrent_queries_for_user(self) -> google.protobuf.wrappers_pb2.Int64Value:
        """The maximum number of concurrent requests per user.
        Default value: 0 (no limit).
        """

    @property
    def force_index_by_date(self) -> google.protobuf.wrappers_pb2.BoolValue:
        """If enabled, query is not executed if the ClickHouse can't use index by date.
        This setting has effect only for tables of the MergeTree family.

        Default value: **false** (setting is disabled, query executes even if ClickHouse can't use index by date).

        See in-depth description in [ClickHouse documentation](https://clickhouse.com/docs/en/operations/settings/settings/#settings-force_index_by_date).
        """

    @property
    def force_primary_key(self) -> google.protobuf.wrappers_pb2.BoolValue:
        """If enabled, query is not executed if the ClickHouse can't use index by primary key.
        This setting has effect only for tables of the MergeTree family.

        Default value: **false** (setting is disabled, query executes even if ClickHouse can't use index by primary key).

        See in-depth description in [ClickHouse documentation](https://clickhouse.com/docs/en/operations/settings/settings/#force-primary-key).
        """

    @property
    def max_rows_to_read(self) -> google.protobuf.wrappers_pb2.Int64Value:
        """Limits the maximum number of rows that can be read from a table when running a query.

        Minimal value and default value: **0**, no limitation is set.

        See in-depth description in [ClickHouse documentation](https://clickhouse.com/docs/en/operations/settings/query-complexity/#max-rows-to-read).
        """

    @property
    def max_bytes_to_read(self) -> google.protobuf.wrappers_pb2.Int64Value:
        """Limits the maximum number of bytes (uncompressed data) that can be read from a table when running a query.

        Minimal value and default value: **0**, no limitation is set.
        """

    @property
    def max_rows_to_group_by(self) -> google.protobuf.wrappers_pb2.Int64Value:
        """Limits the maximum number of unique keys received from aggregation function.
        This setting helps to reduce RAM consumption while doing aggregation.

        Minimal value and default value: **0**, no limitation is set.
        """

    @property
    def max_rows_to_sort(self) -> google.protobuf.wrappers_pb2.Int64Value:
        """Limits the maximum number of rows that can be read from a table for sorting.
        This setting helps to reduce RAM consumption.

        Minimal value and default value: **0**, no limitation is set.
        """

    @property
    def max_bytes_to_sort(self) -> google.protobuf.wrappers_pb2.Int64Value:
        """Limits the maximum number of bytes (uncompressed data) that can be read from a table for sorting.
        This setting helps to reduce RAM consumption.

        Minimal value and default value: **0**, no limitation is set.
        """

    @property
    def max_result_rows(self) -> google.protobuf.wrappers_pb2.Int64Value:
        """Limits the number of rows in the result.
        This limitation is also checked for subqueries and parts of distributed queries that run on remote servers.

        Minimal value and default value: **0**, no limitation is set.
        """

    @property
    def max_result_bytes(self) -> google.protobuf.wrappers_pb2.Int64Value:
        """Limits the number of bytes in the result.
        This limitation is also checked for subqueries and parts of distributed queries that run on remote servers.

        Minimal value and default value: **0**, no limitation is set.
        """

    @property
    def max_rows_in_distinct(self) -> google.protobuf.wrappers_pb2.Int64Value:
        """Limits the maximum number of different rows when using **DISTINCT**.

        Minimal value and default value: **0**, no limitation is set.
        """

    @property
    def max_bytes_in_distinct(self) -> google.protobuf.wrappers_pb2.Int64Value:
        """Limits the maximum size of a hash table in bytes (uncompressed data) when using **DISTINCT**."""

    @property
    def max_rows_to_transfer(self) -> google.protobuf.wrappers_pb2.Int64Value:
        """Limits the maximum number of rows that can be passed to a remote server or saved in a temporary table when using **GLOBAL IN**.

        Minimal value and default value: **0**, no limitation is set.
        """

    @property
    def max_bytes_to_transfer(self) -> google.protobuf.wrappers_pb2.Int64Value:
        """Limits the maximum number of bytes (uncompressed data) that can be passed to a remote server or saved in a temporary
        table when using **GLOBAL IN**.

        Minimal value and default value: **0**, no limitation is set.
        """

    @property
    def max_execution_time(self) -> google.protobuf.wrappers_pb2.Int64Value:
        """Limits the maximum query execution time in milliseconds.
        At this moment, this limitation is not checked when passing one of the sorting stages, as well as merging and finalizing aggregation funictions.

        Minimal value and default value: **0**, no limitation is set.
        """

    @property
    def max_rows_in_set(self) -> google.protobuf.wrappers_pb2.Int64Value:
        """Limit on the number of rows in the set resulting from the execution of the IN section."""

    @property
    def max_bytes_in_set(self) -> google.protobuf.wrappers_pb2.Int64Value:
        """Limit on the number of bytes in the set resulting from the execution of the IN section."""

    @property
    def max_rows_in_join(self) -> google.protobuf.wrappers_pb2.Int64Value:
        """Limit on maximum size of the hash table for JOIN, in rows."""

    @property
    def max_bytes_in_join(self) -> google.protobuf.wrappers_pb2.Int64Value:
        """Limit on maximum size of the hash table for JOIN, in bytes."""

    @property
    def join_algorithm(self) -> google.protobuf.internal.containers.RepeatedScalarFieldContainer[global___UserSettings.JoinAlgorithm.ValueType]:
        """See in-depth description in [ClickHouse documentation](https://clickhouse.com/docs/en/operations/settings/settings/#settings-join_algorithm)."""

    @property
    def any_join_distinct_right_table_keys(self) -> google.protobuf.wrappers_pb2.BoolValue:
        """See in-depth description in [ClickHouse documentation](https://clickhouse.com/docs/en/operations/settings/settings/#any_join_distinct_right_table_keys)."""

    @property
    def max_columns_to_read(self) -> google.protobuf.wrappers_pb2.Int64Value:
        """Limits the maximum number of columns that can be read from a table in a single query.
        If the query requires to read more columns to complete, then it will be aborted.

        Minimal value and default value: **0**, no limitation is set.
        """

    @property
    def max_temporary_columns(self) -> google.protobuf.wrappers_pb2.Int64Value:
        """Limits the maximum number of temporary columns that must be kept in RAM at the same time when running a query, including constant columns.

        Minimal value and default value: **0**, no limitation is set.
        """

    @property
    def max_temporary_non_const_columns(self) -> google.protobuf.wrappers_pb2.Int64Value:
        """Limits the maximum number of temporary columns that must be kept in RAM at the same time when running a query, excluding constant columns.

        Minimal value and default value: **0**, no limitation is set.
        """

    @property
    def max_query_size(self) -> google.protobuf.wrappers_pb2.Int64Value:
        """Limits the size of the part of a query that can be transferred to RAM for parsing with the SQL parser, in bytes.

        Value must be greater than **0** (default: **262144**).

        See in-depth description in [ClickHouse documentation](https://clickhouse.com/docs/en/operations/settings/settings/#settings-max_query_size).
        """

    @property
    def max_ast_depth(self) -> google.protobuf.wrappers_pb2.Int64Value:
        """Limits the maximum depth of query syntax tree.

        Executing a big and complex query may result in building a syntax tree of enormous depth.
        By using this setting, you can prohibit execution of over-sized or non-optimized queries for huge tables.

        For example, the **SELECT *** query may result in more complex and deeper syntax tree, compared to the **SELECT ... WHERE ...** query, containing constraints and conditions, in the most cases.
        A user can be forced to construct more optimized queries, if this setting is used.

        Value must be greater than **0** (default: **1000**).
        If a too small value is set, it may render ClickHouse unable to execute even simple queries.

        See in-depth description in [ClickHouse documentation](https://clickhouse.com/docs/en/operations/settings/query-complexity/#max-ast-depth).
        """

    @property
    def max_ast_elements(self) -> google.protobuf.wrappers_pb2.Int64Value:
        """Limits the maximum size of query syntax tree in number of nodes.

        Executing a big and complex query may result in building a syntax tree of enormous size.
        By using this setting, you can prohibit execution of over-sized or non-optimized queries for huge tables.

        Value must be greater than **0** (default: **50000**).
        If a too small value is set, it may render ClickHouse unable to execute even simple queries.

        See in-depth description in [ClickHouse documentation](https://clickhouse.com/docs/en/operations/settings/query-complexity/#max-ast-elements).
        """

    @property
    def max_expanded_ast_elements(self) -> google.protobuf.wrappers_pb2.Int64Value:
        """Limits the maximum size of query syntax tree in number of nodes after expansion of aliases and the asterisk values.

        Executing a big and complex query may result in building a syntax tree of enormous size.
        By using this setting, you can prohibit execution of over-sized or non-optimized queries for huge tables.

        Value must be greater than **0** (default: **500000**).
        If a too small value is set, it may render ClickHouse unable to execute even simple queries.
        """

    @property
    def min_execution_speed(self) -> google.protobuf.wrappers_pb2.Int64Value:
        """Minimal execution speed in rows per second."""

    @property
    def min_execution_speed_bytes(self) -> google.protobuf.wrappers_pb2.Int64Value:
        """Minimal execution speed in bytes per second."""

    @property
    def input_format_values_interpret_expressions(self) -> google.protobuf.wrappers_pb2.BoolValue:
        """Enables or disables SQL parser if the fast stream parser cannot parse the data.

        Enable this setting, if the data that you want to insert into a table contains SQL expressions.

        For example, the stream parser is unable to parse a value that contains **now()** expression; therefore an **INSERT** query for this value will fail and no data will be inserted into a table.
        With enabled SQL parser, this expression is parsed correctly: the **now()** expression will be parsed as SQL function, interpreted, and the current date and time will be inserted into the table as a result.

        This setting has effect only if you use [Values](https://clickhouse.com/docs/en/interfaces/formats/#data-format-values) format when inserting data.

        Default value: **true** (SQL parser is enabled).

        See in-depth description in [ClickHouse documentation](https://clickhouse.com/docs/en/operations/settings/settings/#settings-input_format_values_interpret_expressions).
        """

    @property
    def input_format_defaults_for_omitted_fields(self) -> google.protobuf.wrappers_pb2.BoolValue:
        """Enables or disables replacing omitted input values with default values of the respective columns when performing **INSERT** queries.

        Default value: **true** (replacing is enabled).
        """

    @property
    def input_format_null_as_default(self) -> google.protobuf.wrappers_pb2.BoolValue:
        """See in-depth description in [ClickHouse documentation](https://clickhouse.com/docs/en/operations/settings/settings/#input_format_null_as_default)."""

    @property
    def input_format_with_names_use_header(self) -> google.protobuf.wrappers_pb2.BoolValue:
        """See in-depth description in [ClickHouse documentation](https://clickhouse.com/docs/en/operations/settings/settings/#input_format_with_names_use_header)."""

    @property
    def output_format_json_quote_64bit_integers(self) -> google.protobuf.wrappers_pb2.BoolValue:
        """Enables quoting of 64-bit integers in JSON output format.

        If this setting is enabled, then 64-bit integers (**UInt64** and **Int64**) will be quoted when written to JSON output in order to maintain compatibility with the most of the JavaScript engines.
        Otherwise, such integers will not be quoted.

        Default value: **false** (quoting 64-bit integers is disabled).
        """

    @property
    def output_format_json_quote_denormals(self) -> google.protobuf.wrappers_pb2.BoolValue:
        """Enables special floating-point values (**+nan**, **-nan**, **+inf** and **-inf**) in JSON output format.

        Default value: **false** (special values do not present in output).
        """

    @property
    def low_cardinality_allow_in_native_format(self) -> google.protobuf.wrappers_pb2.BoolValue:
        """Determines whether to use LowCardinality type in Native format.

        * **true** (default)-yes, use.
        * **false**-convert LowCardinality columns to regular columns when doing **SELECT**, and convert regular columns to LowCardinality when doing **INSERT**.

        LowCardinality columns (aka sparse columns) store data in more effective way, compared to regular columns, by using hash tables.
        If data to insert suits this storage format, ClickHouse will place them into LowCardinality column.

        If you use a third-party ClickHouse client that can't work with LowCardinality columns, then this client will not be able to correctly interpret the result of the query that asks for data stored in LowCardinality column.
        Disable this setting to convert LowCardinality column to regular column when creating the result, so such clients will be able to process the result.

        Official ClickHouse client works with LowCardinality columns out-of-the-box.

        Default value: **true** (LowCardinality columns are used in Native format).
        """

    @property
    def allow_suspicious_low_cardinality_types(self) -> google.protobuf.wrappers_pb2.BoolValue:
        """Allows specifying **LowCardinality** modifier for types of small fixed size (8 or less) in CREATE TABLE statements. Enabling this may increase merge times and memory consumption.

        More info see in [ClickHouse documentation](https://clickhouse.com/docs/en/operations/settings/settings/#allow_suspicious_low_cardinality_types).
        """

    @property
    def empty_result_for_aggregation_by_empty_set(self) -> google.protobuf.wrappers_pb2.BoolValue:
        """Enables returning of empty result when aggregating without keys (with **GROUP BY** operation absent) on empty set (e.g., **SELECT count(*) FROM table WHERE 0**).

        * **true**-ClickHouse will return an empty result for such queries.
        * **false** (default)-ClickHouse will return a single-line result consisting of **NULL** values for aggregation functions, in accordance with SQL standard.
        """

    @property
    def http_connection_timeout(self) -> google.protobuf.wrappers_pb2.Int64Value:
        """HTTP connection timeout, in milliseconds.

        Value must be greater than **0** (default: **1000**, 1 second).
        """

    @property
    def http_receive_timeout(self) -> google.protobuf.wrappers_pb2.Int64Value:
        """HTTP receive timeout, in milliseconds.

        Value must be greater than **0** (default: **1800000**, 1800 seconds, 30 minutes).
        """

    @property
    def http_send_timeout(self) -> google.protobuf.wrappers_pb2.Int64Value:
        """HTTP send timeout, in milliseconds.

        Value must be greater than **0** (default: **1800000**, 1800 seconds, 30 minutes).
        """

    @property
    def enable_http_compression(self) -> google.protobuf.wrappers_pb2.BoolValue:
        """Enables or disables data compression in HTTP responses.

        By default, ClickHouse stores data compressed. When executing a query, its result is uncompressed.
        Use this setting to command ClickHouse to compress the result when sending it via HTTP.

        Enable this setting and add the **Accept-Encoding: <compression method>** HTTP header in a HTTP request to force compression of HTTP response from ClickHouse.

        ClickHouse support the following compression methods: **gzip**, **br** and **deflate**.

        Default value: **false** (compression is disabled).

        See in-depth description in [ClickHouse documentation](https://clickhouse.com/docs/en/interfaces/http/).
        """

    @property
    def send_progress_in_http_headers(self) -> google.protobuf.wrappers_pb2.BoolValue:
        """Enables progress notifications using **X-ClickHouse-Progress** HTTP header.

        Default value: **false** (notifications disabled).
        """

    @property
    def http_headers_progress_interval(self) -> google.protobuf.wrappers_pb2.Int64Value:
        """Minimum interval between progress notifications with **X-ClickHouse-Progress** HTTP header, in milliseconds.

        Value must be greater than **0** (default: **100**).
        """

    @property
    def add_http_cors_header(self) -> google.protobuf.wrappers_pb2.BoolValue:
        """Adds CORS header in HTTP responses.

        Default value: **false** (header is not added).
        """

    @property
    def cancel_http_readonly_queries_on_client_close(self) -> google.protobuf.wrappers_pb2.BoolValue:
        """Cancels HTTP read-only queries (e.g. SELECT) when a client closes the connection without waiting for the response.

        Default value: **false**.

        More info see in [ClickHouse documentation](https://clickhouse.com/docs/en/operations/settings/settings/#cancel-http-readonly-queries-on-client-close).
        """

    @property
    def max_http_get_redirects(self) -> google.protobuf.wrappers_pb2.Int64Value:
        """Limits the maximum number of HTTP GET redirect hops for [URL-engine](https://clickhouse.com/docs/en/engines/table-engines/special/url) tables.

        If the parameter is set to **0** (default), no hops is allowed.

        More info see in [ClickHouse documentation](https://clickhouse.com/docs/en/operations/settings/settings/#setting-max_http_get_redirects).
        """

    @property
    def joined_subquery_requires_alias(self) -> google.protobuf.wrappers_pb2.BoolValue: ...
    @property
    def join_use_nulls(self) -> google.protobuf.wrappers_pb2.BoolValue: ...
    @property
    def transform_null_in(self) -> google.protobuf.wrappers_pb2.BoolValue: ...
    @property
    def flatten_nested(self) -> google.protobuf.wrappers_pb2.BoolValue:
        """Sets the data format of a [nested](https://clickhouse.com/docs/en/sql-reference/data-types/nested-data-structures/nested) columns.

        More info see in [ClickHouse documentation](https://clickhouse.com/docs/en/operations/settings/settings/#flatten-nested).
        """

    @property
    def format_regexp_skip_unmatched(self) -> google.protobuf.wrappers_pb2.BoolValue:
        """See in-depth description in [ClickHouse documentation](https://clickhouse.com/docs/en/operations/settings/settings/#format_regexp_skip_unmatched)."""

    @property
    def async_insert(self) -> google.protobuf.wrappers_pb2.BoolValue:
        """Enables asynchronous inserts.

        Disabled by default.

        More info see in [ClickHouse documentation](https://clickhouse.com/docs/en/operations/settings/settings/#async-insert).
        """

    @property
    def async_insert_threads(self) -> google.protobuf.wrappers_pb2.Int64Value:
        """The maximum number of threads for background data parsing and insertion.

        If the parameter is set to **0**, asynchronous insertions are disabled. Default value: **16**.

        More info see in [ClickHouse documentation](https://clickhouse.com/docs/en/operations/settings/settings/#async-insert-threads).
        """

    @property
    def wait_for_async_insert(self) -> google.protobuf.wrappers_pb2.BoolValue:
        """Enables waiting for processing of asynchronous insertion. If enabled, server returns OK only after the data is inserted.

        More info see in [ClickHouse documentation](https://clickhouse.com/docs/en/operations/settings/settings/#wait-for-async-insert).
        """

    @property
    def wait_for_async_insert_timeout(self) -> google.protobuf.wrappers_pb2.Int64Value:
        """The timeout (in seconds) for waiting for processing of asynchronous insertion.

        Default value: **120**.

        More info see in [ClickHouse documentation](https://clickhouse.com/docs/en/operations/settings/settings/#wait-for-async-insert-timeout).
        """

    @property
    def async_insert_max_data_size(self) -> google.protobuf.wrappers_pb2.Int64Value:
        """The maximum size of the unparsed data in bytes collected per query before being inserted.

        If the parameter is set to **0**, asynchronous insertions are disabled. Default value: **100000**.

        More info see in [ClickHouse documentation](https://clickhouse.com/docs/en/operations/settings/settings/#async-insert-max-data-size).
        """

    @property
    def async_insert_busy_timeout(self) -> google.protobuf.wrappers_pb2.Int64Value:
        """The maximum timeout in milliseconds since the first INSERT query before inserting collected data.

        If the parameter is set to **0**, the timeout is disabled. Default value: **200**.

        More info see in [ClickHouse documentation](https://clickhouse.com/docs/en/operations/settings/settings/#async-insert-busy-timeout-ms).
        """

    @property
    def async_insert_stale_timeout(self) -> google.protobuf.wrappers_pb2.Int64Value:
        """The maximum timeout in milliseconds since the last INSERT query before dumping collected data. If enabled, the settings prolongs the [async_insert_busy_timeout] with every INSERT query as long as [async_insert_max_data_size] is not exceeded.

        More info see in [ClickHouse documentation](https://clickhouse.com/docs/en/operations/settings/settings/#async-insert-stale-timeout-ms).
        """

    @property
    def memory_profiler_step(self) -> google.protobuf.wrappers_pb2.Int64Value:
        """Memory profiler step (in bytes).

        If the next query step requires more memory than this parameter specifies, the memory profiler collects the allocating stack trace. Values lower than a few megabytes slow down query processing.

        Default value: **4194304** (4 MB). Zero means disabled memory profiler.
        """

    @property
    def memory_profiler_sample_probability(self) -> google.protobuf.wrappers_pb2.DoubleValue:
        """Collect random allocations and deallocations and write them into system.trace_log with 'MemorySample' trace_type. The probability is for every alloc/free regardless to the size of the allocation.

        Possible values: from **0** to **1**. Default: **0**.
        """

    @property
    def max_final_threads(self) -> google.protobuf.wrappers_pb2.Int64Value:
        """Sets the maximum number of parallel threads for the SELECT query data read phase with the FINAL modifier.
        See in-depth description in [ClickHouse documentation](https://clickhouse.com/docs/en/operations/settings/settings#max-final-threads).
        """

    @property
    def input_format_parallel_parsing(self) -> google.protobuf.wrappers_pb2.BoolValue:
        """Enables or disables order-preserving parallel parsing of data formats. Supported only for [TSV](https://clickhouse.com/docs/en/interfaces/formats#tabseparated), [TKSV](https://clickhouse.com/docs/en/interfaces/formats#tskv), [CSV](https://clickhouse.com/docs/en/interfaces/formats#csv) and [JSONEachRow](https://clickhouse.com/docs/en/interfaces/formats#jsoneachrow) formats.
        See in-depth description in [ClickHouse documentation](https://clickhouse.com/docs/en/operations/settings/settings#input-format-parallel-parsing)
        """

    @property
    def input_format_import_nested_json(self) -> google.protobuf.wrappers_pb2.BoolValue:
        """Enables or disables the insertion of JSON data with nested objects.
        See in-depth description in [ClickHouse documentation](https://clickhouse.com/docs/en/operations/settings/settings#input-format-parallel-parsing)
        """

    @property
    def max_read_buffer_size(self) -> google.protobuf.wrappers_pb2.Int64Value:
        """The maximum size of the buffer to read from the filesystem.
        See in-depth description in [ClickHouse documentation](https://clickhouse.com/codebrowser/ClickHouse/src/Core/Settings.h.html#DB::SettingsTraits::Data::max_read_buffer_size)
        """

    @property
    def insert_keeper_max_retries(self) -> google.protobuf.wrappers_pb2.Int64Value:
        """The setting sets the maximum number of retries for ClickHouse Keeper (or ZooKeeper) requests during insert into replicated MergeTree. Only Keeper requests which failed due to network error, Keeper session timeout, or request timeout are considered for retries.
        Default: 20 from 23.2, 0(disabled) before
        Min_version: 22.11
        See in-depth description in [ClickHouse documentation](https://clickhouse.com/docs/en/operations/settings/settings#insert_keeper_max_retries)
        """

    @property
    def max_temporary_data_on_disk_size_for_user(self) -> google.protobuf.wrappers_pb2.Int64Value:
        """The maximum amount of data consumed by temporary files on disk in bytes for all concurrently running user queries. Zero means unlimited.
        Default: 0 - unlimited
        Min_version: 22.10
        See in-depth description in [ClickHouse documentation](https://clickhouse.com/docs/en/operations/settings/query-complexity#settings_max_temporary_data_on_disk_size_for_user)
        """

    @property
    def max_temporary_data_on_disk_size_for_query(self) -> google.protobuf.wrappers_pb2.Int64Value:
        """The maximum amount of data consumed by temporary files on disk in bytes for all concurrently running queries. Zero means unlimited.
        Default: 0 - unlimited
        Min_version: 22.10
        See in-depth description in [ClickHouse documentation](https://clickhouse.com/docs/en/operations/settings/query-complexity#settings_max_temporary_data_on_disk_size_for_query)
        """

    @property
    def max_parser_depth(self) -> google.protobuf.wrappers_pb2.Int64Value:
        """Limits maximum recursion depth in the recursive descent parser. Allows controlling the stack size.
        Default: 1000
        Special: 0 - unlimited
        See in-depth description in [ClickHouse documentation](https://clickhouse.com/docs/en/operations/settings/settings#max_parser_depth)
        """

    @property
    def memory_overcommit_ratio_denominator(self) -> google.protobuf.wrappers_pb2.Int64Value:
        """It represents soft memory limit in case when hard limit is reached on user level. This value is used to compute overcommit ratio for the query. Zero means skip the query.
        Default: 1GiB
        Min_version: 22.5
        See in-depth description in [ClickHouse documentation](https://clickhouse.com/docs/en/operations/settings/settings#memory_overcommit_ratio_denominator)
        """

    @property
    def memory_overcommit_ratio_denominator_for_user(self) -> google.protobuf.wrappers_pb2.Int64Value:
        """It represents soft memory limit in case when hard limit is reached on global level. This value is used to compute overcommit ratio for the query. Zero means skip the query.
        Default: 1GiB
        Min_version: 22.5
        See in-depth description in [ClickHouse documentation](https://clickhouse.com/docs/en/operations/settings/settings#memory_overcommit_ratio_denominator_for_user)
        """

    @property
    def memory_usage_overcommit_max_wait_microseconds(self) -> google.protobuf.wrappers_pb2.Int64Value:
        """Maximum time thread will wait for memory to be freed in the case of memory overcommit on a user level. If the timeout is reached and memory is not freed, an exception is thrown.
        Default: 5000000
        Min_version: 22.5
        See in-depth description in [ClickHouse documentation](https://clickhouse.com/docs/en/operations/settings/settings#memory_usage_overcommit_max_wait_microseconds)
        """

    @property
    def compile(self) -> google.protobuf.wrappers_pb2.BoolValue:
        """The setting is deprecated and has no effect."""

    @property
    def min_count_to_compile(self) -> google.protobuf.wrappers_pb2.Int64Value:
        """The setting is deprecated and has no effect."""

    def __init__(
        self,
        *,
        readonly: google.protobuf.wrappers_pb2.Int64Value | None = ...,
        allow_ddl: google.protobuf.wrappers_pb2.BoolValue | None = ...,
        allow_introspection_functions: google.protobuf.wrappers_pb2.BoolValue | None = ...,
        connect_timeout: google.protobuf.wrappers_pb2.Int64Value | None = ...,
        connect_timeout_with_failover: google.protobuf.wrappers_pb2.Int64Value | None = ...,
        receive_timeout: google.protobuf.wrappers_pb2.Int64Value | None = ...,
        send_timeout: google.protobuf.wrappers_pb2.Int64Value | None = ...,
        timeout_before_checking_execution_speed: google.protobuf.wrappers_pb2.Int64Value | None = ...,
        insert_quorum: google.protobuf.wrappers_pb2.Int64Value | None = ...,
        insert_quorum_timeout: google.protobuf.wrappers_pb2.Int64Value | None = ...,
        insert_quorum_parallel: google.protobuf.wrappers_pb2.BoolValue | None = ...,
        insert_null_as_default: google.protobuf.wrappers_pb2.BoolValue | None = ...,
        select_sequential_consistency: google.protobuf.wrappers_pb2.BoolValue | None = ...,
        deduplicate_blocks_in_dependent_materialized_views: google.protobuf.wrappers_pb2.BoolValue | None = ...,
        replication_alter_partitions_sync: google.protobuf.wrappers_pb2.Int64Value | None = ...,
        max_replica_delay_for_distributed_queries: google.protobuf.wrappers_pb2.Int64Value | None = ...,
        fallback_to_stale_replicas_for_distributed_queries: google.protobuf.wrappers_pb2.BoolValue | None = ...,
        distributed_product_mode: global___UserSettings.DistributedProductMode.ValueType = ...,
        distributed_aggregation_memory_efficient: google.protobuf.wrappers_pb2.BoolValue | None = ...,
        distributed_ddl_task_timeout: google.protobuf.wrappers_pb2.Int64Value | None = ...,
        skip_unavailable_shards: google.protobuf.wrappers_pb2.BoolValue | None = ...,
        compile_expressions: google.protobuf.wrappers_pb2.BoolValue | None = ...,
        min_count_to_compile_expression: google.protobuf.wrappers_pb2.Int64Value | None = ...,
        max_block_size: google.protobuf.wrappers_pb2.Int64Value | None = ...,
        min_insert_block_size_rows: google.protobuf.wrappers_pb2.Int64Value | None = ...,
        min_insert_block_size_bytes: google.protobuf.wrappers_pb2.Int64Value | None = ...,
        max_insert_block_size: google.protobuf.wrappers_pb2.Int64Value | None = ...,
        min_bytes_to_use_direct_io: google.protobuf.wrappers_pb2.Int64Value | None = ...,
        use_uncompressed_cache: google.protobuf.wrappers_pb2.BoolValue | None = ...,
        merge_tree_max_rows_to_use_cache: google.protobuf.wrappers_pb2.Int64Value | None = ...,
        merge_tree_max_bytes_to_use_cache: google.protobuf.wrappers_pb2.Int64Value | None = ...,
        merge_tree_min_rows_for_concurrent_read: google.protobuf.wrappers_pb2.Int64Value | None = ...,
        merge_tree_min_bytes_for_concurrent_read: google.protobuf.wrappers_pb2.Int64Value | None = ...,
        max_bytes_before_external_group_by: google.protobuf.wrappers_pb2.Int64Value | None = ...,
        max_bytes_before_external_sort: google.protobuf.wrappers_pb2.Int64Value | None = ...,
        group_by_two_level_threshold: google.protobuf.wrappers_pb2.Int64Value | None = ...,
        group_by_two_level_threshold_bytes: google.protobuf.wrappers_pb2.Int64Value | None = ...,
        priority: google.protobuf.wrappers_pb2.Int64Value | None = ...,
        max_threads: google.protobuf.wrappers_pb2.Int64Value | None = ...,
        max_memory_usage: google.protobuf.wrappers_pb2.Int64Value | None = ...,
        max_memory_usage_for_user: google.protobuf.wrappers_pb2.Int64Value | None = ...,
        max_network_bandwidth: google.protobuf.wrappers_pb2.Int64Value | None = ...,
        max_network_bandwidth_for_user: google.protobuf.wrappers_pb2.Int64Value | None = ...,
        max_partitions_per_insert_block: google.protobuf.wrappers_pb2.Int64Value | None = ...,
        max_concurrent_queries_for_user: google.protobuf.wrappers_pb2.Int64Value | None = ...,
        force_index_by_date: google.protobuf.wrappers_pb2.BoolValue | None = ...,
        force_primary_key: google.protobuf.wrappers_pb2.BoolValue | None = ...,
        max_rows_to_read: google.protobuf.wrappers_pb2.Int64Value | None = ...,
        max_bytes_to_read: google.protobuf.wrappers_pb2.Int64Value | None = ...,
        read_overflow_mode: global___UserSettings.OverflowMode.ValueType = ...,
        max_rows_to_group_by: google.protobuf.wrappers_pb2.Int64Value | None = ...,
        group_by_overflow_mode: global___UserSettings.GroupByOverflowMode.ValueType = ...,
        max_rows_to_sort: google.protobuf.wrappers_pb2.Int64Value | None = ...,
        max_bytes_to_sort: google.protobuf.wrappers_pb2.Int64Value | None = ...,
        sort_overflow_mode: global___UserSettings.OverflowMode.ValueType = ...,
        max_result_rows: google.protobuf.wrappers_pb2.Int64Value | None = ...,
        max_result_bytes: google.protobuf.wrappers_pb2.Int64Value | None = ...,
        result_overflow_mode: global___UserSettings.OverflowMode.ValueType = ...,
        max_rows_in_distinct: google.protobuf.wrappers_pb2.Int64Value | None = ...,
        max_bytes_in_distinct: google.protobuf.wrappers_pb2.Int64Value | None = ...,
        distinct_overflow_mode: global___UserSettings.OverflowMode.ValueType = ...,
        max_rows_to_transfer: google.protobuf.wrappers_pb2.Int64Value | None = ...,
        max_bytes_to_transfer: google.protobuf.wrappers_pb2.Int64Value | None = ...,
        transfer_overflow_mode: global___UserSettings.OverflowMode.ValueType = ...,
        max_execution_time: google.protobuf.wrappers_pb2.Int64Value | None = ...,
        timeout_overflow_mode: global___UserSettings.OverflowMode.ValueType = ...,
        max_rows_in_set: google.protobuf.wrappers_pb2.Int64Value | None = ...,
        max_bytes_in_set: google.protobuf.wrappers_pb2.Int64Value | None = ...,
        set_overflow_mode: global___UserSettings.OverflowMode.ValueType = ...,
        max_rows_in_join: google.protobuf.wrappers_pb2.Int64Value | None = ...,
        max_bytes_in_join: google.protobuf.wrappers_pb2.Int64Value | None = ...,
        join_overflow_mode: global___UserSettings.OverflowMode.ValueType = ...,
        join_algorithm: collections.abc.Iterable[global___UserSettings.JoinAlgorithm.ValueType] | None = ...,
        any_join_distinct_right_table_keys: google.protobuf.wrappers_pb2.BoolValue | None = ...,
        max_columns_to_read: google.protobuf.wrappers_pb2.Int64Value | None = ...,
        max_temporary_columns: google.protobuf.wrappers_pb2.Int64Value | None = ...,
        max_temporary_non_const_columns: google.protobuf.wrappers_pb2.Int64Value | None = ...,
        max_query_size: google.protobuf.wrappers_pb2.Int64Value | None = ...,
        max_ast_depth: google.protobuf.wrappers_pb2.Int64Value | None = ...,
        max_ast_elements: google.protobuf.wrappers_pb2.Int64Value | None = ...,
        max_expanded_ast_elements: google.protobuf.wrappers_pb2.Int64Value | None = ...,
        min_execution_speed: google.protobuf.wrappers_pb2.Int64Value | None = ...,
        min_execution_speed_bytes: google.protobuf.wrappers_pb2.Int64Value | None = ...,
        count_distinct_implementation: global___UserSettings.CountDistinctImplementation.ValueType = ...,
        input_format_values_interpret_expressions: google.protobuf.wrappers_pb2.BoolValue | None = ...,
        input_format_defaults_for_omitted_fields: google.protobuf.wrappers_pb2.BoolValue | None = ...,
        input_format_null_as_default: google.protobuf.wrappers_pb2.BoolValue | None = ...,
        date_time_input_format: global___UserSettings.DateTimeInputFormat.ValueType = ...,
        input_format_with_names_use_header: google.protobuf.wrappers_pb2.BoolValue | None = ...,
        output_format_json_quote_64bit_integers: google.protobuf.wrappers_pb2.BoolValue | None = ...,
        output_format_json_quote_denormals: google.protobuf.wrappers_pb2.BoolValue | None = ...,
        date_time_output_format: global___UserSettings.DateTimeOutputFormat.ValueType = ...,
        low_cardinality_allow_in_native_format: google.protobuf.wrappers_pb2.BoolValue | None = ...,
        allow_suspicious_low_cardinality_types: google.protobuf.wrappers_pb2.BoolValue | None = ...,
        empty_result_for_aggregation_by_empty_set: google.protobuf.wrappers_pb2.BoolValue | None = ...,
        http_connection_timeout: google.protobuf.wrappers_pb2.Int64Value | None = ...,
        http_receive_timeout: google.protobuf.wrappers_pb2.Int64Value | None = ...,
        http_send_timeout: google.protobuf.wrappers_pb2.Int64Value | None = ...,
        enable_http_compression: google.protobuf.wrappers_pb2.BoolValue | None = ...,
        send_progress_in_http_headers: google.protobuf.wrappers_pb2.BoolValue | None = ...,
        http_headers_progress_interval: google.protobuf.wrappers_pb2.Int64Value | None = ...,
        add_http_cors_header: google.protobuf.wrappers_pb2.BoolValue | None = ...,
        cancel_http_readonly_queries_on_client_close: google.protobuf.wrappers_pb2.BoolValue | None = ...,
        max_http_get_redirects: google.protobuf.wrappers_pb2.Int64Value | None = ...,
        joined_subquery_requires_alias: google.protobuf.wrappers_pb2.BoolValue | None = ...,
        join_use_nulls: google.protobuf.wrappers_pb2.BoolValue | None = ...,
        transform_null_in: google.protobuf.wrappers_pb2.BoolValue | None = ...,
        quota_mode: global___UserSettings.QuotaMode.ValueType = ...,
        flatten_nested: google.protobuf.wrappers_pb2.BoolValue | None = ...,
        format_regexp: builtins.str = ...,
        format_regexp_escaping_rule: global___UserSettings.FormatRegexpEscapingRule.ValueType = ...,
        format_regexp_skip_unmatched: google.protobuf.wrappers_pb2.BoolValue | None = ...,
        async_insert: google.protobuf.wrappers_pb2.BoolValue | None = ...,
        async_insert_threads: google.protobuf.wrappers_pb2.Int64Value | None = ...,
        wait_for_async_insert: google.protobuf.wrappers_pb2.BoolValue | None = ...,
        wait_for_async_insert_timeout: google.protobuf.wrappers_pb2.Int64Value | None = ...,
        async_insert_max_data_size: google.protobuf.wrappers_pb2.Int64Value | None = ...,
        async_insert_busy_timeout: google.protobuf.wrappers_pb2.Int64Value | None = ...,
        async_insert_stale_timeout: google.protobuf.wrappers_pb2.Int64Value | None = ...,
        memory_profiler_step: google.protobuf.wrappers_pb2.Int64Value | None = ...,
        memory_profiler_sample_probability: google.protobuf.wrappers_pb2.DoubleValue | None = ...,
        max_final_threads: google.protobuf.wrappers_pb2.Int64Value | None = ...,
        input_format_parallel_parsing: google.protobuf.wrappers_pb2.BoolValue | None = ...,
        input_format_import_nested_json: google.protobuf.wrappers_pb2.BoolValue | None = ...,
        local_filesystem_read_method: global___UserSettings.LocalFilesystemReadMethod.ValueType = ...,
        max_read_buffer_size: google.protobuf.wrappers_pb2.Int64Value | None = ...,
        insert_keeper_max_retries: google.protobuf.wrappers_pb2.Int64Value | None = ...,
        max_temporary_data_on_disk_size_for_user: google.protobuf.wrappers_pb2.Int64Value | None = ...,
        max_temporary_data_on_disk_size_for_query: google.protobuf.wrappers_pb2.Int64Value | None = ...,
        max_parser_depth: google.protobuf.wrappers_pb2.Int64Value | None = ...,
        remote_filesystem_read_method: global___UserSettings.RemoteFilesystemReadMethod.ValueType = ...,
        memory_overcommit_ratio_denominator: google.protobuf.wrappers_pb2.Int64Value | None = ...,
        memory_overcommit_ratio_denominator_for_user: google.protobuf.wrappers_pb2.Int64Value | None = ...,
        memory_usage_overcommit_max_wait_microseconds: google.protobuf.wrappers_pb2.Int64Value | None = ...,
        compile: google.protobuf.wrappers_pb2.BoolValue | None = ...,
        min_count_to_compile: google.protobuf.wrappers_pb2.Int64Value | None = ...,
    ) -> None: ...
    def HasField(self, field_name: typing.Literal["add_http_cors_header", b"add_http_cors_header", "allow_ddl", b"allow_ddl", "allow_introspection_functions", b"allow_introspection_functions", "allow_suspicious_low_cardinality_types", b"allow_suspicious_low_cardinality_types", "any_join_distinct_right_table_keys", b"any_join_distinct_right_table_keys", "async_insert", b"async_insert", "async_insert_busy_timeout", b"async_insert_busy_timeout", "async_insert_max_data_size", b"async_insert_max_data_size", "async_insert_stale_timeout", b"async_insert_stale_timeout", "async_insert_threads", b"async_insert_threads", "cancel_http_readonly_queries_on_client_close", b"cancel_http_readonly_queries_on_client_close", "compile", b"compile", "compile_expressions", b"compile_expressions", "connect_timeout", b"connect_timeout", "connect_timeout_with_failover", b"connect_timeout_with_failover", "deduplicate_blocks_in_dependent_materialized_views", b"deduplicate_blocks_in_dependent_materialized_views", "distributed_aggregation_memory_efficient", b"distributed_aggregation_memory_efficient", "distributed_ddl_task_timeout", b"distributed_ddl_task_timeout", "empty_result_for_aggregation_by_empty_set", b"empty_result_for_aggregation_by_empty_set", "enable_http_compression", b"enable_http_compression", "fallback_to_stale_replicas_for_distributed_queries", b"fallback_to_stale_replicas_for_distributed_queries", "flatten_nested", b"flatten_nested", "force_index_by_date", b"force_index_by_date", "force_primary_key", b"force_primary_key", "format_regexp_skip_unmatched", b"format_regexp_skip_unmatched", "group_by_two_level_threshold", b"group_by_two_level_threshold", "group_by_two_level_threshold_bytes", b"group_by_two_level_threshold_bytes", "http_connection_timeout", b"http_connection_timeout", "http_headers_progress_interval", b"http_headers_progress_interval", "http_receive_timeout", b"http_receive_timeout", "http_send_timeout", b"http_send_timeout", "input_format_defaults_for_omitted_fields", b"input_format_defaults_for_omitted_fields", "input_format_import_nested_json", b"input_format_import_nested_json", "input_format_null_as_default", b"input_format_null_as_default", "input_format_parallel_parsing", b"input_format_parallel_parsing", "input_format_values_interpret_expressions", b"input_format_values_interpret_expressions", "input_format_with_names_use_header", b"input_format_with_names_use_header", "insert_keeper_max_retries", b"insert_keeper_max_retries", "insert_null_as_default", b"insert_null_as_default", "insert_quorum", b"insert_quorum", "insert_quorum_parallel", b"insert_quorum_parallel", "insert_quorum_timeout", b"insert_quorum_timeout", "join_use_nulls", b"join_use_nulls", "joined_subquery_requires_alias", b"joined_subquery_requires_alias", "low_cardinality_allow_in_native_format", b"low_cardinality_allow_in_native_format", "max_ast_depth", b"max_ast_depth", "max_ast_elements", b"max_ast_elements", "max_block_size", b"max_block_size", "max_bytes_before_external_group_by", b"max_bytes_before_external_group_by", "max_bytes_before_external_sort", b"max_bytes_before_external_sort", "max_bytes_in_distinct", b"max_bytes_in_distinct", "max_bytes_in_join", b"max_bytes_in_join", "max_bytes_in_set", b"max_bytes_in_set", "max_bytes_to_read", b"max_bytes_to_read", "max_bytes_to_sort", b"max_bytes_to_sort", "max_bytes_to_transfer", b"max_bytes_to_transfer", "max_columns_to_read", b"max_columns_to_read", "max_concurrent_queries_for_user", b"max_concurrent_queries_for_user", "max_execution_time", b"max_execution_time", "max_expanded_ast_elements", b"max_expanded_ast_elements", "max_final_threads", b"max_final_threads", "max_http_get_redirects", b"max_http_get_redirects", "max_insert_block_size", b"max_insert_block_size", "max_memory_usage", b"max_memory_usage", "max_memory_usage_for_user", b"max_memory_usage_for_user", "max_network_bandwidth", b"max_network_bandwidth", "max_network_bandwidth_for_user", b"max_network_bandwidth_for_user", "max_parser_depth", b"max_parser_depth", "max_partitions_per_insert_block", b"max_partitions_per_insert_block", "max_query_size", b"max_query_size", "max_read_buffer_size", b"max_read_buffer_size", "max_replica_delay_for_distributed_queries", b"max_replica_delay_for_distributed_queries", "max_result_bytes", b"max_result_bytes", "max_result_rows", b"max_result_rows", "max_rows_in_distinct", b"max_rows_in_distinct", "max_rows_in_join", b"max_rows_in_join", "max_rows_in_set", b"max_rows_in_set", "max_rows_to_group_by", b"max_rows_to_group_by", "max_rows_to_read", b"max_rows_to_read", "max_rows_to_sort", b"max_rows_to_sort", "max_rows_to_transfer", b"max_rows_to_transfer", "max_temporary_columns", b"max_temporary_columns", "max_temporary_data_on_disk_size_for_query", b"max_temporary_data_on_disk_size_for_query", "max_temporary_data_on_disk_size_for_user", b"max_temporary_data_on_disk_size_for_user", "max_temporary_non_const_columns", b"max_temporary_non_const_columns", "max_threads", b"max_threads", "memory_overcommit_ratio_denominator", b"memory_overcommit_ratio_denominator", "memory_overcommit_ratio_denominator_for_user", b"memory_overcommit_ratio_denominator_for_user", "memory_profiler_sample_probability", b"memory_profiler_sample_probability", "memory_profiler_step", b"memory_profiler_step", "memory_usage_overcommit_max_wait_microseconds", b"memory_usage_overcommit_max_wait_microseconds", "merge_tree_max_bytes_to_use_cache", b"merge_tree_max_bytes_to_use_cache", "merge_tree_max_rows_to_use_cache", b"merge_tree_max_rows_to_use_cache", "merge_tree_min_bytes_for_concurrent_read", b"merge_tree_min_bytes_for_concurrent_read", "merge_tree_min_rows_for_concurrent_read", b"merge_tree_min_rows_for_concurrent_read", "min_bytes_to_use_direct_io", b"min_bytes_to_use_direct_io", "min_count_to_compile", b"min_count_to_compile", "min_count_to_compile_expression", b"min_count_to_compile_expression", "min_execution_speed", b"min_execution_speed", "min_execution_speed_bytes", b"min_execution_speed_bytes", "min_insert_block_size_bytes", b"min_insert_block_size_bytes", "min_insert_block_size_rows", b"min_insert_block_size_rows", "output_format_json_quote_64bit_integers", b"output_format_json_quote_64bit_integers", "output_format_json_quote_denormals", b"output_format_json_quote_denormals", "priority", b"priority", "readonly", b"readonly", "receive_timeout", b"receive_timeout", "replication_alter_partitions_sync", b"replication_alter_partitions_sync", "select_sequential_consistency", b"select_sequential_consistency", "send_progress_in_http_headers", b"send_progress_in_http_headers", "send_timeout", b"send_timeout", "skip_unavailable_shards", b"skip_unavailable_shards", "timeout_before_checking_execution_speed", b"timeout_before_checking_execution_speed", "transform_null_in", b"transform_null_in", "use_uncompressed_cache", b"use_uncompressed_cache", "wait_for_async_insert", b"wait_for_async_insert", "wait_for_async_insert_timeout", b"wait_for_async_insert_timeout"]) -> builtins.bool: ...
    def ClearField(self, field_name: typing.Literal["add_http_cors_header", b"add_http_cors_header", "allow_ddl", b"allow_ddl", "allow_introspection_functions", b"allow_introspection_functions", "allow_suspicious_low_cardinality_types", b"allow_suspicious_low_cardinality_types", "any_join_distinct_right_table_keys", b"any_join_distinct_right_table_keys", "async_insert", b"async_insert", "async_insert_busy_timeout", b"async_insert_busy_timeout", "async_insert_max_data_size", b"async_insert_max_data_size", "async_insert_stale_timeout", b"async_insert_stale_timeout", "async_insert_threads", b"async_insert_threads", "cancel_http_readonly_queries_on_client_close", b"cancel_http_readonly_queries_on_client_close", "compile", b"compile", "compile_expressions", b"compile_expressions", "connect_timeout", b"connect_timeout", "connect_timeout_with_failover", b"connect_timeout_with_failover", "count_distinct_implementation", b"count_distinct_implementation", "date_time_input_format", b"date_time_input_format", "date_time_output_format", b"date_time_output_format", "deduplicate_blocks_in_dependent_materialized_views", b"deduplicate_blocks_in_dependent_materialized_views", "distinct_overflow_mode", b"distinct_overflow_mode", "distributed_aggregation_memory_efficient", b"distributed_aggregation_memory_efficient", "distributed_ddl_task_timeout", b"distributed_ddl_task_timeout", "distributed_product_mode", b"distributed_product_mode", "empty_result_for_aggregation_by_empty_set", b"empty_result_for_aggregation_by_empty_set", "enable_http_compression", b"enable_http_compression", "fallback_to_stale_replicas_for_distributed_queries", b"fallback_to_stale_replicas_for_distributed_queries", "flatten_nested", b"flatten_nested", "force_index_by_date", b"force_index_by_date", "force_primary_key", b"force_primary_key", "format_regexp", b"format_regexp", "format_regexp_escaping_rule", b"format_regexp_escaping_rule", "format_regexp_skip_unmatched", b"format_regexp_skip_unmatched", "group_by_overflow_mode", b"group_by_overflow_mode", "group_by_two_level_threshold", b"group_by_two_level_threshold", "group_by_two_level_threshold_bytes", b"group_by_two_level_threshold_bytes", "http_connection_timeout", b"http_connection_timeout", "http_headers_progress_interval", b"http_headers_progress_interval", "http_receive_timeout", b"http_receive_timeout", "http_send_timeout", b"http_send_timeout", "input_format_defaults_for_omitted_fields", b"input_format_defaults_for_omitted_fields", "input_format_import_nested_json", b"input_format_import_nested_json", "input_format_null_as_default", b"input_format_null_as_default", "input_format_parallel_parsing", b"input_format_parallel_parsing", "input_format_values_interpret_expressions", b"input_format_values_interpret_expressions", "input_format_with_names_use_header", b"input_format_with_names_use_header", "insert_keeper_max_retries", b"insert_keeper_max_retries", "insert_null_as_default", b"insert_null_as_default", "insert_quorum", b"insert_quorum", "insert_quorum_parallel", b"insert_quorum_parallel", "insert_quorum_timeout", b"insert_quorum_timeout", "join_algorithm", b"join_algorithm", "join_overflow_mode", b"join_overflow_mode", "join_use_nulls", b"join_use_nulls", "joined_subquery_requires_alias", b"joined_subquery_requires_alias", "local_filesystem_read_method", b"local_filesystem_read_method", "low_cardinality_allow_in_native_format", b"low_cardinality_allow_in_native_format", "max_ast_depth", b"max_ast_depth", "max_ast_elements", b"max_ast_elements", "max_block_size", b"max_block_size", "max_bytes_before_external_group_by", b"max_bytes_before_external_group_by", "max_bytes_before_external_sort", b"max_bytes_before_external_sort", "max_bytes_in_distinct", b"max_bytes_in_distinct", "max_bytes_in_join", b"max_bytes_in_join", "max_bytes_in_set", b"max_bytes_in_set", "max_bytes_to_read", b"max_bytes_to_read", "max_bytes_to_sort", b"max_bytes_to_sort", "max_bytes_to_transfer", b"max_bytes_to_transfer", "max_columns_to_read", b"max_columns_to_read", "max_concurrent_queries_for_user", b"max_concurrent_queries_for_user", "max_execution_time", b"max_execution_time", "max_expanded_ast_elements", b"max_expanded_ast_elements", "max_final_threads", b"max_final_threads", "max_http_get_redirects", b"max_http_get_redirects", "max_insert_block_size", b"max_insert_block_size", "max_memory_usage", b"max_memory_usage", "max_memory_usage_for_user", b"max_memory_usage_for_user", "max_network_bandwidth", b"max_network_bandwidth", "max_network_bandwidth_for_user", b"max_network_bandwidth_for_user", "max_parser_depth", b"max_parser_depth", "max_partitions_per_insert_block", b"max_partitions_per_insert_block", "max_query_size", b"max_query_size", "max_read_buffer_size", b"max_read_buffer_size", "max_replica_delay_for_distributed_queries", b"max_replica_delay_for_distributed_queries", "max_result_bytes", b"max_result_bytes", "max_result_rows", b"max_result_rows", "max_rows_in_distinct", b"max_rows_in_distinct", "max_rows_in_join", b"max_rows_in_join", "max_rows_in_set", b"max_rows_in_set", "max_rows_to_group_by", b"max_rows_to_group_by", "max_rows_to_read", b"max_rows_to_read", "max_rows_to_sort", b"max_rows_to_sort", "max_rows_to_transfer", b"max_rows_to_transfer", "max_temporary_columns", b"max_temporary_columns", "max_temporary_data_on_disk_size_for_query", b"max_temporary_data_on_disk_size_for_query", "max_temporary_data_on_disk_size_for_user", b"max_temporary_data_on_disk_size_for_user", "max_temporary_non_const_columns", b"max_temporary_non_const_columns", "max_threads", b"max_threads", "memory_overcommit_ratio_denominator", b"memory_overcommit_ratio_denominator", "memory_overcommit_ratio_denominator_for_user", b"memory_overcommit_ratio_denominator_for_user", "memory_profiler_sample_probability", b"memory_profiler_sample_probability", "memory_profiler_step", b"memory_profiler_step", "memory_usage_overcommit_max_wait_microseconds", b"memory_usage_overcommit_max_wait_microseconds", "merge_tree_max_bytes_to_use_cache", b"merge_tree_max_bytes_to_use_cache", "merge_tree_max_rows_to_use_cache", b"merge_tree_max_rows_to_use_cache", "merge_tree_min_bytes_for_concurrent_read", b"merge_tree_min_bytes_for_concurrent_read", "merge_tree_min_rows_for_concurrent_read", b"merge_tree_min_rows_for_concurrent_read", "min_bytes_to_use_direct_io", b"min_bytes_to_use_direct_io", "min_count_to_compile", b"min_count_to_compile", "min_count_to_compile_expression", b"min_count_to_compile_expression", "min_execution_speed", b"min_execution_speed", "min_execution_speed_bytes", b"min_execution_speed_bytes", "min_insert_block_size_bytes", b"min_insert_block_size_bytes", "min_insert_block_size_rows", b"min_insert_block_size_rows", "output_format_json_quote_64bit_integers", b"output_format_json_quote_64bit_integers", "output_format_json_quote_denormals", b"output_format_json_quote_denormals", "priority", b"priority", "quota_mode", b"quota_mode", "read_overflow_mode", b"read_overflow_mode", "readonly", b"readonly", "receive_timeout", b"receive_timeout", "remote_filesystem_read_method", b"remote_filesystem_read_method", "replication_alter_partitions_sync", b"replication_alter_partitions_sync", "result_overflow_mode", b"result_overflow_mode", "select_sequential_consistency", b"select_sequential_consistency", "send_progress_in_http_headers", b"send_progress_in_http_headers", "send_timeout", b"send_timeout", "set_overflow_mode", b"set_overflow_mode", "skip_unavailable_shards", b"skip_unavailable_shards", "sort_overflow_mode", b"sort_overflow_mode", "timeout_before_checking_execution_speed", b"timeout_before_checking_execution_speed", "timeout_overflow_mode", b"timeout_overflow_mode", "transfer_overflow_mode", b"transfer_overflow_mode", "transform_null_in", b"transform_null_in", "use_uncompressed_cache", b"use_uncompressed_cache", "wait_for_async_insert", b"wait_for_async_insert", "wait_for_async_insert_timeout", b"wait_for_async_insert_timeout"]) -> None: ...

global___UserSettings = UserSettings

@typing.final
class UserQuota(google.protobuf.message.Message):
    """ClickHouse quota representation. Each quota associated with an user and limits it resource usage for an interval.
    See in-depth description [ClickHouse documentation](https://clickhouse.com/docs/en/operations/quotas/).
    """

    DESCRIPTOR: google.protobuf.descriptor.Descriptor

    INTERVAL_DURATION_FIELD_NUMBER: builtins.int
    QUERIES_FIELD_NUMBER: builtins.int
    ERRORS_FIELD_NUMBER: builtins.int
    RESULT_ROWS_FIELD_NUMBER: builtins.int
    READ_ROWS_FIELD_NUMBER: builtins.int
    EXECUTION_TIME_FIELD_NUMBER: builtins.int
    @property
    def interval_duration(self) -> google.protobuf.wrappers_pb2.Int64Value:
        """Duration of interval for quota in milliseconds.
        Minimal value is 1 second.
        """

    @property
    def queries(self) -> google.protobuf.wrappers_pb2.Int64Value:
        """The total number of queries.
        0 - unlimited.
        """

    @property
    def errors(self) -> google.protobuf.wrappers_pb2.Int64Value:
        """The number of queries that threw exception.
        0 - unlimited.
        """

    @property
    def result_rows(self) -> google.protobuf.wrappers_pb2.Int64Value:
        """The total number of rows given as the result..
        0 - unlimited.
        """

    @property
    def read_rows(self) -> google.protobuf.wrappers_pb2.Int64Value:
        """The total number of source rows read from tables for running the query, on all remote servers.
        0 - unlimited.
        """

    @property
    def execution_time(self) -> google.protobuf.wrappers_pb2.Int64Value:
        """The total query execution time, in milliseconds (wall time).
        0 - unlimited.
        """

    def __init__(
        self,
        *,
        interval_duration: google.protobuf.wrappers_pb2.Int64Value | None = ...,
        queries: google.protobuf.wrappers_pb2.Int64Value | None = ...,
        errors: google.protobuf.wrappers_pb2.Int64Value | None = ...,
        result_rows: google.protobuf.wrappers_pb2.Int64Value | None = ...,
        read_rows: google.protobuf.wrappers_pb2.Int64Value | None = ...,
        execution_time: google.protobuf.wrappers_pb2.Int64Value | None = ...,
    ) -> None: ...
    def HasField(self, field_name: typing.Literal["errors", b"errors", "execution_time", b"execution_time", "interval_duration", b"interval_duration", "queries", b"queries", "read_rows", b"read_rows", "result_rows", b"result_rows"]) -> builtins.bool: ...
    def ClearField(self, field_name: typing.Literal["errors", b"errors", "execution_time", b"execution_time", "interval_duration", b"interval_duration", "queries", b"queries", "read_rows", b"read_rows", "result_rows", b"result_rows"]) -> None: ...

global___UserQuota = UserQuota
